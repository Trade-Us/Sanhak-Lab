{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "f = open('resources/InsectWingbeatSound/InsectWingbeatSound_TEST','r')\n",
    "data = f.read()\n",
    "f.close()\n",
    "# 개행문자 기준으로 끊어서 리스트로\n",
    "data_list = data.split('\\n')\n",
    "\n",
    "# \",\" 기준으로 끊어서 리스트로\n",
    "emptylist = []\n",
    "for list_part in data_list:\n",
    "    emptylist.append(list_part.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str -> float 변환\n",
    "tofloat = []\n",
    "for partlist in emptylist:\n",
    "    tofloat.append([float(i) for i in partlist]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980,)\n",
      "(1980, 256)\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "data_list = []\n",
    "for datas in tofloat:\n",
    "    labels.append(datas[0])\n",
    "    data_list.append(datas[1:])\n",
    "print(np.shape(labels))\n",
    "print(np.shape(data_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readFile import split_into_values, toRPdata\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "\n",
    "def Standard(data):\n",
    "    SS = StandardScaler().fit(data)\n",
    "    scaled = SS.transform(data)\n",
    "    return scaled\n",
    "\n",
    "def MinMax(data):\n",
    "    MMS = MinMaxScaler().fit(data)\n",
    "    scaled = MMS.transform(data)\n",
    "    return scaled\n",
    "\n",
    "# result_list transpose\n",
    "result_T = [list(x) for x in zip(*data_list)]\n",
    "\n",
    "# minmax 정규화\n",
    "result_scaled = Standard(result_T)\n",
    "\n",
    "# 다시 result transpose 해서 원래대로\n",
    "result_scaled = [list(x) for x in zip(*result_scaled)]\n",
    "\n",
    "result_ = np.array(result_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, 256)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, 256, 256, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = result_.reshape(result_.shape[0], 1, result_.shape[1])\n",
    "X = toRPdata(data, threshold='point', percentage=30)\n",
    "#X = toRPdata(data)\n",
    "    \n",
    "X_scaled = np.expand_dims(X, axis=3)\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fb5326f898>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-5\n",
    "optimizer='Adam'\n",
    "loss='mse'\n",
    "image_size = 256 #1024, 256\n",
    "dimension = 8 # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 128)     1280      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256, 256, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 64)      73792     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        18464     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 2)         578       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 2)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 2)           38        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2, 2, 2)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 32, 32, 2)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 32)        608       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 256, 256, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 256, 256, 1)       1153      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 256, 256, 1)       0         \n",
      "=================================================================\n",
      "Total params: 188,265\n",
      "Trainable params: 188,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from utils import split_data, normalization_tool\n",
    "from agent import Autoencoder_Agent\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_data(X_scaled, X_scaled) #데이터 분리\n",
    "\n",
    "autoencoder = Autoencoder_Agent(model_size=image_size, dimension=dimension, optimizer=optimizer,learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "  2/149 [..............................] - ETA: 9s - loss: 0.2499WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0539s vs `on_train_batch_end` time: 0.0818s). Check your callbacks.\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.2216\n",
      "Epoch 00001: val_loss improved from inf to 0.20451, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.2216 - val_loss: 0.2045\n",
      "Epoch 2/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.2037\n",
      "Epoch 00002: val_loss improved from 0.20451 to 0.20270, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 23s 151ms/step - loss: 0.2037 - val_loss: 0.2027\n",
      "Epoch 3/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.2021\n",
      "Epoch 00003: val_loss improved from 0.20270 to 0.20111, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 22s 151ms/step - loss: 0.2021 - val_loss: 0.2011\n",
      "Epoch 4/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.2008\n",
      "Epoch 00004: val_loss improved from 0.20111 to 0.20006, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 22s 151ms/step - loss: 0.2008 - val_loss: 0.2001\n",
      "Epoch 5/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1996\n",
      "Epoch 00005: val_loss improved from 0.20006 to 0.19834, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 22s 151ms/step - loss: 0.1996 - val_loss: 0.1983\n",
      "Epoch 6/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1974\n",
      "Epoch 00006: val_loss improved from 0.19834 to 0.19610, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 22s 151ms/step - loss: 0.1974 - val_loss: 0.1961\n",
      "Epoch 7/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1959\n",
      "Epoch 00007: val_loss improved from 0.19610 to 0.19510, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 22s 151ms/step - loss: 0.1959 - val_loss: 0.1951\n",
      "Epoch 8/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1949\n",
      "Epoch 00008: val_loss improved from 0.19510 to 0.19432, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 22s 151ms/step - loss: 0.1949 - val_loss: 0.1943\n",
      "Epoch 9/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1944\n",
      "Epoch 00009: val_loss improved from 0.19432 to 0.19412, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 23s 151ms/step - loss: 0.1944 - val_loss: 0.1941\n",
      "Epoch 10/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1940\n",
      "Epoch 00010: val_loss improved from 0.19412 to 0.19392, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 23s 153ms/step - loss: 0.1940 - val_loss: 0.1939\n",
      "Epoch 11/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1937\n",
      "Epoch 00011: val_loss improved from 0.19392 to 0.19367, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 23s 152ms/step - loss: 0.1937 - val_loss: 0.1937\n",
      "Epoch 12/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1935\n",
      "Epoch 00012: val_loss improved from 0.19367 to 0.19353, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 159ms/step - loss: 0.1935 - val_loss: 0.1935\n",
      "Epoch 13/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1933\n",
      "Epoch 00013: val_loss did not improve from 0.19353\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1933 - val_loss: 0.1936\n",
      "Epoch 14/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1932\n",
      "Epoch 00014: val_loss improved from 0.19353 to 0.19351, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1932 - val_loss: 0.1935\n",
      "Epoch 15/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1931\n",
      "Epoch 00015: val_loss improved from 0.19351 to 0.19317, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1931 - val_loss: 0.1932\n",
      "Epoch 16/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1928\n",
      "Epoch 00016: val_loss improved from 0.19317 to 0.19305, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 164ms/step - loss: 0.1928 - val_loss: 0.1930\n",
      "Epoch 17/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1927\n",
      "Epoch 00017: val_loss improved from 0.19305 to 0.19301, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1927 - val_loss: 0.1930\n",
      "Epoch 18/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1927\n",
      "Epoch 00018: val_loss improved from 0.19301 to 0.19301, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1927 - val_loss: 0.1930\n",
      "Epoch 19/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1925\n",
      "Epoch 00019: val_loss did not improve from 0.19301\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1925 - val_loss: 0.1930\n",
      "Epoch 20/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1925\n",
      "Epoch 00020: val_loss improved from 0.19301 to 0.19275, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1925 - val_loss: 0.1928\n",
      "Epoch 21/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1924\n",
      "Epoch 00021: val_loss did not improve from 0.19275\n",
      "149/149 [==============================] - 25s 165ms/step - loss: 0.1924 - val_loss: 0.1929\n",
      "Epoch 22/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1922\n",
      "Epoch 00022: val_loss did not improve from 0.19275\n",
      "149/149 [==============================] - 24s 164ms/step - loss: 0.1922 - val_loss: 0.1928\n",
      "Epoch 23/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1921\n",
      "Epoch 00023: val_loss improved from 0.19275 to 0.19253, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1921 - val_loss: 0.1925\n",
      "Epoch 24/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1920\n",
      "Epoch 00024: val_loss did not improve from 0.19253\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.1920 - val_loss: 0.1926\n",
      "Epoch 25/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1920\n",
      "Epoch 00025: val_loss improved from 0.19253 to 0.19253, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1920 - val_loss: 0.1925\n",
      "Epoch 26/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1918\n",
      "Epoch 00026: val_loss improved from 0.19253 to 0.19238, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.1918 - val_loss: 0.1924\n",
      "Epoch 27/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1917\n",
      "Epoch 00027: val_loss improved from 0.19238 to 0.19234, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1917 - val_loss: 0.1923\n",
      "Epoch 28/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1916\n",
      "Epoch 00028: val_loss did not improve from 0.19234\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.1916 - val_loss: 0.1924\n",
      "Epoch 29/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1916\n",
      "Epoch 00029: val_loss improved from 0.19234 to 0.19223, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1916 - val_loss: 0.1922\n",
      "Epoch 30/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1915\n",
      "Epoch 00030: val_loss did not improve from 0.19223\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1915 - val_loss: 0.1924\n",
      "Epoch 31/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1914\n",
      "Epoch 00031: val_loss did not improve from 0.19223\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.1914 - val_loss: 0.1922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1914\n",
      "Epoch 00032: val_loss improved from 0.19223 to 0.19220, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1914 - val_loss: 0.1922\n",
      "Epoch 33/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1913\n",
      "Epoch 00033: val_loss improved from 0.19220 to 0.19208, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1913 - val_loss: 0.1921\n",
      "Epoch 34/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1912\n",
      "Epoch 00034: val_loss improved from 0.19208 to 0.19197, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.1912 - val_loss: 0.1920\n",
      "Epoch 35/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1911\n",
      "Epoch 00035: val_loss improved from 0.19197 to 0.19191, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.1911 - val_loss: 0.1919\n",
      "Epoch 36/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1910\n",
      "Epoch 00036: val_loss improved from 0.19191 to 0.19190, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.1910 - val_loss: 0.1919\n",
      "Epoch 37/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1910\n",
      "Epoch 00037: val_loss did not improve from 0.19190\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.1910 - val_loss: 0.1919\n",
      "Epoch 38/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1909\n",
      "Epoch 00038: val_loss improved from 0.19190 to 0.19181, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.1909 - val_loss: 0.1918\n",
      "Epoch 39/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1908\n",
      "Epoch 00039: val_loss improved from 0.19181 to 0.19174, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.1908 - val_loss: 0.1917\n",
      "Epoch 40/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1908\n",
      "Epoch 00040: val_loss improved from 0.19174 to 0.19171, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 25s 166ms/step - loss: 0.1908 - val_loss: 0.1917\n",
      "Epoch 41/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1907\n",
      "Epoch 00041: val_loss did not improve from 0.19171\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1907 - val_loss: 0.1918\n",
      "Epoch 42/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1906\n",
      "Epoch 00042: val_loss improved from 0.19171 to 0.19165, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1906 - val_loss: 0.1916\n",
      "Epoch 43/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1906\n",
      "Epoch 00043: val_loss improved from 0.19165 to 0.19161, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1906 - val_loss: 0.1916\n",
      "Epoch 44/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1905\n",
      "Epoch 00044: val_loss improved from 0.19161 to 0.19158, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1905 - val_loss: 0.1916\n",
      "Epoch 45/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1904\n",
      "Epoch 00045: val_loss did not improve from 0.19158\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1904 - val_loss: 0.1917\n",
      "Epoch 46/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1904\n",
      "Epoch 00046: val_loss improved from 0.19158 to 0.19144, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1904 - val_loss: 0.1914\n",
      "Epoch 47/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1903\n",
      "Epoch 00047: val_loss did not improve from 0.19144\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1903 - val_loss: 0.1914\n",
      "Epoch 48/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1903\n",
      "Epoch 00048: val_loss did not improve from 0.19144\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1903 - val_loss: 0.1916\n",
      "Epoch 49/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1902\n",
      "Epoch 00049: val_loss improved from 0.19144 to 0.19144, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1902 - val_loss: 0.1914\n",
      "Epoch 50/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1902\n",
      "Epoch 00050: val_loss did not improve from 0.19144\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1902 - val_loss: 0.1916\n",
      "Epoch 51/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1901\n",
      "Epoch 00051: val_loss improved from 0.19144 to 0.19127, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1901 - val_loss: 0.1913\n",
      "Epoch 52/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1901\n",
      "Epoch 00052: val_loss did not improve from 0.19127\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1901 - val_loss: 0.1914\n",
      "Epoch 53/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1901\n",
      "Epoch 00053: val_loss did not improve from 0.19127\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1901 - val_loss: 0.1915\n",
      "Epoch 54/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1900\n",
      "Epoch 00054: val_loss did not improve from 0.19127\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1900 - val_loss: 0.1913\n",
      "Epoch 55/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1899\n",
      "Epoch 00055: val_loss improved from 0.19127 to 0.19117, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1899 - val_loss: 0.1912\n",
      "Epoch 56/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1899\n",
      "Epoch 00056: val_loss did not improve from 0.19117\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1899 - val_loss: 0.1912\n",
      "Epoch 57/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1898\n",
      "Epoch 00057: val_loss improved from 0.19117 to 0.19114, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1898 - val_loss: 0.1911\n",
      "Epoch 58/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1897\n",
      "Epoch 00058: val_loss improved from 0.19114 to 0.19107, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1897 - val_loss: 0.1911\n",
      "Epoch 59/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1897\n",
      "Epoch 00059: val_loss did not improve from 0.19107\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1897 - val_loss: 0.1911\n",
      "Epoch 60/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1896\n",
      "Epoch 00060: val_loss improved from 0.19107 to 0.19105, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1896 - val_loss: 0.1910\n",
      "Epoch 61/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1896\n",
      "Epoch 00061: val_loss did not improve from 0.19105\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1896 - val_loss: 0.1912\n",
      "Epoch 62/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1895\n",
      "Epoch 00062: val_loss did not improve from 0.19105\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1895 - val_loss: 0.1912\n",
      "Epoch 63/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1895\n",
      "Epoch 00063: val_loss did not improve from 0.19105\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1895 - val_loss: 0.1911\n",
      "Epoch 64/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1895\n",
      "Epoch 00064: val_loss improved from 0.19105 to 0.19098, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1895 - val_loss: 0.1910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1894\n",
      "Epoch 00065: val_loss did not improve from 0.19098\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1894 - val_loss: 0.1910\n",
      "Epoch 66/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1894\n",
      "Epoch 00066: val_loss improved from 0.19098 to 0.19091, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 25s 165ms/step - loss: 0.1894 - val_loss: 0.1909\n",
      "Epoch 67/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1893\n",
      "Epoch 00067: val_loss did not improve from 0.19091\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1893 - val_loss: 0.1909\n",
      "Epoch 68/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1892\n",
      "Epoch 00068: val_loss improved from 0.19091 to 0.19084, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1892 - val_loss: 0.1908\n",
      "Epoch 69/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1892\n",
      "Epoch 00069: val_loss did not improve from 0.19084\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1892 - val_loss: 0.1911\n",
      "Epoch 70/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1891\n",
      "Epoch 00070: val_loss improved from 0.19084 to 0.19083, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1891 - val_loss: 0.1908\n",
      "Epoch 71/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1892\n",
      "Epoch 00071: val_loss did not improve from 0.19083\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1892 - val_loss: 0.1910\n",
      "Epoch 72/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1891\n",
      "Epoch 00072: val_loss did not improve from 0.19083\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1891 - val_loss: 0.1909\n",
      "Epoch 73/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1890\n",
      "Epoch 00073: val_loss improved from 0.19083 to 0.19071, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 164ms/step - loss: 0.1890 - val_loss: 0.1907\n",
      "Epoch 74/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1890\n",
      "Epoch 00074: val_loss did not improve from 0.19071\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1890 - val_loss: 0.1909\n",
      "Epoch 75/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1889\n",
      "Epoch 00075: val_loss did not improve from 0.19071\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.1889 - val_loss: 0.1908\n",
      "Epoch 76/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00076: val_loss did not improve from 0.19071\n",
      "149/149 [==============================] - 25s 166ms/step - loss: 0.1888 - val_loss: 0.1908\n",
      "Epoch 77/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00077: val_loss improved from 0.19071 to 0.19065, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1888 - val_loss: 0.1907\n",
      "Epoch 78/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00078: val_loss did not improve from 0.19065\n",
      "149/149 [==============================] - 24s 164ms/step - loss: 0.1887 - val_loss: 0.1907\n",
      "Epoch 79/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00079: val_loss improved from 0.19065 to 0.19064, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.1887 - val_loss: 0.1906\n",
      "Epoch 80/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00080: val_loss improved from 0.19064 to 0.19061, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1887 - val_loss: 0.1906\n",
      "Epoch 81/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00081: val_loss improved from 0.19061 to 0.19056, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 25s 165ms/step - loss: 0.1886 - val_loss: 0.1906\n",
      "Epoch 82/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00082: val_loss improved from 0.19056 to 0.19051, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1886 - val_loss: 0.1905\n",
      "Epoch 83/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1885\n",
      "Epoch 00083: val_loss did not improve from 0.19051\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1885 - val_loss: 0.1907\n",
      "Epoch 84/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1885\n",
      "Epoch 00084: val_loss improved from 0.19051 to 0.19050, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1885 - val_loss: 0.1905\n",
      "Epoch 85/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1884\n",
      "Epoch 00085: val_loss improved from 0.19050 to 0.19045, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1884 - val_loss: 0.1904\n",
      "Epoch 86/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00086: val_loss did not improve from 0.19045\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1883 - val_loss: 0.1905\n",
      "Epoch 87/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00087: val_loss did not improve from 0.19045\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1883 - val_loss: 0.1906\n",
      "Epoch 88/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00088: val_loss improved from 0.19045 to 0.19036, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1883 - val_loss: 0.1904\n",
      "Epoch 89/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00089: val_loss improved from 0.19036 to 0.19031, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1883 - val_loss: 0.1903\n",
      "Epoch 90/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1881\n",
      "Epoch 00090: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1881 - val_loss: 0.1905\n",
      "Epoch 91/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00091: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1882 - val_loss: 0.1903\n",
      "Epoch 92/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1881\n",
      "Epoch 00092: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1881 - val_loss: 0.1904\n",
      "Epoch 93/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1881\n",
      "Epoch 00093: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1881 - val_loss: 0.1904\n",
      "Epoch 94/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00094: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1880 - val_loss: 0.1903\n",
      "Epoch 95/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00095: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1880 - val_loss: 0.1906\n",
      "Epoch 96/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00096: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1879 - val_loss: 0.1904\n",
      "Epoch 97/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00097: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 25s 165ms/step - loss: 0.1879 - val_loss: 0.1904\n",
      "Epoch 98/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00098: val_loss improved from 0.19031 to 0.19023, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1878 - val_loss: 0.1902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00099: val_loss did not improve from 0.19023\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1878 - val_loss: 0.1904\n",
      "Epoch 100/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00100: val_loss did not improve from 0.19023\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1878 - val_loss: 0.1903\n",
      "Epoch 101/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00101: val_loss did not improve from 0.19023\n",
      "149/149 [==============================] - 24s 164ms/step - loss: 0.1877 - val_loss: 0.1904\n",
      "Epoch 102/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00102: val_loss improved from 0.19023 to 0.19014, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 25s 165ms/step - loss: 0.1877 - val_loss: 0.1901\n",
      "Epoch 103/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00103: val_loss did not improve from 0.19014\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1877 - val_loss: 0.1901\n",
      "Epoch 104/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00104: val_loss did not improve from 0.19014\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1876 - val_loss: 0.1902\n",
      "Epoch 105/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00105: val_loss did not improve from 0.19014\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1876 - val_loss: 0.1903\n",
      "Epoch 106/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1875\n",
      "Epoch 00106: val_loss improved from 0.19014 to 0.19013, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1875 - val_loss: 0.1901\n",
      "Epoch 107/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1875\n",
      "Epoch 00107: val_loss did not improve from 0.19013\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1875 - val_loss: 0.1901\n",
      "Epoch 108/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1875\n",
      "Epoch 00108: val_loss did not improve from 0.19013\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1875 - val_loss: 0.1901\n",
      "Epoch 109/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00109: val_loss improved from 0.19013 to 0.19004, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1874 - val_loss: 0.1900\n",
      "Epoch 110/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00110: val_loss did not improve from 0.19004\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1874 - val_loss: 0.1902\n",
      "Epoch 111/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00111: val_loss improved from 0.19004 to 0.19004, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 160ms/step - loss: 0.1874 - val_loss: 0.1900\n",
      "Epoch 112/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00112: val_loss did not improve from 0.19004\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1873 - val_loss: 0.1901\n",
      "Epoch 113/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00113: val_loss did not improve from 0.19004\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1873 - val_loss: 0.1902\n",
      "Epoch 114/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1872\n",
      "Epoch 00114: val_loss did not improve from 0.19004\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1872 - val_loss: 0.1904\n",
      "Epoch 115/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1872\n",
      "Epoch 00115: val_loss improved from 0.19004 to 0.19003, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1872 - val_loss: 0.1900\n",
      "Epoch 116/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00116: val_loss improved from 0.19003 to 0.19002, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1873 - val_loss: 0.1900\n",
      "Epoch 117/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1871\n",
      "Epoch 00117: val_loss did not improve from 0.19002\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1871 - val_loss: 0.1904\n",
      "Epoch 118/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1872\n",
      "Epoch 00118: val_loss did not improve from 0.19002\n",
      "149/149 [==============================] - 24s 164ms/step - loss: 0.1872 - val_loss: 0.1907\n",
      "Epoch 119/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1871\n",
      "Epoch 00119: val_loss did not improve from 0.19002\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1871 - val_loss: 0.1904\n",
      "Epoch 120/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1871\n",
      "Epoch 00120: val_loss did not improve from 0.19002\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1871 - val_loss: 0.1900\n",
      "Epoch 121/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1870\n",
      "Epoch 00121: val_loss did not improve from 0.19002\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1870 - val_loss: 0.1901\n",
      "Epoch 122/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1870\n",
      "Epoch 00122: val_loss did not improve from 0.19002\n",
      "149/149 [==============================] - 24s 164ms/step - loss: 0.1870 - val_loss: 0.1901\n",
      "Epoch 123/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1870\n",
      "Epoch 00123: val_loss did not improve from 0.19002\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1870 - val_loss: 0.1901\n",
      "Epoch 124/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1869\n",
      "Epoch 00124: val_loss did not improve from 0.19002\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1869 - val_loss: 0.1901\n",
      "Epoch 125/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1870\n",
      "Epoch 00125: val_loss improved from 0.19002 to 0.19001, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1870 - val_loss: 0.1900\n",
      "Epoch 126/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1868\n",
      "Epoch 00126: val_loss did not improve from 0.19001\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1868 - val_loss: 0.1902\n",
      "Epoch 127/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1868\n",
      "Epoch 00127: val_loss did not improve from 0.19001\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1868 - val_loss: 0.1906\n",
      "Epoch 128/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1868\n",
      "Epoch 00128: val_loss did not improve from 0.19001\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1868 - val_loss: 0.1901\n",
      "Epoch 129/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1868\n",
      "Epoch 00129: val_loss improved from 0.19001 to 0.18999, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1868 - val_loss: 0.1900\n",
      "Epoch 130/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1868\n",
      "Epoch 00130: val_loss did not improve from 0.18999\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1868 - val_loss: 0.1900\n",
      "Epoch 131/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1867\n",
      "Epoch 00131: val_loss did not improve from 0.18999\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1867 - val_loss: 0.1902\n",
      "Epoch 132/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1867\n",
      "Epoch 00132: val_loss did not improve from 0.18999\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1867 - val_loss: 0.1902\n",
      "Epoch 133/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1867\n",
      "Epoch 00133: val_loss did not improve from 0.18999\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1867 - val_loss: 0.1901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1867\n",
      "Epoch 00134: val_loss did not improve from 0.18999\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1867 - val_loss: 0.1900\n",
      "Epoch 135/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1866\n",
      "Epoch 00135: val_loss did not improve from 0.18999\n",
      "149/149 [==============================] - 24s 164ms/step - loss: 0.1866 - val_loss: 0.1901\n",
      "Epoch 136/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1866\n",
      "Epoch 00136: val_loss did not improve from 0.18999\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1866 - val_loss: 0.1901\n",
      "Epoch 137/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1866\n",
      "Epoch 00137: val_loss did not improve from 0.18999\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1866 - val_loss: 0.1900\n",
      "Epoch 138/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1865\n",
      "Epoch 00138: val_loss improved from 0.18999 to 0.18994, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1865 - val_loss: 0.1899\n",
      "Epoch 139/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1865\n",
      "Epoch 00139: val_loss did not improve from 0.18994\n",
      "149/149 [==============================] - 24s 164ms/step - loss: 0.1865 - val_loss: 0.1900\n",
      "Epoch 140/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1866\n",
      "Epoch 00140: val_loss did not improve from 0.18994\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1866 - val_loss: 0.1900\n",
      "Epoch 141/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1864\n",
      "Epoch 00141: val_loss did not improve from 0.18994\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1864 - val_loss: 0.1901\n",
      "Epoch 142/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1864\n",
      "Epoch 00142: val_loss improved from 0.18994 to 0.18992, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 164ms/step - loss: 0.1864 - val_loss: 0.1899\n",
      "Epoch 143/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1864\n",
      "Epoch 00143: val_loss improved from 0.18992 to 0.18990, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1864 - val_loss: 0.1899\n",
      "Epoch 144/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1863\n",
      "Epoch 00144: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 25s 165ms/step - loss: 0.1863 - val_loss: 0.1903\n",
      "Epoch 145/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1863\n",
      "Epoch 00145: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1863 - val_loss: 0.1899\n",
      "Epoch 146/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1863\n",
      "Epoch 00146: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1863 - val_loss: 0.1900\n",
      "Epoch 147/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1863\n",
      "Epoch 00147: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1863 - val_loss: 0.1902\n",
      "Epoch 148/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1863\n",
      "Epoch 00148: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1863 - val_loss: 0.1900\n",
      "Epoch 149/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1863\n",
      "Epoch 00149: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1863 - val_loss: 0.1900\n",
      "Epoch 150/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1862\n",
      "Epoch 00150: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1862 - val_loss: 0.1901\n",
      "Epoch 151/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1862\n",
      "Epoch 00151: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1862 - val_loss: 0.1901\n",
      "Epoch 152/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1862\n",
      "Epoch 00152: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 164ms/step - loss: 0.1862 - val_loss: 0.1900\n",
      "Epoch 153/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1861\n",
      "Epoch 00153: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1861 - val_loss: 0.1900\n",
      "Epoch 154/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1862\n",
      "Epoch 00154: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1862 - val_loss: 0.1900\n",
      "Epoch 155/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1861\n",
      "Epoch 00155: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1861 - val_loss: 0.1899\n",
      "Epoch 156/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1861\n",
      "Epoch 00156: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1861 - val_loss: 0.1900\n",
      "Epoch 157/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1861\n",
      "Epoch 00157: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1861 - val_loss: 0.1901\n",
      "Epoch 158/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1860\n",
      "Epoch 00158: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1860 - val_loss: 0.1899\n",
      "Epoch 159/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1859\n",
      "Epoch 00159: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1859 - val_loss: 0.1901\n",
      "Epoch 160/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1860\n",
      "Epoch 00160: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1860 - val_loss: 0.1901\n",
      "Epoch 161/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1859\n",
      "Epoch 00161: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1859 - val_loss: 0.1900\n",
      "Epoch 162/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1859\n",
      "Epoch 00162: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1859 - val_loss: 0.1900\n",
      "Epoch 163/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1859\n",
      "Epoch 00163: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1859 - val_loss: 0.1901\n",
      "Epoch 164/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1858\n",
      "Epoch 00164: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1858 - val_loss: 0.1899\n",
      "Epoch 165/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1858\n",
      "Epoch 00165: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1858 - val_loss: 0.1900\n",
      "Epoch 166/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1858\n",
      "Epoch 00166: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1858 - val_loss: 0.1902\n",
      "Epoch 167/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1858\n",
      "Epoch 00167: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1858 - val_loss: 0.1902\n",
      "Epoch 168/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1859\n",
      "Epoch 00168: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1859 - val_loss: 0.1903\n",
      "Epoch 169/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1857\n",
      "Epoch 00169: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1857 - val_loss: 0.1901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1857\n",
      "Epoch 00170: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1857 - val_loss: 0.1902\n",
      "Epoch 171/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1856\n",
      "Epoch 00171: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1856 - val_loss: 0.1901\n",
      "Epoch 172/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1857\n",
      "Epoch 00172: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1857 - val_loss: 0.1903\n",
      "Epoch 173/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1857\n",
      "Epoch 00173: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1857 - val_loss: 0.1902\n",
      "Epoch 174/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1857\n",
      "Epoch 00174: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1857 - val_loss: 0.1899\n",
      "Epoch 175/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1856\n",
      "Epoch 00175: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1856 - val_loss: 0.1900\n",
      "Epoch 176/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1856\n",
      "Epoch 00176: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1856 - val_loss: 0.1901\n",
      "Epoch 177/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1855\n",
      "Epoch 00177: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1855 - val_loss: 0.1900\n",
      "Epoch 178/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1855\n",
      "Epoch 00178: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1855 - val_loss: 0.1902\n",
      "Epoch 179/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1855\n",
      "Epoch 00179: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1855 - val_loss: 0.1900\n",
      "Epoch 180/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1854\n",
      "Epoch 00180: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1854 - val_loss: 0.1900\n",
      "Epoch 181/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1855\n",
      "Epoch 00181: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1855 - val_loss: 0.1906\n",
      "Epoch 182/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1854\n",
      "Epoch 00182: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1854 - val_loss: 0.1901\n",
      "Epoch 183/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1855\n",
      "Epoch 00183: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1855 - val_loss: 0.1900\n",
      "Epoch 184/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1854\n",
      "Epoch 00184: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1854 - val_loss: 0.1902\n",
      "Epoch 185/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1853\n",
      "Epoch 00185: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1853 - val_loss: 0.1903\n",
      "Epoch 186/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1853\n",
      "Epoch 00186: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1853 - val_loss: 0.1901\n",
      "Epoch 187/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1853\n",
      "Epoch 00187: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1853 - val_loss: 0.1900\n",
      "Epoch 188/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1853\n",
      "Epoch 00188: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1853 - val_loss: 0.1900\n",
      "Epoch 189/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1852\n",
      "Epoch 00189: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1852 - val_loss: 0.1900\n",
      "Epoch 190/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1853\n",
      "Epoch 00190: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1853 - val_loss: 0.1902\n",
      "Epoch 191/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1852\n",
      "Epoch 00191: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1852 - val_loss: 0.1901\n",
      "Epoch 192/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1852\n",
      "Epoch 00192: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1852 - val_loss: 0.1901\n",
      "Epoch 193/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1852\n",
      "Epoch 00193: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1852 - val_loss: 0.1900\n",
      "Epoch 194/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1851\n",
      "Epoch 00194: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1851 - val_loss: 0.1902\n",
      "Epoch 195/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1851\n",
      "Epoch 00195: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1851 - val_loss: 0.1902\n",
      "Epoch 196/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1851\n",
      "Epoch 00196: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1851 - val_loss: 0.1901\n",
      "Epoch 197/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1851\n",
      "Epoch 00197: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1851 - val_loss: 0.1901\n",
      "Epoch 198/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1851\n",
      "Epoch 00198: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1851 - val_loss: 0.1900\n",
      "Epoch 199/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1851\n",
      "Epoch 00199: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1851 - val_loss: 0.1900\n",
      "Epoch 200/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1850\n",
      "Epoch 00200: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1850 - val_loss: 0.1901\n",
      "Epoch 201/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1850\n",
      "Epoch 00201: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1850 - val_loss: 0.1900\n",
      "Epoch 202/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1849\n",
      "Epoch 00202: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1849 - val_loss: 0.1902\n",
      "Epoch 203/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1849\n",
      "Epoch 00203: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1849 - val_loss: 0.1902\n",
      "Epoch 204/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1849\n",
      "Epoch 00204: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1849 - val_loss: 0.1902\n",
      "Epoch 205/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1849\n",
      "Epoch 00205: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1849 - val_loss: 0.1902\n",
      "Epoch 206/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1850\n",
      "Epoch 00206: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1850 - val_loss: 0.1900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1848\n",
      "Epoch 00207: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1848 - val_loss: 0.1903\n",
      "Epoch 208/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1848\n",
      "Epoch 00208: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1848 - val_loss: 0.1901\n",
      "Epoch 209/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1849\n",
      "Epoch 00209: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1849 - val_loss: 0.1903\n",
      "Epoch 210/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1848\n",
      "Epoch 00210: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1848 - val_loss: 0.1902\n",
      "Epoch 211/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1847\n",
      "Epoch 00211: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 25s 165ms/step - loss: 0.1847 - val_loss: 0.1902\n",
      "Epoch 212/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1847\n",
      "Epoch 00212: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 25s 165ms/step - loss: 0.1847 - val_loss: 0.1903\n",
      "Epoch 213/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1847\n",
      "Epoch 00213: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1847 - val_loss: 0.1902\n",
      "Epoch 214/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1847\n",
      "Epoch 00214: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1847 - val_loss: 0.1904\n",
      "Epoch 215/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1847\n",
      "Epoch 00215: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1847 - val_loss: 0.1900\n",
      "Epoch 216/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1846\n",
      "Epoch 00216: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1846 - val_loss: 0.1902\n",
      "Epoch 217/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1847\n",
      "Epoch 00217: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1847 - val_loss: 0.1901\n",
      "Epoch 218/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1846\n",
      "Epoch 00218: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1846 - val_loss: 0.1901\n",
      "Epoch 219/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1846\n",
      "Epoch 00219: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1846 - val_loss: 0.1903\n",
      "Epoch 220/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1846\n",
      "Epoch 00220: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1846 - val_loss: 0.1902\n",
      "Epoch 221/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1845\n",
      "Epoch 00221: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1845 - val_loss: 0.1901\n",
      "Epoch 222/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1845\n",
      "Epoch 00222: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1845 - val_loss: 0.1902\n",
      "Epoch 223/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1845\n",
      "Epoch 00223: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1845 - val_loss: 0.1902\n",
      "Epoch 224/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1845\n",
      "Epoch 00224: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1845 - val_loss: 0.1902\n",
      "Epoch 225/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1844\n",
      "Epoch 00225: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1844 - val_loss: 0.1901\n",
      "Epoch 226/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1844\n",
      "Epoch 00226: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1844 - val_loss: 0.1901\n",
      "Epoch 227/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1845\n",
      "Epoch 00227: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1845 - val_loss: 0.1901\n",
      "Epoch 228/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1844\n",
      "Epoch 00228: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1844 - val_loss: 0.1903\n",
      "Epoch 229/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1844\n",
      "Epoch 00229: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1844 - val_loss: 0.1901\n",
      "Epoch 230/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1843\n",
      "Epoch 00230: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1843 - val_loss: 0.1902\n",
      "Epoch 231/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1843\n",
      "Epoch 00231: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1843 - val_loss: 0.1903\n",
      "Epoch 232/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1842\n",
      "Epoch 00232: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1842 - val_loss: 0.1901\n",
      "Epoch 233/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1843\n",
      "Epoch 00233: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1843 - val_loss: 0.1904\n",
      "Epoch 234/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1844\n",
      "Epoch 00234: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1844 - val_loss: 0.1901\n",
      "Epoch 235/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1843\n",
      "Epoch 00235: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1843 - val_loss: 0.1901\n",
      "Epoch 236/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1842\n",
      "Epoch 00236: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 164ms/step - loss: 0.1842 - val_loss: 0.1901\n",
      "Epoch 237/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1842\n",
      "Epoch 00237: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 161ms/step - loss: 0.1842 - val_loss: 0.1902\n",
      "Epoch 238/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1842\n",
      "Epoch 00238: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1842 - val_loss: 0.1902\n",
      "Epoch 239/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1842\n",
      "Epoch 00239: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 163ms/step - loss: 0.1842 - val_loss: 0.1903\n",
      "Epoch 240/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1841\n",
      "Epoch 00240: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1841 - val_loss: 0.1903\n",
      "Epoch 241/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1842\n",
      "Epoch 00241: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1842 - val_loss: 0.1902\n",
      "Epoch 242/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1842\n",
      "Epoch 00242: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1842 - val_loss: 0.1902\n",
      "Epoch 243/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.1841\n",
      "Epoch 00243: val_loss did not improve from 0.18990\n",
      "149/149 [==============================] - 24s 162ms/step - loss: 0.1841 - val_loss: 0.1902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00243: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist = autoencoder.train(X_train,batch_size,epochs,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "# loss_ax.plot([hist['loss'][i] - hist['val_loss'][i] for i in range(len(hist['loss']))], 'g', label='loss - val loss')\n",
    "\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980, 8)\n"
     ]
    }
   ],
   "source": [
    "features = np.empty((0,8), float)\n",
    "for i in range(66):\n",
    "    features = np.append(features, autoencoder.feature_extract(X_scaled[i*30:(i+1)*30]), axis=0)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "result = KMeans(n_clusters=11).fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  7  1  4  2  1  2 10  8  3  4  6  9  3 10  1  7  7  8  5  1  4  5  5\n",
      "  1  2  1  2  3  3  3 10  1  1 10  3  0  2  1  7  8  1  8  2  7  7  1  7\n",
      "  7  2  1  3  6  6  6  1 10  1 10  1  2  3 10  4  7  9  8  3  8  3  5  8\n",
      "  3  3  1 10  0  2  2  5  7  9  1 10  7  2  1  7  2  1  4  7 10  3  7  2\n",
      "  2  2  1  8  1  1  4  9  2 10  2  3  7  2  2  3  8  3  3  3  1  6  0  1\n",
      "  3  5  8  2  5  1  2  4  3  7  5  1  5  4  8  7 10  2  4  3  8  1 10  8\n",
      "  6  5  7  8  4  1  1  1  1  1  8  2  4  7  2  1  1  1  5  2  1  1  0  6\n",
      "  6  4  4  5  5  7  7  3  1  2  8  2  1  1 10  3  7  8  2  3  9  4  4 10\n",
      "  8  7  2 10  2  4  3  0  7  6  3  6  7  0  3 10  1  7  5  5  6  2  1  6\n",
      "  6 10 10 10  2 10  8  6  1  2  9  5  1  4  7  5  1  5  2  2  7  6  2  6\n",
      "  1 10  4  0  0  4  2  6  7  7  3  7  3  3  1  3  0 10  1  7  4  2  3  4\n",
      "  4  7  1  5  7  3  0  4  7  2  2  3  7  1  6  7  2  1  7  1  1  7  1  2\n",
      "  6  5  1  3  1  7  7  4  1  3  1  1  7  3  4  2  1  0  1  1  7  2  7  3\n",
      "  3  5  7  1 10  6  7  7  7  1  3  7  0  2  1  9 10  3 10  9 10  4 10  1\n",
      "  5  3  7  3  3  7  7  2  4  4  3  2  3  4  4 10  9  4  3  5  2  3  1  1\n",
      "  2 10  9 10  7  5  1  5  4  1  7  3  1 10  6  6  1  1  1  2  1  9  1  2\n",
      "  3  9  1  1  8  7  2 10  3  2  3  6  1  2  7  2  1  6  1  7  2  1 10  3\n",
      "  3 10  8 10 10  1  7  9  8  0  9  1  1  2  3  4 10  6  2  4  8  2  7  1\n",
      "  6  2  1  2  4  4  5  2  4  1  9  1  1  5  2  0  1  1  1  4 10  7  1  1\n",
      "  1  7  2  7  1  6  4  2  6  1  6  7  4  3  2  7  4  3  2  9  2  2  4  3\n",
      "  2  7  1  2  9  2  2  4  7  1  7  8  1  5  3  4  2  2  4  5  2  7  5 10\n",
      "  2  8  8  1  2  1  1  7  1  7  2  2  3  4  1  2  4  2  4  1  4 10  1  1\n",
      "  1  7  7  3  7 10  1  3  1  5  2  3  1  0  7  1  8  3  1  4  7  1  2  1\n",
      "  3  2  3  8  3 10  7  7  5  7  3 10 10  8  4  3  1  1  1 10  1  2  8  4\n",
      "  2  7  1  7  7  2  4  7  6  3  2  5  6  5 10  1  2  1  2  2  1  7  7  6\n",
      "  1 10  3 10  9  2  3  6  2  7  2  7  2  4  3 10  2  4  6  3  7  2  1 10\n",
      "  3  2  3  6  3 10  7  3  7  1  3  7  7  5  7  2  3  7  2  1  1  3 10  8\n",
      "  2  7  2  4  5  3  8  1 10  0  1  7  7  8  2  3  8  2  1  7  2  2  1  4\n",
      "  4  1  2  1  1  1  7 10  2  2  1  7  3  7  0  8  7 10  2  5 10  8  1  1\n",
      "  2  8  2  1  3  5  0  3  1  1  3  9  1  1 10  0 10 10  3  7  7  1  2  1\n",
      "  0  1  8  3  3  6 10  2  2  5  3  8  2  2  7  7  7  7  4  4  2  4  8  3\n",
      "  3  1  2  2  6  1  7  7  6  3  7  1  6  5  4  6  2 10  7  5  3  2  3  9\n",
      "  4  1  7 10  7  6  8  1  4  9  5  6  2  1  1  2  1  5  4  7  2  9  3  1\n",
      "  0  3  2  2  8  4  4  4  5  1  2  8  1  7  8  7  2  1  3  6  3  1  2  5\n",
      "  2  1  7  3  1  1  1  3  7  6  2  3  1  9  2  7  7  5  1 10  3  2  2  3\n",
      "  3  9  7  2  8  8  1 10  2  3  3  3  2  3  3  5  5 10  1  6  2  3  9  7\n",
      "  7  1  1  8  2  2  3  2  7  7 10  1  0  1  9  1  4  1  8  4  5 10  1  2\n",
      " 10  7  0  7  5  9  8  5  1  3  6  2  2  8  2  7  2  1  1  1  3  4 10  7\n",
      "  2  1  1  3  1  2  1  3  5  6  7  3  6  7  2  5  2  0  7  4  1  7  2  7\n",
      "  5 10  2  1  9  4  9  7  2  2  9  5  2  8 10  3  2  6  3  1  1  9  4  6\n",
      "  9  2  1  7  3  2  1  6  9  1  1  2  2  9  0  3 10  3  1  2  7  2  7  9\n",
      "  4  6  2  7  7  1  7  9  6  1  2  8  1  2 10 10  8  8  9  7  1  5  6  3\n",
      "  2  7  4  2  6  6  8  3  3  7  7  1  5  7 10 10  0  9  7 10  1  4 10  7\n",
      "  1  1  1  5  1  1  1  1  6  3  7  1  4  4  4  3  2  7  1  5  5  0  1  1\n",
      "  2  3  1  3  5  7  7  8  6  2  1  3  2  2  5 10  1  7  2  1  1 10  6  2\n",
      "  8  7  1  6 10  5  7  4 10  1  2  7  9  7  8  4  0  7  1  5  7  7  2  1\n",
      "  4  1  1  3  1  1 10  1 10  7 10  1  1  2  1  1  5  7  3  1  1  2  2  9\n",
      "  2  6  5  1  1  2  5  4  2  2  3  7  3  5  9  8  1  2  2  1  6  7  9  5\n",
      "  9 10  2  3  2  1  8  4  3  1  5  5 10  2 10  8  9  2  4  6  3  3 10  1\n",
      "  1  6  3  8  4  7 10  2  4  7  7  7  3  2  2  4  4  3  3  6  0  1  4 10\n",
      "  9 10  1  5 10  1  7  3  4  5  4  4  1  1  1  3  2  3  1  0 10  2  1 10\n",
      "  9  3  4  2  4  1  5  2  1  5  5  3  1  7  4  4  7  4  2  1  7  1  2  9\n",
      "  5  6  9  5  0  3  1  9  2  7  4 10  3  5  4  4  1  7  7  6  1 10  5  1\n",
      "  2  4  4  8  7  1  3  4  2  3 10  7 10  5  3  1  1 10  7  3  3  7  4  5\n",
      "  3  8  5  8  7  9  8  8  6  6  2  4  5  1  8  5  2  1  8  4  7  7  4  5\n",
      "  4  2  4  6  2  2  2  1  6  6  4  1  2  4  5  1  7  3  5 10  1  4  7  7\n",
      " 10  2  7  7  2  1  0  2  8  4  1  2  6  4  0  5  7  1  1 10  1  7  4  1\n",
      "  8  8  4  1  1  7  2  1  8  5  7  8  1 10 10 10  2  1  8  1  1  7  4  6\n",
      "  2  5  7  7  3 10  1  6  4  2  1  3  1  4  2  6  8  1 10  2  5  2  7  1\n",
      "  2  3  5  2  2  8  7  8  3  9  3 10  7  1  1  4  1  5  7  3  4 10 10  1\n",
      "  2  2  3  1  1  2  1  7  1  3  6  7  5  4  2  2  1  4 10  1  3  2 10  7\n",
      "  3  7  4 10  2  1  7  4  2  5  1  2  1  1  0 10  4  1  3  6 10  2  6  7\n",
      " 10 10  7  6  7  8  2  9  0 10  5 10 10  5  2  3  3  1  0 10  9  1  0  3\n",
      "  8  1  9  5  1  3  7  3 10  1  6 10  4  2 10  4  1  4  7  3  0  5  3  4\n",
      "  1  1  1  2  2  7  6  2  7 10  5  9  4  2  4  6  1  2  1  8  1 10  3 10\n",
      "  2  0  4  4  7  8  7  4  5  2  1  8  9  2  2  5  1  7  4 10  3  5  1  1\n",
      "  4  7 10  0 10  5 10  5  1  8  1  7  1  0  2  7  6  7  8  2  3  7  1  1\n",
      "  1  8  2  1  5  4  8  6  4 10  1  2  3  7  7  2  1  3  6  6  1  8  3  1\n",
      "  2  4  1  4  5  2 10  7  2  2  6  4  6  2  2 10  3  0 10  1  3  2  7  2\n",
      "  2  1  3  1  2  4  5  3  2  8  2  4  4  3  1  8  1  8  2  1  4  3  7  1\n",
      "  2  0  2  7  5  7  1  1  7  1  3  1  9  4  4  7  3  1  7  4  1  7  7  2\n",
      "  8  7  0  1  1  7  3  9  4  4  2  2  1  1  2 10  6  6  5  2  3 10  1  3\n",
      "  9  8  7 10 10  7  1  3 10  2  1  3  2  1  5  1  4  1 10  7  4  2  2  1\n",
      "  1  1  4  5  1  3  1  4  6  3  5  2  9  3  8  2  4  8  4  1  1  2  4  7\n",
      "  5  7  1  5  0  1  1  3 10  3  7  8  2  5  7  4  7  3  1  1  2  5  9  3\n",
      "  3 10  2  1  7  5  1  1  3  5  4  1  8  6  3 10 10  4  1  7  6  4  3  2\n",
      "  2  6  8  7  2  1  0  3  4  8  1  3  6  5  2  3 10  3  1  0  2  6  7  7\n",
      "  2  5  3  7 10  1  1  2  4  6  7  4  2  3  1 10  7  1  2  1  1  2  6  1\n",
      "  4  2  7  7  1  5  6  9  2  8  1  6  5  1  2  0 10  2  8 10  2  1  2 10\n",
      "  2  1  1 10  7  0  3  2  1  7  3  3  4  2  1  3  0  8  2  2  2  2  1  2\n",
      "  7 10  8  1 10  1  8 10  5  2  2  7  3  3  9  1  2  4 10  1 10  1  1  3\n",
      "  9  3  7  6  7  5  1 10 10  4  7  2 10  7  7  3  1  1  3 10  5  1  0  4\n",
      "  8  7  5  7  6  1  1  2  4  1  6  1]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy\n",
    "\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "print(result.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "from matplotlib import cm\n",
    "\n",
    "def plotSilhouette(X, y_km):\n",
    "    cluster_labels = np.unique(y_km.labels_)\n",
    "    n_clusters = cluster_labels.shape[0]\n",
    "    silhouette_vals = silhouette_score(X, y_km.labels_,metric='euclidean')\n",
    "    print(silhouette_vals)\n",
    "#     y_ax_lower, y_ax_upper = 0,0\n",
    "#     yticks = []\n",
    "    \n",
    "#     for i , c in enumerate(cluster_labels):\n",
    "#         c_silhouette_vals = silhouette_vals[y_km.labels_ == c]\n",
    "#         c_silhouette_vals.sort()\n",
    "#         y_ax_upper += len(c_silhouette_vals)\n",
    "#         color = cm.jet(i/n_clusters)\n",
    "        \n",
    "#         plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0,edgecolor='none', color=color)\n",
    "#         yticks.append((y_ax_lower + y_ax_upper)/2)\n",
    "#         y_ax_lower += len(c_silhouette_vals)\n",
    "    \n",
    "#     silhouette_avg = np.mean(silhouette_vals)\n",
    "#     plt.axvline(silhouette_avg, color='red', linestyle='--')\n",
    "#     plt.yticks(yticks, cluster_labels+1)\n",
    "#     plt.ylabel('cluster')\n",
    "#     plt.xlabel('silhouette score')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1704536760514606\n"
     ]
    }
   ],
   "source": [
    "plotSilhouette(features,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save(\"insect_128_8_mse.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
