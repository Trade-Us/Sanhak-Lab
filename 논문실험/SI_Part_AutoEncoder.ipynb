{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "f = open('resources/InsectWingbeatSound/InsectWingbeatSound_TEST','r')\n",
    "data = f.read()\n",
    "f.close()\n",
    "# 개행문자 기준으로 끊어서 리스트로\n",
    "data_list = data.split('\\n')\n",
    "\n",
    "# \",\" 기준으로 끊어서 리스트로\n",
    "emptylist = []\n",
    "for list_part in data_list:\n",
    "    emptylist.append(list_part.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str -> float 변환\n",
    "tofloat = []\n",
    "for partlist in emptylist:\n",
    "    tofloat.append([float(i) for i in partlist]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980,)\n",
      "(1980, 256)\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "data_list = []\n",
    "for datas in tofloat:\n",
    "    labels.append(datas[0])\n",
    "    data_list.append(datas[1:])\n",
    "print(np.shape(labels))\n",
    "print(np.shape(data_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(max(labels))\n",
    "print(min(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readFile import split_into_values, toRPdata\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "\n",
    "def Standard(data):\n",
    "    SS = StandardScaler().fit(data)\n",
    "    scaled = SS.transform(data)\n",
    "    return scaled\n",
    "\n",
    "def MinMax(data):\n",
    "    MMS = MinMaxScaler().fit(data)\n",
    "    scaled = MMS.transform(data)\n",
    "    return scaled\n",
    "\n",
    "# result_list transpose\n",
    "result_T = [list(x) for x in zip(*data_list)]\n",
    "\n",
    "# minmax 정규화\n",
    "result_scaled = Standard(result_T)\n",
    "\n",
    "# 다시 result transpose 해서 원래대로\n",
    "result_scaled = [list(x) for x in zip(*result_scaled)]\n",
    "\n",
    "result_ = np.array(result_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, 256)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, 256, 256, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = result_.reshape(result_.shape[0], 1, result_.shape[1])\n",
    "X = toRPdata(data, threshold='point', percentage=30)\n",
    "#X = toRPdata(data)\n",
    "    \n",
    "X_scaled = np.expand_dims(X, axis=3)\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22960729610>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-5\n",
    "optimizer='Adam'\n",
    "loss='binary_crossentropy'\n",
    "image_size = 256 #1024, 256\n",
    "dimension = 64 # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 128)     1280      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 256, 256, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 64)      73792     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 32)        18464     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 1)         289       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 8, 8, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 1)           10        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 1)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 256, 256, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 256, 256, 1)       1153      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 256, 256, 1)       0         \n",
      "=================================================================\n",
      "Total params: 187,660\n",
      "Trainable params: 187,660\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5000\n",
      "WARNING:tensorflow:From c:\\users\\mosan\\anaconda3\\envs\\ottflab\\lib\\site-packages\\tensorflow\\python\\data\\ops\\multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
      "INFO:tensorflow:batch_all_reduce: 18 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 18 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.6748INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63062, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 17s 114ms/step - loss: 0.6748 - val_loss: 0.6306\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 2/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.6131\n",
      "Epoch 00002: val_loss improved from 0.63062 to 0.61098, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 89ms/step - loss: 0.6130 - val_loss: 0.6110\n",
      "Epoch 3/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.6108\n",
      "Epoch 00003: val_loss improved from 0.61098 to 0.61065, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 89ms/step - loss: 0.6108 - val_loss: 0.6107\n",
      "Epoch 4/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.6102\n",
      "Epoch 00004: val_loss improved from 0.61065 to 0.61016, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.6102 - val_loss: 0.6102\n",
      "Epoch 5/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.6094\n",
      "Epoch 00005: val_loss improved from 0.61016 to 0.60946, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.6094 - val_loss: 0.6095\n",
      "Epoch 6/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.6078\n",
      "Epoch 00006: val_loss improved from 0.60946 to 0.60161, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 89ms/step - loss: 0.6078 - val_loss: 0.6016\n",
      "Epoch 7/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5902\n",
      "Epoch 00007: val_loss improved from 0.60161 to 0.57860, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 88ms/step - loss: 0.5902 - val_loss: 0.5786\n",
      "Epoch 8/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5740\n",
      "Epoch 00008: val_loss improved from 0.57860 to 0.56850, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 89ms/step - loss: 0.5740 - val_loss: 0.5685\n",
      "Epoch 9/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5661\n",
      "Epoch 00009: val_loss improved from 0.56850 to 0.56315, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 89ms/step - loss: 0.5661 - val_loss: 0.5631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5614\n",
      "Epoch 00010: val_loss improved from 0.56315 to 0.55892, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.5614 - val_loss: 0.5589\n",
      "Epoch 11/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5582\n",
      "Epoch 00011: val_loss improved from 0.55892 to 0.55649, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.5582 - val_loss: 0.5565\n",
      "Epoch 12/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5554\n",
      "Epoch 00012: val_loss improved from 0.55649 to 0.55497, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.5554 - val_loss: 0.5550\n",
      "Epoch 13/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5538\n",
      "Epoch 00013: val_loss improved from 0.55497 to 0.55281, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 89ms/step - loss: 0.5538 - val_loss: 0.5528\n",
      "Epoch 14/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5520\n",
      "Epoch 00014: val_loss improved from 0.55281 to 0.55145, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 88ms/step - loss: 0.5521 - val_loss: 0.5515\n",
      "Epoch 15/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5505\n",
      "Epoch 00015: val_loss improved from 0.55145 to 0.55057, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 88ms/step - loss: 0.5505 - val_loss: 0.5506\n",
      "Epoch 16/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5489\n",
      "Epoch 00016: val_loss did not improve from 0.55057\n",
      "149/149 [==============================] - 13s 88ms/step - loss: 0.5489 - val_loss: 0.5514\n",
      "Epoch 17/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5484\n",
      "Epoch 00017: val_loss improved from 0.55057 to 0.54809, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.5484 - val_loss: 0.5481\n",
      "Epoch 18/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5467\n",
      "Epoch 00018: val_loss improved from 0.54809 to 0.54745, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 89ms/step - loss: 0.5467 - val_loss: 0.5474\n",
      "Epoch 19/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5455\n",
      "Epoch 00019: val_loss improved from 0.54745 to 0.54609, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 88ms/step - loss: 0.5456 - val_loss: 0.5461\n",
      "Epoch 20/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5450\n",
      "Epoch 00020: val_loss improved from 0.54609 to 0.54541, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.5450 - val_loss: 0.5454\n",
      "Epoch 21/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5437\n",
      "Epoch 00021: val_loss did not improve from 0.54541\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5437 - val_loss: 0.5457\n",
      "Epoch 22/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5432\n",
      "Epoch 00022: val_loss improved from 0.54541 to 0.54412, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 103ms/step - loss: 0.5432 - val_loss: 0.5441\n",
      "Epoch 23/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5423\n",
      "Epoch 00023: val_loss improved from 0.54412 to 0.54374, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5423 - val_loss: 0.5437\n",
      "Epoch 24/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5418\n",
      "Epoch 00024: val_loss improved from 0.54374 to 0.54307, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5418 - val_loss: 0.5431\n",
      "Epoch 25/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5409\n",
      "Epoch 00025: val_loss improved from 0.54307 to 0.54244, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5409 - val_loss: 0.5424\n",
      "Epoch 26/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5405\n",
      "Epoch 00026: val_loss improved from 0.54244 to 0.54182, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 97ms/step - loss: 0.5405 - val_loss: 0.5418\n",
      "Epoch 27/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5397\n",
      "Epoch 00027: val_loss improved from 0.54182 to 0.54154, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 97ms/step - loss: 0.5397 - val_loss: 0.5415\n",
      "Epoch 28/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5388\n",
      "Epoch 00028: val_loss improved from 0.54154 to 0.54116, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.5388 - val_loss: 0.5412\n",
      "Epoch 29/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5386\n",
      "Epoch 00029: val_loss improved from 0.54116 to 0.54097, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.5386 - val_loss: 0.5410\n",
      "Epoch 30/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5377\n",
      "Epoch 00030: val_loss improved from 0.54097 to 0.54005, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5377 - val_loss: 0.5400\n",
      "Epoch 31/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5370\n",
      "Epoch 00031: val_loss improved from 0.54005 to 0.53951, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5370 - val_loss: 0.5395\n",
      "Epoch 32/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5364\n",
      "Epoch 00032: val_loss improved from 0.53951 to 0.53897, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5364 - val_loss: 0.5390\n",
      "Epoch 33/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5360\n",
      "Epoch 00033: val_loss improved from 0.53897 to 0.53882, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5359 - val_loss: 0.5388\n",
      "Epoch 34/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5356\n",
      "Epoch 00034: val_loss improved from 0.53882 to 0.53797, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5356 - val_loss: 0.5380\n",
      "Epoch 35/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5348\n",
      "Epoch 00035: val_loss improved from 0.53797 to 0.53748, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5348 - val_loss: 0.5375\n",
      "Epoch 36/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5341\n",
      "Epoch 00036: val_loss did not improve from 0.53748\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5341 - val_loss: 0.5380\n",
      "Epoch 37/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5335\n",
      "Epoch 00037: val_loss improved from 0.53748 to 0.53726, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5335 - val_loss: 0.5373\n",
      "Epoch 38/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5332\n",
      "Epoch 00038: val_loss improved from 0.53726 to 0.53623, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5332 - val_loss: 0.5362\n",
      "Epoch 39/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5326\n",
      "Epoch 00039: val_loss improved from 0.53623 to 0.53565, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 97ms/step - loss: 0.5326 - val_loss: 0.5357\n",
      "Epoch 40/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5318\n",
      "Epoch 00040: val_loss improved from 0.53565 to 0.53527, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5318 - val_loss: 0.5353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5312\n",
      "Epoch 00041: val_loss improved from 0.53527 to 0.53459, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5312 - val_loss: 0.5346\n",
      "Epoch 42/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5310\n",
      "Epoch 00042: val_loss did not improve from 0.53459\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5310 - val_loss: 0.5357\n",
      "Epoch 43/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5303- ETA: 0s - loss: 0.5\n",
      "Epoch 00043: val_loss did not improve from 0.53459\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5303 - val_loss: 0.5348\n",
      "Epoch 44/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5299\n",
      "Epoch 00044: val_loss improved from 0.53459 to 0.53313, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5299 - val_loss: 0.5331\n",
      "Epoch 45/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5290\n",
      "Epoch 00045: val_loss did not improve from 0.53313\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5290 - val_loss: 0.5332\n",
      "Epoch 46/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5283\n",
      "Epoch 00046: val_loss improved from 0.53313 to 0.53235, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5283 - val_loss: 0.5324\n",
      "Epoch 47/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5281\n",
      "Epoch 00047: val_loss did not improve from 0.53235\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5280 - val_loss: 0.5324\n",
      "Epoch 48/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5275\n",
      "Epoch 00048: val_loss improved from 0.53235 to 0.53180, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5275 - val_loss: 0.5318\n",
      "Epoch 49/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5269\n",
      "Epoch 00049: val_loss improved from 0.53180 to 0.53136, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5269 - val_loss: 0.5314\n",
      "Epoch 50/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5265\n",
      "Epoch 00050: val_loss did not improve from 0.53136\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5265 - val_loss: 0.5318\n",
      "Epoch 51/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5261\n",
      "Epoch 00051: val_loss improved from 0.53136 to 0.53042, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5261 - val_loss: 0.5304\n",
      "Epoch 52/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5254\n",
      "Epoch 00052: val_loss improved from 0.53042 to 0.53030, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5254 - val_loss: 0.5303\n",
      "Epoch 53/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5250\n",
      "Epoch 00053: val_loss improved from 0.53030 to 0.52990, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5250 - val_loss: 0.5299\n",
      "Epoch 54/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5243\n",
      "Epoch 00054: val_loss improved from 0.52990 to 0.52946, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5243 - val_loss: 0.5295\n",
      "Epoch 55/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5239\n",
      "Epoch 00055: val_loss improved from 0.52946 to 0.52882, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 97ms/step - loss: 0.5240 - val_loss: 0.5288\n",
      "Epoch 56/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5239\n",
      "Epoch 00056: val_loss improved from 0.52882 to 0.52867, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5238 - val_loss: 0.5287\n",
      "Epoch 57/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5229\n",
      "Epoch 00057: val_loss improved from 0.52867 to 0.52825, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5229 - val_loss: 0.5282\n",
      "Epoch 58/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5226\n",
      "Epoch 00058: val_loss improved from 0.52825 to 0.52796, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5226 - val_loss: 0.5280\n",
      "Epoch 59/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5223\n",
      "Epoch 00059: val_loss improved from 0.52796 to 0.52793, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.5223 - val_loss: 0.5279\n",
      "Epoch 60/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5218\n",
      "Epoch 00060: val_loss improved from 0.52793 to 0.52729, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5219 - val_loss: 0.5273\n",
      "Epoch 61/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.521 - ETA: 0s - loss: 0.5213\n",
      "Epoch 00061: val_loss did not improve from 0.52729\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.5213 - val_loss: 0.5277\n",
      "Epoch 62/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5210\n",
      "Epoch 00062: val_loss improved from 0.52729 to 0.52660, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.5210 - val_loss: 0.5266\n",
      "Epoch 63/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5208\n",
      "Epoch 00063: val_loss improved from 0.52660 to 0.52639, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5208 - val_loss: 0.5264\n",
      "Epoch 64/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5202\n",
      "Epoch 00064: val_loss did not improve from 0.52639\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.5202 - val_loss: 0.5286\n",
      "Epoch 65/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5200\n",
      "Epoch 00065: val_loss improved from 0.52639 to 0.52576, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 102ms/step - loss: 0.5200 - val_loss: 0.5258\n",
      "Epoch 66/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5195\n",
      "Epoch 00066: val_loss did not improve from 0.52576\n",
      "149/149 [==============================] - 15s 102ms/step - loss: 0.5195 - val_loss: 0.5263\n",
      "Epoch 67/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5192\n",
      "Epoch 00067: val_loss improved from 0.52576 to 0.52544, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 102ms/step - loss: 0.5192 - val_loss: 0.5254\n",
      "Epoch 68/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5191\n",
      "Epoch 00068: val_loss improved from 0.52544 to 0.52535, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.5191 - val_loss: 0.5254\n",
      "Epoch 69/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5188\n",
      "Epoch 00069: val_loss improved from 0.52535 to 0.52486, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 102ms/step - loss: 0.5188 - val_loss: 0.5249\n",
      "Epoch 70/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5183- ETA: 1s - \n",
      "Epoch 00070: val_loss did not improve from 0.52486\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.5183 - val_loss: 0.5249\n",
      "Epoch 71/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5181\n",
      "Epoch 00071: val_loss improved from 0.52486 to 0.52444, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5180 - val_loss: 0.5244\n",
      "Epoch 72/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5178\n",
      "Epoch 00072: val_loss improved from 0.52444 to 0.52431, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 97ms/step - loss: 0.5178 - val_loss: 0.5243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5173\n",
      "Epoch 00073: val_loss improved from 0.52431 to 0.52422, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5172 - val_loss: 0.5242\n",
      "Epoch 74/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5171\n",
      "Epoch 00074: val_loss did not improve from 0.52422\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5171 - val_loss: 0.5245\n",
      "Epoch 75/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5170\n",
      "Epoch 00075: val_loss did not improve from 0.52422\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5170 - val_loss: 0.5245\n",
      "Epoch 76/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5167\n",
      "Epoch 00076: val_loss improved from 0.52422 to 0.52366, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5167 - val_loss: 0.5237\n",
      "Epoch 77/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5164\n",
      "Epoch 00077: val_loss did not improve from 0.52366\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5164 - val_loss: 0.5242\n",
      "Epoch 78/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5161\n",
      "Epoch 00078: val_loss improved from 0.52366 to 0.52338, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5161 - val_loss: 0.5234\n",
      "Epoch 79/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5157\n",
      "Epoch 00079: val_loss did not improve from 0.52338\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5157 - val_loss: 0.5237\n",
      "Epoch 80/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5156\n",
      "Epoch 00080: val_loss improved from 0.52338 to 0.52296, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5157 - val_loss: 0.5230\n",
      "Epoch 81/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5152\n",
      "Epoch 00081: val_loss did not improve from 0.52296\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5152 - val_loss: 0.5238\n",
      "Epoch 82/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5153\n",
      "Epoch 00082: val_loss improved from 0.52296 to 0.52290, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5153 - val_loss: 0.5229\n",
      "Epoch 83/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5150\n",
      "Epoch 00083: val_loss did not improve from 0.52290\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5150 - val_loss: 0.5233\n",
      "Epoch 84/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5147\n",
      "Epoch 00084: val_loss did not improve from 0.52290\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5147 - val_loss: 0.5238\n",
      "Epoch 85/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5142\n",
      "Epoch 00085: val_loss did not improve from 0.52290\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5142 - val_loss: 0.5237\n",
      "Epoch 86/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5141\n",
      "Epoch 00086: val_loss improved from 0.52290 to 0.52284, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5141 - val_loss: 0.5228\n",
      "Epoch 87/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5140\n",
      "Epoch 00087: val_loss improved from 0.52284 to 0.52240, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 104ms/step - loss: 0.5140 - val_loss: 0.5224\n",
      "Epoch 88/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5135\n",
      "Epoch 00088: val_loss did not improve from 0.52240\n",
      "149/149 [==============================] - 16s 106ms/step - loss: 0.5135 - val_loss: 0.5224\n",
      "Epoch 89/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5131\n",
      "Epoch 00089: val_loss did not improve from 0.52240\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.5132 - val_loss: 0.5226\n",
      "Epoch 90/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5133\n",
      "Epoch 00090: val_loss did not improve from 0.52240\n",
      "149/149 [==============================] - 15s 97ms/step - loss: 0.5133 - val_loss: 0.5226\n",
      "Epoch 91/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5130\n",
      "Epoch 00091: val_loss improved from 0.52240 to 0.52206, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5130 - val_loss: 0.5221\n",
      "Epoch 92/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5127\n",
      "Epoch 00092: val_loss improved from 0.52206 to 0.52178, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5127 - val_loss: 0.5218\n",
      "Epoch 93/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5126\n",
      "Epoch 00093: val_loss improved from 0.52178 to 0.52169, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5126 - val_loss: 0.5217\n",
      "Epoch 94/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5122\n",
      "Epoch 00094: val_loss did not improve from 0.52169\n",
      "149/149 [==============================] - 15s 103ms/step - loss: 0.5122 - val_loss: 0.5218\n",
      "Epoch 95/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5121\n",
      "Epoch 00095: val_loss improved from 0.52169 to 0.52138, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.5121 - val_loss: 0.5214\n",
      "Epoch 96/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5117\n",
      "Epoch 00096: val_loss improved from 0.52138 to 0.52127, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5117 - val_loss: 0.5213\n",
      "Epoch 97/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5116\n",
      "Epoch 00097: val_loss improved from 0.52127 to 0.52112, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.5116 - val_loss: 0.5211\n",
      "Epoch 98/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5113\n",
      "Epoch 00098: val_loss improved from 0.52112 to 0.52087, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.5113 - val_loss: 0.5209\n",
      "Epoch 99/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5113- ETA:\n",
      "Epoch 00099: val_loss did not improve from 0.52087\n",
      "149/149 [==============================] - 15s 103ms/step - loss: 0.5113 - val_loss: 0.5216\n",
      "Epoch 100/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5109\n",
      "Epoch 00100: val_loss improved from 0.52087 to 0.52044, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.5109 - val_loss: 0.5204\n",
      "Epoch 101/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5108\n",
      "Epoch 00101: val_loss improved from 0.52044 to 0.52028, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 102ms/step - loss: 0.5108 - val_loss: 0.5203\n",
      "Epoch 102/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5104\n",
      "Epoch 00102: val_loss did not improve from 0.52028\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5104 - val_loss: 0.5206\n",
      "Epoch 103/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5106\n",
      "Epoch 00103: val_loss did not improve from 0.52028\n",
      "149/149 [==============================] - 15s 97ms/step - loss: 0.5106 - val_loss: 0.5204\n",
      "Epoch 104/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5100\n",
      "Epoch 00104: val_loss did not improve from 0.52028\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5100 - val_loss: 0.5203\n",
      "Epoch 105/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5101\n",
      "Epoch 00105: val_loss improved from 0.52028 to 0.52009, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5101 - val_loss: 0.5201\n",
      "Epoch 106/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149/149 [==============================] - ETA: 0s - loss: 0.5101\n",
      "Epoch 00106: val_loss did not improve from 0.52009\n",
      "149/149 [==============================] - 16s 105ms/step - loss: 0.5101 - val_loss: 0.5203\n",
      "Epoch 107/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5098\n",
      "Epoch 00107: val_loss improved from 0.52009 to 0.51979, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 103ms/step - loss: 0.5098 - val_loss: 0.5198\n",
      "Epoch 108/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5094\n",
      "Epoch 00108: val_loss did not improve from 0.51979\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5094 - val_loss: 0.5203\n",
      "Epoch 109/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5094\n",
      "Epoch 00109: val_loss did not improve from 0.51979\n",
      "149/149 [==============================] - 16s 104ms/step - loss: 0.5094 - val_loss: 0.5202\n",
      "Epoch 110/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5092\n",
      "Epoch 00110: val_loss did not improve from 0.51979\n",
      "149/149 [==============================] - 15s 103ms/step - loss: 0.5092 - val_loss: 0.5198\n",
      "Epoch 111/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5088\n",
      "Epoch 00111: val_loss did not improve from 0.51979\n",
      "149/149 [==============================] - 15s 102ms/step - loss: 0.5088 - val_loss: 0.5198\n",
      "Epoch 112/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5086\n",
      "Epoch 00112: val_loss did not improve from 0.51979\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5086 - val_loss: 0.5201\n",
      "Epoch 113/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5088\n",
      "Epoch 00113: val_loss improved from 0.51979 to 0.51947, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.5088 - val_loss: 0.5195\n",
      "Epoch 114/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5087\n",
      "Epoch 00114: val_loss did not improve from 0.51947\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.5087 - val_loss: 0.5201\n",
      "Epoch 115/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5083\n",
      "Epoch 00115: val_loss did not improve from 0.51947\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5083 - val_loss: 0.5200\n",
      "Epoch 116/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5081- ETA: 0s - loss: 0.\n",
      "Epoch 00116: val_loss improved from 0.51947 to 0.51918, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5081 - val_loss: 0.5192\n",
      "Epoch 117/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5079\n",
      "Epoch 00117: val_loss did not improve from 0.51918\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5079 - val_loss: 0.5198\n",
      "Epoch 118/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5080\n",
      "Epoch 00118: val_loss improved from 0.51918 to 0.51915, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.5080 - val_loss: 0.5191\n",
      "Epoch 119/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5074\n",
      "Epoch 00119: val_loss did not improve from 0.51915\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5074 - val_loss: 0.5193\n",
      "Epoch 120/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5074\n",
      "Epoch 00120: val_loss did not improve from 0.51915\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5074 - val_loss: 0.5195\n",
      "Epoch 121/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5072\n",
      "Epoch 00121: val_loss did not improve from 0.51915\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5072 - val_loss: 0.5193\n",
      "Epoch 122/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5071\n",
      "Epoch 00122: val_loss did not improve from 0.51915\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5071 - val_loss: 0.5192\n",
      "Epoch 123/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5070\n",
      "Epoch 00123: val_loss improved from 0.51915 to 0.51907, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5070 - val_loss: 0.5191\n",
      "Epoch 124/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5066\n",
      "Epoch 00124: val_loss did not improve from 0.51907\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5066 - val_loss: 0.5196\n",
      "Epoch 125/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5067\n",
      "Epoch 00125: val_loss improved from 0.51907 to 0.51865, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.5067 - val_loss: 0.5186\n",
      "Epoch 126/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5064\n",
      "Epoch 00126: val_loss did not improve from 0.51865\n",
      "149/149 [==============================] - 14s 92ms/step - loss: 0.5064 - val_loss: 0.5195\n",
      "Epoch 127/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5066\n",
      "Epoch 00127: val_loss did not improve from 0.51865\n",
      "149/149 [==============================] - 14s 91ms/step - loss: 0.5066 - val_loss: 0.5189\n",
      "Epoch 128/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5065\n",
      "Epoch 00128: val_loss did not improve from 0.51865\n",
      "149/149 [==============================] - 14s 92ms/step - loss: 0.5065 - val_loss: 0.5201\n",
      "Epoch 129/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5061\n",
      "Epoch 00129: val_loss did not improve from 0.51865\n",
      "149/149 [==============================] - 14s 91ms/step - loss: 0.5061 - val_loss: 0.5187\n",
      "Epoch 130/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5057\n",
      "Epoch 00130: val_loss did not improve from 0.51865\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5057 - val_loss: 0.5189\n",
      "Epoch 131/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5057\n",
      "Epoch 00131: val_loss did not improve from 0.51865\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.5057 - val_loss: 0.5188\n",
      "Epoch 132/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5056\n",
      "Epoch 00132: val_loss improved from 0.51865 to 0.51863, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.5056 - val_loss: 0.5186\n",
      "Epoch 133/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5053\n",
      "Epoch 00133: val_loss did not improve from 0.51863\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.5053 - val_loss: 0.5189\n",
      "Epoch 134/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5054\n",
      "Epoch 00134: val_loss did not improve from 0.51863\n",
      "149/149 [==============================] - 15s 102ms/step - loss: 0.5053 - val_loss: 0.5199\n",
      "Epoch 135/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5053\n",
      "Epoch 00135: val_loss did not improve from 0.51863\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5053 - val_loss: 0.5202\n",
      "Epoch 136/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5050\n",
      "Epoch 00136: val_loss did not improve from 0.51863\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5050 - val_loss: 0.5191\n",
      "Epoch 137/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5051\n",
      "Epoch 00137: val_loss improved from 0.51863 to 0.51824, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.5051 - val_loss: 0.5182\n",
      "Epoch 138/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5052\n",
      "Epoch 00138: val_loss did not improve from 0.51824\n",
      "149/149 [==============================] - 15s 102ms/step - loss: 0.5052 - val_loss: 0.5186\n",
      "Epoch 139/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5045\n",
      "Epoch 00139: val_loss did not improve from 0.51824\n",
      "149/149 [==============================] - 15s 102ms/step - loss: 0.5045 - val_loss: 0.5195\n",
      "Epoch 140/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5045\n",
      "Epoch 00140: val_loss improved from 0.51824 to 0.51814, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 102ms/step - loss: 0.5045 - val_loss: 0.5181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5044\n",
      "Epoch 00141: val_loss did not improve from 0.51814\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5044 - val_loss: 0.5185\n",
      "Epoch 142/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5043\n",
      "Epoch 00142: val_loss improved from 0.51814 to 0.51801, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.5043 - val_loss: 0.5180\n",
      "Epoch 143/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5041\n",
      "Epoch 00143: val_loss did not improve from 0.51801\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5041 - val_loss: 0.5185\n",
      "Epoch 144/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5041\n",
      "Epoch 00144: val_loss did not improve from 0.51801\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5041 - val_loss: 0.5184\n",
      "Epoch 145/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5041\n",
      "Epoch 00145: val_loss did not improve from 0.51801\n",
      "149/149 [==============================] - 15s 97ms/step - loss: 0.5041 - val_loss: 0.5184\n",
      "Epoch 146/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5041\n",
      "Epoch 00146: val_loss did not improve from 0.51801\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5041 - val_loss: 0.5181\n",
      "Epoch 147/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5038\n",
      "Epoch 00147: val_loss did not improve from 0.51801\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5038 - val_loss: 0.5182\n",
      "Epoch 148/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5034\n",
      "Epoch 00148: val_loss did not improve from 0.51801\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5034 - val_loss: 0.5185\n",
      "Epoch 149/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5033\n",
      "Epoch 00149: val_loss did not improve from 0.51801\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5033 - val_loss: 0.5183\n",
      "Epoch 150/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5031\n",
      "Epoch 00150: val_loss did not improve from 0.51801\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5031 - val_loss: 0.5184\n",
      "Epoch 151/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5030\n",
      "Epoch 00151: val_loss did not improve from 0.51801\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.5030 - val_loss: 0.5183\n",
      "Epoch 152/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5028\n",
      "Epoch 00152: val_loss improved from 0.51801 to 0.51783, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 97ms/step - loss: 0.5028 - val_loss: 0.5178\n",
      "Epoch 153/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5030\n",
      "Epoch 00153: val_loss did not improve from 0.51783\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5030 - val_loss: 0.5180\n",
      "Epoch 154/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5029\n",
      "Epoch 00154: val_loss did not improve from 0.51783\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5029 - val_loss: 0.5180\n",
      "Epoch 155/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5025\n",
      "Epoch 00155: val_loss did not improve from 0.51783\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5025 - val_loss: 0.5180\n",
      "Epoch 156/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5025\n",
      "Epoch 00156: val_loss did not improve from 0.51783\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5025 - val_loss: 0.5186\n",
      "Epoch 157/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5025\n",
      "Epoch 00157: val_loss did not improve from 0.51783\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.5025 - val_loss: 0.5182\n",
      "Epoch 158/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5026\n",
      "Epoch 00158: val_loss did not improve from 0.51783\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.5026 - val_loss: 0.5179\n",
      "Epoch 159/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5022\n",
      "Epoch 00159: val_loss did not improve from 0.51783\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.5022 - val_loss: 0.5180\n",
      "Epoch 160/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5020\n",
      "Epoch 00160: val_loss improved from 0.51783 to 0.51767, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5021 - val_loss: 0.5177\n",
      "Epoch 161/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5017\n",
      "Epoch 00161: val_loss did not improve from 0.51767\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5018 - val_loss: 0.5179\n",
      "Epoch 162/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5017\n",
      "Epoch 00162: val_loss did not improve from 0.51767\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5017 - val_loss: 0.5185\n",
      "Epoch 163/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5016\n",
      "Epoch 00163: val_loss did not improve from 0.51767\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.5016 - val_loss: 0.5184\n",
      "Epoch 164/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5015\n",
      "Epoch 00164: val_loss did not improve from 0.51767\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.5015 - val_loss: 0.5184\n",
      "Epoch 165/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5013\n",
      "Epoch 00165: val_loss did not improve from 0.51767\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5013 - val_loss: 0.5182\n",
      "Epoch 166/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5013\n",
      "Epoch 00166: val_loss did not improve from 0.51767\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5013 - val_loss: 0.5177\n",
      "Epoch 167/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5011\n",
      "Epoch 00167: val_loss did not improve from 0.51767\n",
      "149/149 [==============================] - 14s 94ms/step - loss: 0.5011 - val_loss: 0.5181\n",
      "Epoch 168/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5009\n",
      "Epoch 00168: val_loss did not improve from 0.51767\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5009 - val_loss: 0.5191\n",
      "Epoch 169/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5008\n",
      "Epoch 00169: val_loss did not improve from 0.51767\n",
      "149/149 [==============================] - 15s 97ms/step - loss: 0.5008 - val_loss: 0.5179\n",
      "Epoch 170/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5008\n",
      "Epoch 00170: val_loss improved from 0.51767 to 0.51750, saving model to insectWing_dimension_64.h5\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.5008 - val_loss: 0.5175\n",
      "Epoch 171/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5007\n",
      "Epoch 00171: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.5007 - val_loss: 0.5177\n",
      "Epoch 172/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5006\n",
      "Epoch 00172: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.5006 - val_loss: 0.5176\n",
      "Epoch 173/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5005\n",
      "Epoch 00173: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5005 - val_loss: 0.5182\n",
      "Epoch 174/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5002\n",
      "Epoch 00174: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5002 - val_loss: 0.5188\n",
      "Epoch 175/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5003\n",
      "Epoch 00175: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.5004 - val_loss: 0.5179\n",
      "Epoch 176/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.5004\n",
      "Epoch 00176: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.5003 - val_loss: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5000\n",
      "Epoch 00177: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.5000 - val_loss: 0.5180\n",
      "Epoch 178/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4997\n",
      "Epoch 00178: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.4997 - val_loss: 0.5187\n",
      "Epoch 179/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.4998\n",
      "Epoch 00179: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 94ms/step - loss: 0.4998 - val_loss: 0.5184\n",
      "Epoch 180/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.5001\n",
      "Epoch 00180: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.5001 - val_loss: 0.5179\n",
      "Epoch 181/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4997\n",
      "Epoch 00181: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.4997 - val_loss: 0.5186\n",
      "Epoch 182/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4994\n",
      "Epoch 00182: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 13s 89ms/step - loss: 0.4994 - val_loss: 0.5178\n",
      "Epoch 183/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4995\n",
      "Epoch 00183: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.4995 - val_loss: 0.5184\n",
      "Epoch 184/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4992\n",
      "Epoch 00184: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.4992 - val_loss: 0.5185\n",
      "Epoch 185/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4992\n",
      "Epoch 00185: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.4992 - val_loss: 0.5190\n",
      "Epoch 186/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4989\n",
      "Epoch 00186: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.4989 - val_loss: 0.5181\n",
      "Epoch 187/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4989\n",
      "Epoch 00187: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 13s 89ms/step - loss: 0.4989 - val_loss: 0.5182\n",
      "Epoch 188/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.4987\n",
      "Epoch 00188: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.4987 - val_loss: 0.5188\n",
      "Epoch 189/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4987\n",
      "Epoch 00189: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 92ms/step - loss: 0.4987 - val_loss: 0.5178\n",
      "Epoch 190/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.4984\n",
      "Epoch 00190: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 91ms/step - loss: 0.4986 - val_loss: 0.5192\n",
      "Epoch 191/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4987\n",
      "Epoch 00191: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.4987 - val_loss: 0.5184\n",
      "Epoch 192/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4984\n",
      "Epoch 00192: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.4984 - val_loss: 0.5176\n",
      "Epoch 193/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4983\n",
      "Epoch 00193: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 91ms/step - loss: 0.4983 - val_loss: 0.5182\n",
      "Epoch 194/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4983\n",
      "Epoch 00194: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 13s 90ms/step - loss: 0.4983 - val_loss: 0.5176\n",
      "Epoch 195/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4982\n",
      "Epoch 00195: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.4982 - val_loss: 0.5184\n",
      "Epoch 196/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4982\n",
      "Epoch 00196: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 103ms/step - loss: 0.4982 - val_loss: 0.5190\n",
      "Epoch 197/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4980\n",
      "Epoch 00197: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 16s 109ms/step - loss: 0.4980 - val_loss: 0.5177\n",
      "Epoch 198/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4980\n",
      "Epoch 00198: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.4980 - val_loss: 0.5180\n",
      "Epoch 199/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4980\n",
      "Epoch 00199: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.4980 - val_loss: 0.5176\n",
      "Epoch 200/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4974\n",
      "Epoch 00200: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 100ms/step - loss: 0.4974 - val_loss: 0.5183\n",
      "Epoch 201/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.4975\n",
      "Epoch 00201: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 102ms/step - loss: 0.4973 - val_loss: 0.5182\n",
      "Epoch 202/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4977\n",
      "Epoch 00202: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 103ms/step - loss: 0.4977 - val_loss: 0.5182\n",
      "Epoch 203/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4973\n",
      "Epoch 00203: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.4973 - val_loss: 0.5183\n",
      "Epoch 204/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4972\n",
      "Epoch 00204: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.4972 - val_loss: 0.5182\n",
      "Epoch 205/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4974\n",
      "Epoch 00205: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.4974 - val_loss: 0.5179\n",
      "Epoch 206/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.4970\n",
      "Epoch 00206: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.4971 - val_loss: 0.5183\n",
      "Epoch 207/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4968\n",
      "Epoch 00207: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 102ms/step - loss: 0.4968 - val_loss: 0.5186\n",
      "Epoch 208/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4970\n",
      "Epoch 00208: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 99ms/step - loss: 0.4970 - val_loss: 0.5186\n",
      "Epoch 209/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4970\n",
      "Epoch 00209: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 103ms/step - loss: 0.4970 - val_loss: 0.5180\n",
      "Epoch 210/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4968\n",
      "Epoch 00210: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 103ms/step - loss: 0.4968 - val_loss: 0.5187\n",
      "Epoch 211/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4969\n",
      "Epoch 00211: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.4969 - val_loss: 0.5178\n",
      "Epoch 212/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4965\n",
      "Epoch 00212: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 101ms/step - loss: 0.4965 - val_loss: 0.5185\n",
      "Epoch 213/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4965\n",
      "Epoch 00213: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.4965 - val_loss: 0.5185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4962\n",
      "Epoch 00214: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.4962 - val_loss: 0.5189\n",
      "Epoch 215/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4962\n",
      "Epoch 00215: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.4962 - val_loss: 0.5182\n",
      "Epoch 216/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4960\n",
      "Epoch 00216: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.4960 - val_loss: 0.5176\n",
      "Epoch 217/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4966\n",
      "Epoch 00217: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.4966 - val_loss: 0.5190\n",
      "Epoch 218/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4960\n",
      "Epoch 00218: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.4960 - val_loss: 0.5187\n",
      "Epoch 219/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4959\n",
      "Epoch 00219: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.4959 - val_loss: 0.5182\n",
      "Epoch 220/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4958\n",
      "Epoch 00220: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.4958 - val_loss: 0.5183\n",
      "Epoch 221/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4957\n",
      "Epoch 00221: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.4957 - val_loss: 0.5181\n",
      "Epoch 222/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4959\n",
      "Epoch 00222: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.4959 - val_loss: 0.5180\n",
      "Epoch 223/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4960\n",
      "Epoch 00223: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.4960 - val_loss: 0.5180\n",
      "Epoch 224/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4954\n",
      "Epoch 00224: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.4954 - val_loss: 0.5187\n",
      "Epoch 225/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4959\n",
      "Epoch 00225: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.4959 - val_loss: 0.5181\n",
      "Epoch 226/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4952\n",
      "Epoch 00226: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.4952 - val_loss: 0.5184\n",
      "Epoch 227/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.4952\n",
      "Epoch 00227: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.4952 - val_loss: 0.5186\n",
      "Epoch 228/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4951\n",
      "Epoch 00228: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.4951 - val_loss: 0.5185\n",
      "Epoch 229/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4951\n",
      "Epoch 00229: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.4951 - val_loss: 0.5180\n",
      "Epoch 230/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4950\n",
      "Epoch 00230: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 97ms/step - loss: 0.4950 - val_loss: 0.5190\n",
      "Epoch 231/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4948\n",
      "Epoch 00231: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.4948 - val_loss: 0.5183\n",
      "Epoch 232/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.4948\n",
      "Epoch 00232: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 91ms/step - loss: 0.4948 - val_loss: 0.5182\n",
      "Epoch 233/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4948\n",
      "Epoch 00233: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.4948 - val_loss: 0.5188\n",
      "Epoch 234/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4946\n",
      "Epoch 00234: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.4946 - val_loss: 0.5181\n",
      "Epoch 235/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4946\n",
      "Epoch 00235: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.4946 - val_loss: 0.5180\n",
      "Epoch 236/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4947\n",
      "Epoch 00236: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.4947 - val_loss: 0.5200\n",
      "Epoch 237/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4944\n",
      "Epoch 00237: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.4944 - val_loss: 0.5188\n",
      "Epoch 238/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4943\n",
      "Epoch 00238: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.4943 - val_loss: 0.5188\n",
      "Epoch 239/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4946\n",
      "Epoch 00239: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 94ms/step - loss: 0.4946 - val_loss: 0.5180\n",
      "Epoch 240/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4941\n",
      "Epoch 00240: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 17s 111ms/step - loss: 0.4941 - val_loss: 0.5186\n",
      "Epoch 241/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4941\n",
      "Epoch 00241: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 16s 110ms/step - loss: 0.4941 - val_loss: 0.5182\n",
      "Epoch 242/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4943\n",
      "Epoch 00242: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 17s 113ms/step - loss: 0.4943 - val_loss: 0.5182\n",
      "Epoch 243/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4943\n",
      "Epoch 00243: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 16s 109ms/step - loss: 0.4943 - val_loss: 0.5193\n",
      "Epoch 244/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4940\n",
      "Epoch 00244: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 17s 112ms/step - loss: 0.4940 - val_loss: 0.5191\n",
      "Epoch 245/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4936\n",
      "Epoch 00245: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 17s 112ms/step - loss: 0.4936 - val_loss: 0.5186\n",
      "Epoch 246/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4937\n",
      "Epoch 00246: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 16s 110ms/step - loss: 0.4937 - val_loss: 0.5180\n",
      "Epoch 247/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4940\n",
      "Epoch 00247: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 17s 113ms/step - loss: 0.4940 - val_loss: 0.5186\n",
      "Epoch 248/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4934\n",
      "Epoch 00248: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 17s 111ms/step - loss: 0.4934 - val_loss: 0.5191\n",
      "Epoch 249/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4935\n",
      "Epoch 00249: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 16s 109ms/step - loss: 0.4935 - val_loss: 0.5187\n",
      "Epoch 250/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4934\n",
      "Epoch 00250: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 16s 110ms/step - loss: 0.4934 - val_loss: 0.5192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4932\n",
      "Epoch 00251: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 16s 110ms/step - loss: 0.4932 - val_loss: 0.5190\n",
      "Epoch 252/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4932\n",
      "Epoch 00252: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 17s 111ms/step - loss: 0.4932 - val_loss: 0.5188\n",
      "Epoch 253/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4931- ETA: 1s \n",
      "Epoch 00253: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 16s 110ms/step - loss: 0.4931 - val_loss: 0.5185\n",
      "Epoch 254/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4932\n",
      "Epoch 00254: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 17s 112ms/step - loss: 0.4932 - val_loss: 0.5187\n",
      "Epoch 255/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4934\n",
      "Epoch 00255: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 17s 112ms/step - loss: 0.4934 - val_loss: 0.5187\n",
      "Epoch 256/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4931\n",
      "Epoch 00256: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 16s 110ms/step - loss: 0.4931 - val_loss: 0.5191\n",
      "Epoch 257/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4928\n",
      "Epoch 00257: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 17s 112ms/step - loss: 0.4928 - val_loss: 0.5192\n",
      "Epoch 258/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4930\n",
      "Epoch 00258: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 17s 111ms/step - loss: 0.4930 - val_loss: 0.5185\n",
      "Epoch 259/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4931\n",
      "Epoch 00259: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 17s 112ms/step - loss: 0.4931 - val_loss: 0.5191\n",
      "Epoch 260/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4928\n",
      "Epoch 00260: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 16s 110ms/step - loss: 0.4928 - val_loss: 0.5188\n",
      "Epoch 261/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4926\n",
      "Epoch 00261: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 17s 111ms/step - loss: 0.4926 - val_loss: 0.5192\n",
      "Epoch 262/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4926\n",
      "Epoch 00262: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.4926 - val_loss: 0.5184\n",
      "Epoch 263/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4925\n",
      "Epoch 00263: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.4925 - val_loss: 0.5185\n",
      "Epoch 264/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4922\n",
      "Epoch 00264: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.4922 - val_loss: 0.5188\n",
      "Epoch 265/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4926- ETA: 0s - loss: 0.49\n",
      "Epoch 00265: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 15s 98ms/step - loss: 0.4926 - val_loss: 0.5190\n",
      "Epoch 266/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4921- E\n",
      "Epoch 00266: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.4921 - val_loss: 0.5190\n",
      "Epoch 267/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.4921\n",
      "Epoch 00267: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 95ms/step - loss: 0.4921 - val_loss: 0.5191\n",
      "Epoch 268/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.4923\n",
      "Epoch 00268: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 94ms/step - loss: 0.4922 - val_loss: 0.5197\n",
      "Epoch 269/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4924- ET\n",
      "Epoch 00269: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 97ms/step - loss: 0.4924 - val_loss: 0.5190\n",
      "Epoch 270/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.4919\n",
      "Epoch 00270: val_loss did not improve from 0.51750\n",
      "149/149 [==============================] - 14s 96ms/step - loss: 0.4919 - val_loss: 0.5192\n",
      "Epoch 00270: early stopping\n"
     ]
    }
   ],
   "source": [
    "from utils import split_data, normalization_tool\n",
    "from agent import Autoencoder_Agent\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "X_train, X_test, Y_train, Y_test = split_data(X_scaled, X_scaled) #데이터 분리\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"], cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "with mirrored_strategy.scope():\n",
    "    autoencoder = Autoencoder_Agent(model_size=image_size, dimension=dimension, optimizer=optimizer,learning_rate=learning_rate)\n",
    "    hist = autoencoder.train(X_train,batch_size,epochs,X_test)\n",
    "#     feature = autoencoder.feature_extract(X_scaled)\n",
    "#     print(feature)\n",
    "#     print(feature.shape)\n",
    "# with tf.device('/cpu:0'):\n",
    "#     autoencoder = Autoencoder_Agent(model_size=image_size, dimension=dimension, optimizer=optimizer,learning_rate=learning_rate)\n",
    "#     hist = autoencoder.train(X_train,batch_size,epochs,X_test)\n",
    "#     feature = autoencoder.feature_extract(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01763297 0.         0.         ... 0.05126805 0.10514829 0.1502502 ]\n",
      " [0.0025138  0.         0.         ... 0.06736211 0.05718368 0.09721424]\n",
      " [0.0305176  0.         0.04115462 ... 0.07261652 0.07220548 0.06760643]\n",
      " ...\n",
      " [0.04467504 0.00881677 0.         ... 0.08224609 0.1502502  0.1502502 ]\n",
      " [0.         0.         0.         ... 0.13210015 0.1502502  0.1502502 ]\n",
      " [0.08075318 0.00418469 0.01429312 ... 0.0385666  0.06604405 0.05031005]]\n",
      "(1980, 64)\n"
     ]
    }
   ],
   "source": [
    "it = int(1980 / 30) \n",
    "all_feature = np.array(autoencoder.feature_extract(X_scaled[0:30]))\n",
    "for i in range(1, it):\n",
    "    feature = autoencoder.feature_extract(X_scaled[i*30:(i+1)*30])\n",
    "    all_feature = np.concatenate([all_feature, feature])\n",
    "print(all_feature)\n",
    "print(all_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "# loss_ax.plot([hist['loss'][i] - hist['val_loss'][i] for i in range(len(hist['loss']))], 'g', label='loss - val loss')\n",
    "\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11: 180\n",
      "3: 180\n",
      "6: 180\n",
      "1: 180\n",
      "5: 180\n",
      "4: 180\n",
      "2: 180\n",
      "10: 180\n",
      "7: 180\n",
      "9: 180\n",
      "8: 180\n"
     ]
    }
   ],
   "source": [
    "dic_labels = {}\n",
    "for i in labels:\n",
    "    if f'{i}' not in dic_labels.keys():\n",
    "        dic_labels[f'{i}'] = 1\n",
    "    else:\n",
    "        dic_labels[f'{i}'] += 1\n",
    "\n",
    "for key, value in dic_labels.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: 87\n",
      "9: 135\n",
      "8: 514\n",
      "3: 122\n",
      "5: 248\n",
      "6: 164\n",
      "1: 126\n",
      "10: 104\n",
      "2: 223\n",
      "11: 160\n",
      "7: 97\n"
     ]
    }
   ],
   "source": [
    "labels_ = cluster_result.labels_ + 1\n",
    "dic_labels_ = {}\n",
    "for i in labels_:\n",
    "    if f'{i}' not in dic_labels_.keys():\n",
    "        dic_labels_[f'{i}'] = 1\n",
    "    else:\n",
    "        dic_labels_[f'{i}'] += 1\n",
    "\n",
    "for key, value in dic_labels_.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  2  9  9  2  9  9  2  2  9  2  9  2  9  9  2  9 10  2  2  9  5  9  9\n",
      "  9  5 10  2  5  9  5  2  2  2  2  2  9  2  9  2  2  9  3  2  0  2  9  2\n",
      "  2  2  9  9  9  2  5  9  9  9  2  9  2  2  2  9  9  9  2  2  9  2 10  9\n",
      "  9  1  9  2  9  2  9  9  2  9  9 10  9  2  2  9  9 10 10  9  9  9 10  2\n",
      "  9  2  2  9  2  2  9  1  2  9  2  9  1  2  2  2  9 10  2  1  2  2 10 10\n",
      "  2  9  2  2  2  2  5  2  9 10  9  2  2  9  2  2  2  2  9  5  9  2  2  2\n",
      "  9  9  9  9  2  2  2  9 10  9  2  9  9  2  9 10  2  9  9  9  2  2  9  2\n",
      "  9  2  9  9  9  9  9  9  9  2  9  1]\n",
      "[ 0  4  0  4  6  4  3  4  8  6  7  0  4  8  3  4  3  6  8  7  4  7  8  0\n",
      "  8  7  3  0  0  8  4  8  3  7  4  0  3  8  1  3  4  4  6  4  8  3  4  0\n",
      "  6  8  8  3  6  3  4  8  5  8  8  7  8  4  7  8  3  6  4  3  6  0  6  7\n",
      "  8  8  4  4  4  9  7  4  3  8  8  8  0  7  4  4  1  8  6  4  8  4  8  8\n",
      "  8  4  6  3  7  0  6  4  7  6  4  8  4  0  4  4  8  4  4  3  0  7  4  6\n",
      "  8  4  3  4  7  7  4  6  8  0  0  8  4  4  7  4  8  4 10  7  8  8  4  7\n",
      "  6  8  0  4  4  6  0  8  7  6  8  8  4  8  3  0  7  3  4  4  8  0  4  7\n",
      "  6  8  7  8  8  4  8  0  6  4  0  6]\n",
      "[8 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 7 4 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 5 7 4 4 7 4 1 7 7 7 7 7 7 7 7 1 7 3 7 7 7 7 7 7 7 7 6 7 7 7 7 7 7 7 4 4 7\n",
      " 4 7 7 7 7 7 7 4 6 7 8 7 7 7 7 5 7 7 7 4 7 7 7 7 7 4 1 7 7 7 4 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 4 7 7 1 7 7 7 7 4 7 7 7 7 7 7 7 8 4 4 7 7 7 7 7 7 7\n",
      " 7 0 7 5 7 1 7 3 7 7 4 7 7 7 7 3 5 7 7 7 4 3 7 7 7 5 7 5 7 7 7 7]\n",
      "[4 4 7 7 4 7 0 7 4 7 4 3 4 4 0 7 4 4 6 4 4 7 7 4 4 4 4 4 4 4 7 9 4 3 4 4 7\n",
      " 7 4 4 4 8 4 4 4 7 4 3 8 4 4 7 7 4 7 4 4 3 7 4 4 7 6 3 7 3 4 3 4 9 4 4 8 4\n",
      " 4 7 7 0 4 7 4 7 8 1 4 7 7 3 8 4 7 7 3 4 7 4 4 1 7 4 4 4 4 4 4 4 4 7 4 4 4\n",
      " 4 4 7 7 3 7 4 4 7 7 4 7 8 4 7 4 4 3 7 7 7 7 4 7 7 7 7 4 6 4 0 4 7 4 7 7 7\n",
      " 7 7 4 4 4 4 7 7 3 4 7 7 4 3 0 4 7 3 7 4 7 7 4 4 4 4 4 7 7 7 7 3]\n",
      "[ 5  9  5  5  1  2  5  1  5  1 10  5  5  1  1  1  1 10  5  1  5 10 10  5\n",
      "  1  2 10  5 10  1 10  5  5  1  5 10  7 10  5 10  1 10  5  1  1 10 10  1\n",
      "  2  5 10  1 10 10  1  9  5 10  2 10  5  1 10 10  5 10 10  5 10  1  1  5\n",
      " 10  5  1  5  1  7  5 10  5  5  5  1  5  9  7  1 10  5 10  5 10 10  5 10\n",
      "  2  7 10  1  5  5  5 10  5  0  5 10  5  9  9  1 10  5  5  5 10  5 10  5\n",
      "  5  8  5  5  1  1  6  5  5  1  1  1  5  5 10 10  5  1  5  5  4  5  5 10\n",
      " 10  5  5 10  1  5  1  1 10 10  5  1  5  1  5  5 10  5  0 10  5  5  5  5\n",
      " 10  1  5  5  1  0 10 10  5  1  1 10]\n",
      "[7 4 7 4 7 4 8 7 5 7 1 7 7 4 7 1 4 7 7 5 3 7 7 7 7 4 7 7 7 7 7 3 7 7 7 7 7\n",
      " 4 7 7 7 4 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 4 7 7 7 7 7 7 4 7 7 7 7 7 7 7 7 8\n",
      " 4 7 3 4 7 7 7 7 7 7 7 7 7 7 3 7 7 7 7 7 7 9 7 7 7 9 5 7 7 7 7 7 7 7 7 7 4\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 4 7 7 7 7 1 7 7 7 7 7 5 7 7 7 0 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 4 7 4 7 7 5 4 7 7 1 3 4 7 7 7 7 9 4 4 7 7 4 7]\n",
      "[ 1 10  5  2  1  1 10  1  1 10  1 10  1 10  1  1  5  1  1  7  1 10 10  5\n",
      "  1  1  1 10 10  1 10  9 10  1  1  1 10  1  1  7 10  7 10  5  1  2  1 10\n",
      "  5  7  7  1  1  1  1  1  5  1  1 10  1  1 10 10  1 10 10  9  1  1  1  1\n",
      "  1 10  1  1  1 10  1  1  1  1 10  1  2  5  1  5  1  1  1  1  1  1  3  1\n",
      "  1  1  1  2 10 10  1  1  1  1  1 10  5  2 10  1 10  1  2 10  1 10  1 10\n",
      "  7  1 10  1  1  1  1 10  1  1  5  1  1  1 10  1  1  1  1  1  1 10  3  1\n",
      "  1  1  1 10  1 10  2  1  5  1  1  1  1  1 10 10  1  1  6  1  5  1  1 10\n",
      " 10  1  1  5  1  1  9 10  1  1 10  1]\n",
      "[7 7 1 7 7 5 7 7 7 1 7 7 7 7 7 7 7 7 7 4 7 1 7 2 7 1 7 7 7 7 1 7 7 7 7 7 7\n",
      " 7 7 7 7 7 1 1 7 2 7 7 7 7 1 1 5 7 7 7 7 7 7 7 4 5 7 5 7 7 7 1 7 8 7 7 7 7\n",
      " 7 7 7 1 7 7 1 7 7 7 4 7 4 7 4 7 7 1 7 7 7 3 7 7 7 1 7 7 8 7 1 7 7 1 7 7 7\n",
      " 7 7 7 7 7 1 7 7 7 7 7 1 7 1 7 7 7 7 1 2 7 4 7 7 2 1 7 7 7 1 7 7 7 7 7 7 7\n",
      " 7 7 7 9 5 7 1 1 7 7 4 7 7 7 1 7 7 1 7 7 2 7 7 1 7 7 7 7 7 7 1 1]\n",
      "[ 2  2  5 10 10  1  1 10  5 10  5  3 10  2 10  1 10 10  9  2 10  2  5  9\n",
      "  2  5 10  2  5 10 10  5 10  1  2  5  7  9  1 10  2  5 10 10  5  2  5 10\n",
      "  1 10  5  2  5  5  1 10  5  5  1 10  5 10  1  2  3  2  2  1 10  1  2  5\n",
      "  1 10  5  9  2  5  5  9  1  1  2  1 10  5  3  0 10  1  5 10  5  5  2  1\n",
      "  9  5  3  9 10  7  3  5  5 10 10  1  5  2  9  2 10 10  5  2 10 10  1  2\n",
      " 10  5  5  1 10  5  5  5  1  5  5  3 10  1 10 10  1  5 10  5 10  5  1  2\n",
      "  5  1 10  1  2  5  2  5  5  5  2 10 10  5 10  5  2  9  1 10  5  1  1 10\n",
      " 10 10  5  5 10  5  5 10 10  5 10 10]\n",
      "[3 9 8 4 6 4 8 4 3 5 8 0 4 4 4 8 4 0 8 8 4 4 4 3 0 8 6 8 4 8 8 8 4 8 3 4 7\n",
      " 4 8 4 4 8 4 6 4 5 4 8 8 8 9 8 8 4 4 4 8 8 4 7 6 8 4 8 7 3 9 8 4 6 8 4 6 4\n",
      " 8 8 4 8 0 3 4 3 8 8 4 0 0 3 4 4 3 4 4 4 8 3 8 4 8 8 0 8 4 8 3 8 6 8 4 3 6\n",
      " 4 4 6 4 4 0 8 4 4 0 3 7 8 3 4 4 0 8 8 8 4 4 4 4 4 8 4 0 8 8 7 6 4 3 4 4 0\n",
      " 4 4 3 4 8 0 8 0 8 6 8 3 3 0 0 4 7 3 6 8 6 3 8 4 8 4 0 6 4 8 8 4]\n",
      "[ 3  8  3  6  6  0  0  6  6  6  0  8  0  0  6  6  0  0  0  0  6  6  0  0\n",
      "  8  6  6  8  0  0  0  6  0  3  6  7  0  0  3  6  6  6  4  6  0  5  6  3\n",
      "  0  4  6  6  0  0  3  0  8  0  0  0  6  0  3  0  8  0  6  0  0  0  4  0\n",
      "  8  6  0  0  8  5  4  6  6  6  6  0  6  3  3  0  0  6  0  0  0  8  0  6\n",
      "  0  0  8  6  1  0  8  0  6  6  0  6  0  3  0  6  8  0  0  0  6  0  0  8\n",
      "  0  0  8  6  2  6  6  6  8  0  6  6  0  0  0  3 10  0  0  8  6  6  2  0\n",
      "  3  0  6  6  0  0  6  6  7  3  8  6  6  6  8  8  6  0  6  0  8  6  0  8\n",
      "  0  0  0  3  3  0  6  0  0  8  0  0]\n"
     ]
    }
   ],
   "source": [
    "# 각 label 별 리스트 나누기\n",
    "# labels : 기존 라벨\n",
    "# all_feature : dataset\n",
    "\n",
    "# labels == 1 인 인덱스 : idx\n",
    "# cluster_1 = all_feature[idx]\n",
    "\n",
    "for i in range(1, 12):\n",
    "#     features = np.empty((1,32))\n",
    "    indexes = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == i:\n",
    "            indexes.append(idx)\n",
    "    features = all_feature[indexes]\n",
    "    result = cluster_result.predict(features)\n",
    "    print(result)\n",
    "#     print(features)\n",
    "#     print(features.shape)\n",
    "\n",
    "# 리스트 별로 predict 진행\n",
    "# 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Cluster Algorithm\n",
    "cluster_result = KMeans(n_clusters=11).fit(all_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16142237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "def plotSilhouette(X, y_km):\n",
    "    cluster_labels = np.unique(y_km.labels_)\n",
    "    n_clusters = cluster_labels.shape[0]\n",
    "    silhouette_vals = silhouette_score(X, y_km.labels_,metric='euclidean')\n",
    "    print(silhouette_vals)\n",
    "    \n",
    "plotSilhouette(all_feature,cluster_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6,)\n",
      "[1 2 3 4 5 6]\n",
      "---------------------------\n",
      "(2, 3)\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "---------------------------\n",
      "(1, 6)\n",
      "[[1 2 3 4 5 6]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-165def55aeb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mtest1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_feature\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 0 and the array at index 1 has size 32"
     ]
    }
   ],
   "source": [
    "array4 = array1.reshape(1, 3)  # [[1,2,3]]\n",
    "array5 = array2.reshape(1, 3)  # [[4,5,6]]\n",
    "\n",
    "# 2차원 배열을 위아래로 합치기\n",
    "\n",
    "array6 = np.concatenate([array4, array5], axis=0)\n",
    "print(array6.shape)\n",
    "print(array6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
