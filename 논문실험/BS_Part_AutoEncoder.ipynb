{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "f = open('resources/InsectWingbeatSound/InsectWingbeatSound_TEST','r')\n",
    "data = f.read()\n",
    "f.close()\n",
    "# 개행문자 기준으로 끊어서 리스트로\n",
    "data_list = data.split('\\n')\n",
    "\n",
    "# \",\" 기준으로 끊어서 리스트로\n",
    "emptylist = []\n",
    "for list_part in data_list:\n",
    "    emptylist.append(list_part.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str -> float 변환\n",
    "tofloat = []\n",
    "for partlist in emptylist:\n",
    "    tofloat.append([float(i) for i in partlist]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980,)\n",
      "(1980, 256)\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "data_list = []\n",
    "for datas in tofloat:\n",
    "    labels.append(datas[0])\n",
    "    data_list.append(datas[1:])\n",
    "print(np.shape(labels))\n",
    "print(np.shape(data_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from readFile import split_into_values, toRPdata\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "\n",
    "def Standard(data):\n",
    "    SS = StandardScaler().fit(data)\n",
    "    scaled = SS.transform(data)\n",
    "    return scaled\n",
    "\n",
    "def MinMax(data):\n",
    "    MMS = MinMaxScaler().fit(data)\n",
    "    scaled = MMS.transform(data)\n",
    "    return scaled\n",
    "\n",
    "# result_list transpose\n",
    "result_T = [list(x) for x in zip(*data_list)]\n",
    "\n",
    "# minmax 정규화\n",
    "result_scaled = Standard(result_T)\n",
    "\n",
    "# 다시 result transpose 해서 원래대로\n",
    "result_scaled = [list(x) for x in zip(*result_scaled)]\n",
    "\n",
    "result_ = np.array(result_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, 256)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1980, 256, 256, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = result_.reshape(result_.shape[0], 1, result_.shape[1])\n",
    "X = toRPdata(data, threshold='point', percentage=30)\n",
    "#X = toRPdata(data)\n",
    "    \n",
    "X_scaled = np.expand_dims(X, axis=3)\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x242a13c0828>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-5\n",
    "optimizer='Adam'\n",
    "loss='mse'\n",
    "image_size = 256 #1024, 256\n",
    "dimension = 8 # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 256, 256, 64)      640       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 128, 128, 32)      18464     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 64, 64, 16)        4624      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 32, 32, 2)         290       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 32, 32, 2)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 2, 2, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 2, 2, 2)           38        \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 2, 2, 2)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 32, 32, 2)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 16)        304       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 64, 64, 32)        4640      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 128, 128, 64)      18496     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 256, 256, 1)       577       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 256, 256, 1)       0         \n",
      "=================================================================\n",
      "Total params: 48,073\n",
      "Trainable params: 48,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from utils import split_data, normalization_tool\n",
    "from agent import Autoencoder_Agent\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_data(X_scaled, X_scaled) #데이터 분리\n",
    "\n",
    "autoencoder = Autoencoder_Agent(model_size=image_size, dimension=dimension, optimizer=optimizer,learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "149/149 [==============================] - ETA: 0s - loss: 0.2234\n",
      "Epoch 00001: val_loss improved from inf to 0.20606, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 13s 84ms/step - loss: 0.2234 - val_loss: 0.2061\n",
      "Epoch 2/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.2016\n",
      "Epoch 00002: val_loss improved from 0.20606 to 0.20045, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.2016 - val_loss: 0.2005\n",
      "Epoch 3/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1992\n",
      "Epoch 00003: val_loss improved from 0.20045 to 0.19897, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1993 - val_loss: 0.1990\n",
      "Epoch 4/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1978\n",
      "Epoch 00004: val_loss improved from 0.19897 to 0.19793, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1979 - val_loss: 0.1979\n",
      "Epoch 5/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1968\n",
      "Epoch 00005: val_loss improved from 0.19793 to 0.19679, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1968 - val_loss: 0.1968\n",
      "Epoch 6/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1957\n",
      "Epoch 00006: val_loss improved from 0.19679 to 0.19605, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1957 - val_loss: 0.1961\n",
      "Epoch 7/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1949\n",
      "Epoch 00007: val_loss improved from 0.19605 to 0.19530, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1950 - val_loss: 0.1953\n",
      "Epoch 8/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1945\n",
      "Epoch 00008: val_loss improved from 0.19530 to 0.19488, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1944 - val_loss: 0.1949\n",
      "Epoch 9/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1941\n",
      "Epoch 00009: val_loss improved from 0.19488 to 0.19455, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1940 - val_loss: 0.1945\n",
      "Epoch 10/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1937\n",
      "Epoch 00010: val_loss improved from 0.19455 to 0.19432, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1937 - val_loss: 0.1943\n",
      "Epoch 11/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1934- ETA: 0s - loss: 0.1\n",
      "Epoch 00011: val_loss improved from 0.19432 to 0.19407, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1935 - val_loss: 0.1941\n",
      "Epoch 12/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1933\n",
      "Epoch 00012: val_loss improved from 0.19407 to 0.19389, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1933 - val_loss: 0.1939\n",
      "Epoch 13/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1931\n",
      "Epoch 00013: val_loss improved from 0.19389 to 0.19373, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1931 - val_loss: 0.1937\n",
      "Epoch 14/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1929\n",
      "Epoch 00014: val_loss improved from 0.19373 to 0.19368, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1930 - val_loss: 0.1937\n",
      "Epoch 15/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1927\n",
      "Epoch 00015: val_loss improved from 0.19368 to 0.19349, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1928 - val_loss: 0.1935\n",
      "Epoch 16/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1926\n",
      "Epoch 00016: val_loss improved from 0.19349 to 0.19337, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1926 - val_loss: 0.1934\n",
      "Epoch 17/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1925\n",
      "Epoch 00017: val_loss improved from 0.19337 to 0.19326, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1925 - val_loss: 0.1933\n",
      "Epoch 18/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1925\n",
      "Epoch 00018: val_loss improved from 0.19326 to 0.19312, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1924 - val_loss: 0.1931\n",
      "Epoch 19/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1923\n",
      "Epoch 00019: val_loss improved from 0.19312 to 0.19307, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1923 - val_loss: 0.1931\n",
      "Epoch 20/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1922\n",
      "Epoch 00020: val_loss improved from 0.19307 to 0.19304, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1922 - val_loss: 0.1930\n",
      "Epoch 21/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1921\n",
      "Epoch 00021: val_loss improved from 0.19304 to 0.19286, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1921 - val_loss: 0.1929\n",
      "Epoch 22/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1920\n",
      "Epoch 00022: val_loss improved from 0.19286 to 0.19280, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1920 - val_loss: 0.1928\n",
      "Epoch 23/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1919\n",
      "Epoch 00023: val_loss did not improve from 0.19280\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1919 - val_loss: 0.1930\n",
      "Epoch 24/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1919\n",
      "Epoch 00024: val_loss improved from 0.19280 to 0.19265, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1918 - val_loss: 0.1926\n",
      "Epoch 25/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1918\n",
      "Epoch 00025: val_loss improved from 0.19265 to 0.19257, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1918 - val_loss: 0.1926\n",
      "Epoch 26/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1918\n",
      "Epoch 00026: val_loss improved from 0.19257 to 0.19250, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1917 - val_loss: 0.1925\n",
      "Epoch 27/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1916\n",
      "Epoch 00027: val_loss did not improve from 0.19250\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1916 - val_loss: 0.1925\n",
      "Epoch 28/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1915\n",
      "Epoch 00028: val_loss improved from 0.19250 to 0.19244, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1915 - val_loss: 0.1924\n",
      "Epoch 29/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1914\n",
      "Epoch 00029: val_loss improved from 0.19244 to 0.19238, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1915 - val_loss: 0.1924\n",
      "Epoch 30/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1914\n",
      "Epoch 00030: val_loss improved from 0.19238 to 0.19229, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1914 - val_loss: 0.1923\n",
      "Epoch 31/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1914\n",
      "Epoch 00031: val_loss improved from 0.19229 to 0.19226, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1914 - val_loss: 0.1923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1913\n",
      "Epoch 00032: val_loss improved from 0.19226 to 0.19225, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 79ms/step - loss: 0.1913 - val_loss: 0.1923\n",
      "Epoch 33/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1912\n",
      "Epoch 00033: val_loss improved from 0.19225 to 0.19213, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1912 - val_loss: 0.1921\n",
      "Epoch 34/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1912\n",
      "Epoch 00034: val_loss improved from 0.19213 to 0.19211, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1912 - val_loss: 0.1921\n",
      "Epoch 35/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1911\n",
      "Epoch 00035: val_loss improved from 0.19211 to 0.19206, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1911 - val_loss: 0.1921\n",
      "Epoch 36/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1911\n",
      "Epoch 00036: val_loss improved from 0.19206 to 0.19202, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1911 - val_loss: 0.1920\n",
      "Epoch 37/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1911\n",
      "Epoch 00037: val_loss improved from 0.19202 to 0.19197, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1910 - val_loss: 0.1920\n",
      "Epoch 38/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1910\n",
      "Epoch 00038: val_loss improved from 0.19197 to 0.19195, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1910 - val_loss: 0.1920\n",
      "Epoch 39/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1909\n",
      "Epoch 00039: val_loss improved from 0.19195 to 0.19189, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1909 - val_loss: 0.1919\n",
      "Epoch 40/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1908- ETA: 0s - loss:\n",
      "Epoch 00040: val_loss did not improve from 0.19189\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1908 - val_loss: 0.1920\n",
      "Epoch 41/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1908\n",
      "Epoch 00041: val_loss did not improve from 0.19189\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1908 - val_loss: 0.1919\n",
      "Epoch 42/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1908- ETA: 1s - los - ETA: 0s - lo\n",
      "Epoch 00042: val_loss improved from 0.19189 to 0.19184, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1908 - val_loss: 0.1918\n",
      "Epoch 43/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1907\n",
      "Epoch 00043: val_loss improved from 0.19184 to 0.19180, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1907 - val_loss: 0.1918\n",
      "Epoch 44/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1908\n",
      "Epoch 00044: val_loss improved from 0.19180 to 0.19175, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1907 - val_loss: 0.1918\n",
      "Epoch 45/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1907\n",
      "Epoch 00045: val_loss improved from 0.19175 to 0.19168, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1906 - val_loss: 0.1917\n",
      "Epoch 46/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1906\n",
      "Epoch 00046: val_loss did not improve from 0.19168\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1906 - val_loss: 0.1917\n",
      "Epoch 47/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1906\n",
      "Epoch 00047: val_loss improved from 0.19168 to 0.19163, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1906 - val_loss: 0.1916\n",
      "Epoch 48/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1905\n",
      "Epoch 00048: val_loss improved from 0.19163 to 0.19159, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1905 - val_loss: 0.1916\n",
      "Epoch 49/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1904\n",
      "Epoch 00049: val_loss improved from 0.19159 to 0.19156, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1905 - val_loss: 0.1916\n",
      "Epoch 50/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1904\n",
      "Epoch 00050: val_loss improved from 0.19156 to 0.19154, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1904 - val_loss: 0.1915\n",
      "Epoch 51/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1905\n",
      "Epoch 00051: val_loss improved from 0.19154 to 0.19152, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1904 - val_loss: 0.1915\n",
      "Epoch 52/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1903\n",
      "Epoch 00052: val_loss improved from 0.19152 to 0.19151, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1904 - val_loss: 0.1915\n",
      "Epoch 53/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1903\n",
      "Epoch 00053: val_loss improved from 0.19151 to 0.19148, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1903 - val_loss: 0.1915\n",
      "Epoch 54/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1903\n",
      "Epoch 00054: val_loss improved from 0.19148 to 0.19147, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1903 - val_loss: 0.1915\n",
      "Epoch 55/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1902\n",
      "Epoch 00055: val_loss improved from 0.19147 to 0.19141, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1903 - val_loss: 0.1914\n",
      "Epoch 56/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1902\n",
      "Epoch 00056: val_loss did not improve from 0.19141\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1902 - val_loss: 0.1915\n",
      "Epoch 57/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1901\n",
      "Epoch 00057: val_loss improved from 0.19141 to 0.19135, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1902 - val_loss: 0.1914\n",
      "Epoch 58/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1901\n",
      "Epoch 00058: val_loss improved from 0.19135 to 0.19134, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1902 - val_loss: 0.1913\n",
      "Epoch 59/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1901- ETA: 0s - loss\n",
      "Epoch 00059: val_loss improved from 0.19134 to 0.19131, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1901 - val_loss: 0.1913\n",
      "Epoch 60/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1901\n",
      "Epoch 00060: val_loss improved from 0.19131 to 0.19129, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1901 - val_loss: 0.1913\n",
      "Epoch 61/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1901\n",
      "Epoch 00061: val_loss improved from 0.19129 to 0.19128, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1901 - val_loss: 0.1913\n",
      "Epoch 62/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1900\n",
      "Epoch 00062: val_loss improved from 0.19128 to 0.19123, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1900 - val_loss: 0.1912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1900\n",
      "Epoch 00063: val_loss did not improve from 0.19123\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1900 - val_loss: 0.1913\n",
      "Epoch 64/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1900\n",
      "Epoch 00064: val_loss improved from 0.19123 to 0.19121, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1900 - val_loss: 0.1912\n",
      "Epoch 65/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1900\n",
      "Epoch 00065: val_loss improved from 0.19121 to 0.19118, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1900 - val_loss: 0.1912\n",
      "Epoch 66/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1899\n",
      "Epoch 00066: val_loss did not improve from 0.19118\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1899 - val_loss: 0.1912\n",
      "Epoch 67/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1899\n",
      "Epoch 00067: val_loss improved from 0.19118 to 0.19117, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1899 - val_loss: 0.1912\n",
      "Epoch 68/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1899\n",
      "Epoch 00068: val_loss improved from 0.19117 to 0.19113, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1899 - val_loss: 0.1911\n",
      "Epoch 69/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1899\n",
      "Epoch 00069: val_loss improved from 0.19113 to 0.19111, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1898 - val_loss: 0.1911\n",
      "Epoch 70/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1899\n",
      "Epoch 00070: val_loss improved from 0.19111 to 0.19111, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1898 - val_loss: 0.1911\n",
      "Epoch 71/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1898- ETA: 1\n",
      "Epoch 00071: val_loss improved from 0.19111 to 0.19106, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1898 - val_loss: 0.1911\n",
      "Epoch 72/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1897\n",
      "Epoch 00072: val_loss did not improve from 0.19106\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1898 - val_loss: 0.1912\n",
      "Epoch 73/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1898\n",
      "Epoch 00073: val_loss did not improve from 0.19106\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1897 - val_loss: 0.1911\n",
      "Epoch 74/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1898\n",
      "Epoch 00074: val_loss did not improve from 0.19106\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1897 - val_loss: 0.1911\n",
      "Epoch 75/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1897\n",
      "Epoch 00075: val_loss did not improve from 0.19106\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1897 - val_loss: 0.1911\n",
      "Epoch 76/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1896\n",
      "Epoch 00076: val_loss improved from 0.19106 to 0.19098, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1897 - val_loss: 0.1910\n",
      "Epoch 77/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1896\n",
      "Epoch 00077: val_loss did not improve from 0.19098\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1896 - val_loss: 0.1910\n",
      "Epoch 78/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1896\n",
      "Epoch 00078: val_loss improved from 0.19098 to 0.19095, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1896 - val_loss: 0.1909\n",
      "Epoch 79/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1896\n",
      "Epoch 00079: val_loss improved from 0.19095 to 0.19094, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1896 - val_loss: 0.1909\n",
      "Epoch 80/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1896\n",
      "Epoch 00080: val_loss improved from 0.19094 to 0.19091, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1896 - val_loss: 0.1909\n",
      "Epoch 81/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1895\n",
      "Epoch 00081: val_loss did not improve from 0.19091\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1895 - val_loss: 0.1910\n",
      "Epoch 82/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1895\n",
      "Epoch 00082: val_loss improved from 0.19091 to 0.19088, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1895 - val_loss: 0.1909\n",
      "Epoch 83/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1895\n",
      "Epoch 00083: val_loss did not improve from 0.19088\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1895 - val_loss: 0.1910\n",
      "Epoch 84/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1894\n",
      "Epoch 00084: val_loss did not improve from 0.19088\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1895 - val_loss: 0.1909\n",
      "Epoch 85/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1894\n",
      "Epoch 00085: val_loss improved from 0.19088 to 0.19086, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1895 - val_loss: 0.1909\n",
      "Epoch 86/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1896\n",
      "Epoch 00086: val_loss improved from 0.19086 to 0.19082, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1894 - val_loss: 0.1908\n",
      "Epoch 87/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1894\n",
      "Epoch 00087: val_loss improved from 0.19082 to 0.19080, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1894 - val_loss: 0.1908\n",
      "Epoch 88/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1894\n",
      "Epoch 00088: val_loss did not improve from 0.19080\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1894 - val_loss: 0.1908\n",
      "Epoch 89/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1894\n",
      "Epoch 00089: val_loss did not improve from 0.19080\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1894 - val_loss: 0.1909\n",
      "Epoch 90/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1893- ETA\n",
      "Epoch 00090: val_loss did not improve from 0.19080\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1894 - val_loss: 0.1908\n",
      "Epoch 91/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1893\n",
      "Epoch 00091: val_loss improved from 0.19080 to 0.19077, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1893 - val_loss: 0.1908\n",
      "Epoch 92/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1893\n",
      "Epoch 00092: val_loss improved from 0.19077 to 0.19075, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1893 - val_loss: 0.1907\n",
      "Epoch 93/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1893\n",
      "Epoch 00093: val_loss improved from 0.19075 to 0.19074, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1893 - val_loss: 0.1907\n",
      "Epoch 94/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1893\n",
      "Epoch 00094: val_loss did not improve from 0.19074\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1893 - val_loss: 0.1908\n",
      "Epoch 95/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1892- ETA: 0s - loss:\n",
      "Epoch 00095: val_loss did not improve from 0.19074\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1892 - val_loss: 0.1908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1892\n",
      "Epoch 00096: val_loss did not improve from 0.19074\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1892 - val_loss: 0.1908\n",
      "Epoch 97/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1893\n",
      "Epoch 00097: val_loss did not improve from 0.19074\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1893 - val_loss: 0.1908\n",
      "Epoch 98/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1892\n",
      "Epoch 00098: val_loss improved from 0.19074 to 0.19072, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1892 - val_loss: 0.1907\n",
      "Epoch 99/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1891- ETA: 0s - loss: 0.1\n",
      "Epoch 00099: val_loss improved from 0.19072 to 0.19068, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1892 - val_loss: 0.1907\n",
      "Epoch 100/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1892\n",
      "Epoch 00100: val_loss did not improve from 0.19068\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1892 - val_loss: 0.1907\n",
      "Epoch 101/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1892\n",
      "Epoch 00101: val_loss did not improve from 0.19068\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1892 - val_loss: 0.1907\n",
      "Epoch 102/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1891\n",
      "Epoch 00102: val_loss did not improve from 0.19068\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1891 - val_loss: 0.1907\n",
      "Epoch 103/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1891\n",
      "Epoch 00103: val_loss did not improve from 0.19068\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1891 - val_loss: 0.1907\n",
      "Epoch 104/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1891\n",
      "Epoch 00104: val_loss did not improve from 0.19068\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1891 - val_loss: 0.1908\n",
      "Epoch 105/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1893\n",
      "Epoch 00105: val_loss improved from 0.19068 to 0.19061, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1891 - val_loss: 0.1906\n",
      "Epoch 106/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1891\n",
      "Epoch 00106: val_loss did not improve from 0.19061\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1891 - val_loss: 0.1907\n",
      "Epoch 107/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1891\n",
      "Epoch 00107: val_loss did not improve from 0.19061\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1891 - val_loss: 0.1906\n",
      "Epoch 108/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1890\n",
      "Epoch 00108: val_loss did not improve from 0.19061\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1891 - val_loss: 0.1906\n",
      "Epoch 109/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1890\n",
      "Epoch 00109: val_loss improved from 0.19061 to 0.19061, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1890 - val_loss: 0.1906\n",
      "Epoch 110/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1890\n",
      "Epoch 00110: val_loss did not improve from 0.19061\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1890 - val_loss: 0.1906\n",
      "Epoch 111/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1890\n",
      "Epoch 00111: val_loss did not improve from 0.19061\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1890 - val_loss: 0.1907\n",
      "Epoch 112/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1890\n",
      "Epoch 00112: val_loss did not improve from 0.19061\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1890 - val_loss: 0.1906\n",
      "Epoch 113/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1890\n",
      "Epoch 00113: val_loss did not improve from 0.19061\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1890 - val_loss: 0.1909\n",
      "Epoch 114/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1890\n",
      "Epoch 00114: val_loss improved from 0.19061 to 0.19059, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1889 - val_loss: 0.1906\n",
      "Epoch 115/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1889\n",
      "Epoch 00115: val_loss did not improve from 0.19059\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1889 - val_loss: 0.1907\n",
      "Epoch 116/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1889\n",
      "Epoch 00116: val_loss did not improve from 0.19059\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1889 - val_loss: 0.1906\n",
      "Epoch 117/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1889\n",
      "Epoch 00117: val_loss improved from 0.19059 to 0.19057, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1889 - val_loss: 0.1906\n",
      "Epoch 118/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1889\n",
      "Epoch 00118: val_loss improved from 0.19057 to 0.19057, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1889 - val_loss: 0.1906\n",
      "Epoch 119/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1888- ETA\n",
      "Epoch 00119: val_loss improved from 0.19057 to 0.19053, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1889 - val_loss: 0.1905\n",
      "Epoch 120/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00120: val_loss did not improve from 0.19053\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1889 - val_loss: 0.1907\n",
      "Epoch 121/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00121: val_loss did not improve from 0.19053\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1888 - val_loss: 0.1906\n",
      "Epoch 122/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1889\n",
      "Epoch 00122: val_loss improved from 0.19053 to 0.19051, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1888 - val_loss: 0.1905\n",
      "Epoch 123/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1889\n",
      "Epoch 00123: val_loss did not improve from 0.19051\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1888 - val_loss: 0.1906\n",
      "Epoch 124/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00124: val_loss did not improve from 0.19051\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1888 - val_loss: 0.1906\n",
      "Epoch 125/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00125: val_loss did not improve from 0.19051\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1888 - val_loss: 0.1906\n",
      "Epoch 126/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00126: val_loss did not improve from 0.19051\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1888 - val_loss: 0.1906\n",
      "Epoch 127/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00127: val_loss did not improve from 0.19051\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1888 - val_loss: 0.1905\n",
      "Epoch 128/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00128: val_loss did not improve from 0.19051\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1887 - val_loss: 0.1906\n",
      "Epoch 129/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00129: val_loss did not improve from 0.19051\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1887 - val_loss: 0.1905\n",
      "Epoch 130/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00130: val_loss improved from 0.19051 to 0.19051, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1887 - val_loss: 0.1905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00131: val_loss did not improve from 0.19051\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1887 - val_loss: 0.1906\n",
      "Epoch 132/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00132: val_loss did not improve from 0.19051\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1887 - val_loss: 0.1906\n",
      "Epoch 133/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00133: val_loss did not improve from 0.19051\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1887 - val_loss: 0.1905\n",
      "Epoch 134/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00134: val_loss improved from 0.19051 to 0.19048, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1887 - val_loss: 0.1905\n",
      "Epoch 135/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00135: val_loss did not improve from 0.19048\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1886 - val_loss: 0.1905\n",
      "Epoch 136/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00136: val_loss did not improve from 0.19048\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1887 - val_loss: 0.1905\n",
      "Epoch 137/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00137: val_loss did not improve from 0.19048\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1886 - val_loss: 0.1905\n",
      "Epoch 138/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00138: val_loss did not improve from 0.19048\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1886 - val_loss: 0.1906\n",
      "Epoch 139/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00139: val_loss did not improve from 0.19048\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1886 - val_loss: 0.1905\n",
      "Epoch 140/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00140: val_loss improved from 0.19048 to 0.19048, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1886 - val_loss: 0.1905\n",
      "Epoch 141/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00141: val_loss improved from 0.19048 to 0.19045, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1886 - val_loss: 0.1905\n",
      "Epoch 142/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1887\n",
      "Epoch 00142: val_loss did not improve from 0.19045\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1886 - val_loss: 0.1905\n",
      "Epoch 143/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00143: val_loss did not improve from 0.19045\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1886 - val_loss: 0.1906\n",
      "Epoch 144/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00144: val_loss did not improve from 0.19045\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1886 - val_loss: 0.1905\n",
      "Epoch 145/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1885\n",
      "Epoch 00145: val_loss improved from 0.19045 to 0.19045, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1885 - val_loss: 0.1904\n",
      "Epoch 146/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1885\n",
      "Epoch 00146: val_loss did not improve from 0.19045\n",
      "149/149 [==============================] - 12s 79ms/step - loss: 0.1885 - val_loss: 0.1905\n",
      "Epoch 147/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00147: val_loss did not improve from 0.19045\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1885 - val_loss: 0.1906\n",
      "Epoch 148/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1886\n",
      "Epoch 00148: val_loss improved from 0.19045 to 0.19044, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1885 - val_loss: 0.1904\n",
      "Epoch 149/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1885\n",
      "Epoch 00149: val_loss improved from 0.19044 to 0.19043, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1885 - val_loss: 0.1904\n",
      "Epoch 150/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1885\n",
      "Epoch 00150: val_loss did not improve from 0.19043\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1885 - val_loss: 0.1904\n",
      "Epoch 151/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1885\n",
      "Epoch 00151: val_loss improved from 0.19043 to 0.19040, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1885 - val_loss: 0.1904\n",
      "Epoch 152/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1884\n",
      "Epoch 00152: val_loss did not improve from 0.19040\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1885 - val_loss: 0.1905\n",
      "Epoch 153/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1885\n",
      "Epoch 00153: val_loss did not improve from 0.19040\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1885 - val_loss: 0.1904\n",
      "Epoch 154/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1884\n",
      "Epoch 00154: val_loss did not improve from 0.19040\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1884 - val_loss: 0.1904\n",
      "Epoch 155/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1884\n",
      "Epoch 00155: val_loss did not improve from 0.19040\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1885 - val_loss: 0.1905\n",
      "Epoch 156/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1884\n",
      "Epoch 00156: val_loss did not improve from 0.19040\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1884 - val_loss: 0.1904\n",
      "Epoch 157/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1885\n",
      "Epoch 00157: val_loss did not improve from 0.19040\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1884 - val_loss: 0.1904\n",
      "Epoch 158/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1884\n",
      "Epoch 00158: val_loss did not improve from 0.19040\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1884 - val_loss: 0.1905\n",
      "Epoch 159/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1885\n",
      "Epoch 00159: val_loss improved from 0.19040 to 0.19039, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1884 - val_loss: 0.1904\n",
      "Epoch 160/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1884\n",
      "Epoch 00160: val_loss did not improve from 0.19039\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1884 - val_loss: 0.1905\n",
      "Epoch 161/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1884\n",
      "Epoch 00161: val_loss did not improve from 0.19039\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1884 - val_loss: 0.1904\n",
      "Epoch 162/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00162: val_loss did not improve from 0.19039\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1884 - val_loss: 0.1905\n",
      "Epoch 163/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1883- ETA: 1\n",
      "Epoch 00163: val_loss did not improve from 0.19039\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1883 - val_loss: 0.1904\n",
      "Epoch 164/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00164: val_loss did not improve from 0.19039\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1883 - val_loss: 0.1904\n",
      "Epoch 165/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1884\n",
      "Epoch 00165: val_loss improved from 0.19039 to 0.19039, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1883 - val_loss: 0.1904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1884\n",
      "Epoch 00166: val_loss did not improve from 0.19039\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1883 - val_loss: 0.1904\n",
      "Epoch 167/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00167: val_loss did not improve from 0.19039\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1883 - val_loss: 0.1905\n",
      "Epoch 168/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00168: val_loss improved from 0.19039 to 0.19038, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1883 - val_loss: 0.1904\n",
      "Epoch 169/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00169: val_loss did not improve from 0.19038\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1883 - val_loss: 0.1904\n",
      "Epoch 170/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00170: val_loss did not improve from 0.19038\n",
      "149/149 [==============================] - 12s 79ms/step - loss: 0.1883 - val_loss: 0.1905\n",
      "Epoch 171/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00171: val_loss improved from 0.19038 to 0.19038, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1883 - val_loss: 0.1904\n",
      "Epoch 172/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00172: val_loss did not improve from 0.19038\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1883 - val_loss: 0.1905\n",
      "Epoch 173/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00173: val_loss did not improve from 0.19038\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1883 - val_loss: 0.1904\n",
      "Epoch 174/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00174: val_loss did not improve from 0.19038\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1883 - val_loss: 0.1904\n",
      "Epoch 175/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00175: val_loss did not improve from 0.19038\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1883 - val_loss: 0.1906\n",
      "Epoch 176/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00176: val_loss did not improve from 0.19038\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1882 - val_loss: 0.1904\n",
      "Epoch 177/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00177: val_loss did not improve from 0.19038\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1882 - val_loss: 0.1904\n",
      "Epoch 178/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00178: val_loss did not improve from 0.19038\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1882 - val_loss: 0.1904\n",
      "Epoch 179/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00179: val_loss did not improve from 0.19038\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1882 - val_loss: 0.1904\n",
      "Epoch 180/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00180: val_loss did not improve from 0.19038\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1882 - val_loss: 0.1905\n",
      "Epoch 181/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00181: val_loss did not improve from 0.19038\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1882 - val_loss: 0.1904\n",
      "Epoch 182/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00182: val_loss did not improve from 0.19038\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1882 - val_loss: 0.1904\n",
      "Epoch 183/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00183: val_loss improved from 0.19038 to 0.19034, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1882 - val_loss: 0.1903\n",
      "Epoch 184/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1881\n",
      "Epoch 00184: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1881 - val_loss: 0.1904\n",
      "Epoch 185/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1882\n",
      "Epoch 00185: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1881 - val_loss: 0.1904\n",
      "Epoch 186/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1881\n",
      "Epoch 00186: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1881 - val_loss: 0.1904\n",
      "Epoch 187/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1881\n",
      "Epoch 00187: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1881 - val_loss: 0.1904\n",
      "Epoch 188/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1881\n",
      "Epoch 00188: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1881 - val_loss: 0.1904\n",
      "Epoch 189/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1881\n",
      "Epoch 00189: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1881 - val_loss: 0.1904\n",
      "Epoch 190/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00190: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1881 - val_loss: 0.1904\n",
      "Epoch 191/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1881\n",
      "Epoch 00191: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1881 - val_loss: 0.1904\n",
      "Epoch 192/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1881\n",
      "Epoch 00192: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1881 - val_loss: 0.1904\n",
      "Epoch 193/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00193: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1880 - val_loss: 0.1904\n",
      "Epoch 194/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1881\n",
      "Epoch 00194: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1881 - val_loss: 0.1904\n",
      "Epoch 195/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1881\n",
      "Epoch 00195: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1880 - val_loss: 0.1904\n",
      "Epoch 196/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00196: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1880 - val_loss: 0.1904\n",
      "Epoch 197/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00197: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1880 - val_loss: 0.1904\n",
      "Epoch 198/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00198: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1880 - val_loss: 0.1904\n",
      "Epoch 199/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00199: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1880 - val_loss: 0.1904\n",
      "Epoch 200/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00200: val_loss did not improve from 0.19034\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1880 - val_loss: 0.1904\n",
      "Epoch 201/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00201: val_loss improved from 0.19034 to 0.19033, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1880 - val_loss: 0.1903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00202: val_loss did not improve from 0.19033\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1880 - val_loss: 0.1905\n",
      "Epoch 203/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00203: val_loss did not improve from 0.19033\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1880 - val_loss: 0.1904\n",
      "Epoch 204/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00204: val_loss did not improve from 0.19033\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1880 - val_loss: 0.1904\n",
      "Epoch 205/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00205: val_loss improved from 0.19033 to 0.19031, saving model to insectWing_dimension_8.h5\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1879 - val_loss: 0.1903\n",
      "Epoch 206/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00206: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1879 - val_loss: 0.1904\n",
      "Epoch 207/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00207: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1879 - val_loss: 0.1904\n",
      "Epoch 208/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1880- ETA: 0s - \n",
      "Epoch 00208: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1879 - val_loss: 0.1903\n",
      "Epoch 209/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00209: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1879 - val_loss: 0.1904\n",
      "Epoch 210/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00210: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1879 - val_loss: 0.1904\n",
      "Epoch 211/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00211: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1879 - val_loss: 0.1904\n",
      "Epoch 212/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1880\n",
      "Epoch 00212: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1879 - val_loss: 0.1904\n",
      "Epoch 213/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00213: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1879 - val_loss: 0.1905\n",
      "Epoch 214/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00214: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1879 - val_loss: 0.1904\n",
      "Epoch 215/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00215: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1879 - val_loss: 0.1904\n",
      "Epoch 216/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00216: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1879 - val_loss: 0.1904\n",
      "Epoch 217/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00217: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1879 - val_loss: 0.1904\n",
      "Epoch 218/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00218: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1878 - val_loss: 0.1903\n",
      "Epoch 219/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00219: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1878 - val_loss: 0.1904\n",
      "Epoch 220/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00220: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1878 - val_loss: 0.1904\n",
      "Epoch 221/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00221: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1878 - val_loss: 0.1904\n",
      "Epoch 222/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00222: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1878 - val_loss: 0.1904\n",
      "Epoch 223/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00223: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 75ms/step - loss: 0.1878 - val_loss: 0.1904\n",
      "Epoch 224/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00224: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1878 - val_loss: 0.1904\n",
      "Epoch 225/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1879\n",
      "Epoch 00225: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1878 - val_loss: 0.1904\n",
      "Epoch 226/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00226: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 75ms/step - loss: 0.1878 - val_loss: 0.1904\n",
      "Epoch 227/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00227: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1878 - val_loss: 0.1904\n",
      "Epoch 228/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00228: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1878 - val_loss: 0.1904\n",
      "Epoch 229/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00229: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 75ms/step - loss: 0.1877 - val_loss: 0.1904\n",
      "Epoch 230/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00230: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 75ms/step - loss: 0.1877 - val_loss: 0.1905\n",
      "Epoch 231/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00231: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 75ms/step - loss: 0.1877 - val_loss: 0.1903\n",
      "Epoch 232/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00232: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 75ms/step - loss: 0.1877 - val_loss: 0.1904\n",
      "Epoch 233/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00233: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1877 - val_loss: 0.1904\n",
      "Epoch 234/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00234: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1877 - val_loss: 0.1904\n",
      "Epoch 235/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00235: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1877 - val_loss: 0.1904\n",
      "Epoch 236/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00236: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1877 - val_loss: 0.1904\n",
      "Epoch 237/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00237: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1877 - val_loss: 0.1904\n",
      "Epoch 238/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1878\n",
      "Epoch 00238: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1877 - val_loss: 0.1904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00239: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1877 - val_loss: 0.1904\n",
      "Epoch 240/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00240: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1877 - val_loss: 0.1905\n",
      "Epoch 241/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00241: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1877 - val_loss: 0.1905\n",
      "Epoch 242/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00242: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1876 - val_loss: 0.1904\n",
      "Epoch 243/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00243: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1876 - val_loss: 0.1905\n",
      "Epoch 244/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00244: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1876 - val_loss: 0.1905\n",
      "Epoch 245/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00245: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1876 - val_loss: 0.1904\n",
      "Epoch 246/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00246: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1876 - val_loss: 0.1904\n",
      "Epoch 247/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00247: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 12s 77ms/step - loss: 0.1876 - val_loss: 0.1904\n",
      "Epoch 248/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00248: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1876 - val_loss: 0.1904\n",
      "Epoch 249/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00249: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1876 - val_loss: 0.1905\n",
      "Epoch 250/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00250: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1876 - val_loss: 0.1904\n",
      "Epoch 251/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00251: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1876 - val_loss: 0.1905\n",
      "Epoch 252/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1875\n",
      "Epoch 00252: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1875 - val_loss: 0.1904\n",
      "Epoch 253/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00253: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1876 - val_loss: 0.1905\n",
      "Epoch 254/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1875\n",
      "Epoch 00254: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1875 - val_loss: 0.1904\n",
      "Epoch 255/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1875\n",
      "Epoch 00255: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1875 - val_loss: 0.1905\n",
      "Epoch 256/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1875\n",
      "Epoch 00256: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1875 - val_loss: 0.1904\n",
      "Epoch 257/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1875\n",
      "Epoch 00257: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1875 - val_loss: 0.1905\n",
      "Epoch 258/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1875- ETA: 0s - loss: \n",
      "Epoch 00258: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1875 - val_loss: 0.1904\n",
      "Epoch 259/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00259: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1875 - val_loss: 0.1905\n",
      "Epoch 260/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00260: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1875 - val_loss: 0.1905\n",
      "Epoch 261/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00261: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1875 - val_loss: 0.1904\n",
      "Epoch 262/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00262: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1875 - val_loss: 0.1904\n",
      "Epoch 263/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874-\n",
      "Epoch 00263: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1874 - val_loss: 0.1904\n",
      "Epoch 264/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00264: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 12s 78ms/step - loss: 0.1874 - val_loss: 0.1904\n",
      "Epoch 265/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1875\n",
      "Epoch 00265: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1874 - val_loss: 0.1905\n",
      "Epoch 266/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874- ETA: 0s - loss: \n",
      "Epoch 00266: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1874 - val_loss: 0.1904\n",
      "Epoch 267/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00267: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1874 - val_loss: 0.1904\n",
      "Epoch 268/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00268: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1874 - val_loss: 0.1905\n",
      "Epoch 269/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00269: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1874 - val_loss: 0.1904\n",
      "Epoch 270/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1876\n",
      "Epoch 00270: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1874 - val_loss: 0.1905\n",
      "Epoch 271/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00271: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1874 - val_loss: 0.1905\n",
      "Epoch 272/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00272: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1874 - val_loss: 0.1904\n",
      "Epoch 273/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00273: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 76ms/step - loss: 0.1874 - val_loss: 0.1906\n",
      "Epoch 274/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00274: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1874 - val_loss: 0.1904\n",
      "Epoch 275/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1875\n",
      "Epoch 00275: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1874 - val_loss: 0.1905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00276: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1874 - val_loss: 0.1905\n",
      "Epoch 277/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00277: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1873 - val_loss: 0.1904\n",
      "Epoch 278/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00278: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1873 - val_loss: 0.1905\n",
      "Epoch 279/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00279: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1873 - val_loss: 0.1905\n",
      "Epoch 280/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00280: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1873 - val_loss: 0.1906\n",
      "Epoch 281/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1874\n",
      "Epoch 00281: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1873 - val_loss: 0.1905\n",
      "Epoch 282/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00282: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1873 - val_loss: 0.1905\n",
      "Epoch 283/5000\n",
      "148/149 [============================>.] - ETA: 0s - loss: 0.1873\n",
      "Epoch 00283: val_loss did not improve from 0.19031\n",
      "149/149 [==============================] - 11s 77ms/step - loss: 0.1873 - val_loss: 0.1905\n",
      "Epoch 00283: early stopping\n"
     ]
    }
   ],
   "source": [
    "hist = autoencoder.train(X_train,batch_size,epochs,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "# loss_ax.plot([hist['loss'][i] - hist['val_loss'][i] for i in range(len(hist['loss']))], 'g', label='loss - val loss')\n",
    "\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1980, 8)\n"
     ]
    }
   ],
   "source": [
    "features = np.empty((0,8), float)\n",
    "for i in range(66):\n",
    "    features = np.append(features, autoencoder.feature_extract(X_scaled[i*30:(i+1)*30]), axis=0)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "result = KMeans(n_clusters=11).fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 10  1  4  4  5  1  1  8  4  9  2  3  7  0  5  1  7  2 10  4  4 10  2\n",
      "  0  4  1  4  4  7  0  0  4  4  0  0  2  4  1  5  3  9  8  4  0  1  0  9\n",
      "  7 10  4  9  9  3  4  4  0  0  1  5  4  4  0 10  7  6  6  4  8  0  2  2\n",
      "  4  5  0  0  6  5  5 10  1  9  0  0  1  4  1  1  1  4  4  4  7  4  0  4\n",
      "  4  1  1  6  1  1  3 10  4  0  4  4  5  4  4  4  6  0  0  5  5  2  8  0\n",
      "  4  9  2  4  2  1 10  2  5  4  3  3 10  9  2  5  0  4 10  0  2  0  0  2\n",
      "  4  9  7  8  1  0  0  7  9  4  2  1  3  7  1  5  0  4  9  9  9  4  3  3\n",
      "  2  2  9 10 10  5  4  0  5  4  2  4  9  0  1  4  0  6 10  4  6  4  1  0\n",
      "  6  1  4  0  1  9  5  6  0 10  0  4  4  6  4  0  9  0 10  1  4  4  1 10\n",
      " 10  9  4  1  4  7  2 10  5  4  2  3  9  3  2 10  0  2  4  1  1  9  4  3\n",
      "  1  0  9  6  6 10  9 10  0  5  4  0  4  7  1 10  3  0  9  1  3  1  4  9\n",
      "  1  2  4  2  0  0  8 10  1  4  4  4  7  9 10  0  4  1  9  5  0  4  1  0\n",
      "  8 10  4  4  4  4  1  9  0  0  1  9  4  9  3  1  1  6  5  5  1  1  9  4\n",
      "  1  4  9  5  5  1  9  1  1  9  4  1  6 10  1  2  0  5  0 10  7 10  0  1\n",
      " 10  4  1  5  5  5  1  1  3  4  7  4  4  9 10  4 10  4  4 10  4  9  0  4\n",
      "  4  0 10  7  1  3  9  4  9  4  4  4  4  4  4 10  4  4  0  1  9  2  0  4\n",
      "  0 10  5  5  2  4  4  4  4  4  5  2  1  4  9  4  5  5  5  1  4  5  0  0\n",
      "  9  7  6  4  0  0  9  3  8  6  3  3  1  1  0  9  0 10 10  8 10  1  1 10\n",
      "  1  4  9  9  9  4  3  5 10  3 10  4  2 10  4  6  9  4  9  9  4  4  4  3\n",
      "  5  9 10  1  4  1  3  1  4  1  3  0  1  4  4  4 10  1  0  8  4  1  1  1\n",
      "  4  1 10  4 10  4  1  3  1 10  0  2  5  3  0 10  9  0  1  6  4  5  1  0\n",
      "  0  8  2  9  4  1  7  5  1  5  4  4  0  3  4  0 10  9 10  3  9  4  0  9\n",
      "  9  1  0  4  1  0  5  4  1 10 10  7  3  6  5  1  8  0  0  1  7 10  9  1\n",
      "  9  4  4  2  0  0  0  0  3  4  0  0  0  2  3  4  0  7  1  7  5  5  6  1\n",
      " 10  9  5  9  9  1  9  1 10  4  4  9  4 10  9  4 10  5  4  4  4  0  7  2\n",
      " 10  0  0  0  3  5  4  1  1  0  4  0 10  9  4  7  4  4  1  4  9  1  1  1\n",
      "  4  1  0 10  7  0  1  5  5  0  4  9  4  2  0  5  4  4  4  4  3  1  0  2\n",
      "  3  2  1  4 10  4  2  1  4  8  5  1  7  3  1  1  9  9  0  0  4  8  9  3\n",
      "  1  0  4  5  4  9  0  0  4  4  1  0  4  4  2  8  9  0  4  3  4  2  1  4\n",
      "  4  8  3  1  1 10  8  0  7  1  0 10  9  4  7  6  5  0  4  1  5  4  1  1\n",
      "  2  1  3  0  0  4  0  4  4 10  4  2  4  4  4  4  1  4  9 10  9  9  2  5\n",
      "  4  4  4  1  1  9  1  5 10  4  4  1  3  2 10  1  1  4  9  1  0  9  0  3\n",
      "  9  0  5  7  1  1  8  4  3  6  9  4  9 10  5  4  5  6 10  9  8  2  4  1\n",
      "  6  4  1  4  8  4 10  2 10  4 10  2  1  7  6  1  9  0  1  3  0  7  1  3\n",
      "  1  0  1  4  0  3  5  7  0  8  9  1  5 10  4  4  1 10  5  4  4  4  4  4\n",
      "  1  3  5 10  8  6  5  7  4  0  4  0  1  5  4  3 10  0  4 10  4  0 10  7\n",
      "  5  0  0  3  0  4  4  4  1  5  0  5  6  0 10  2  1  9  0  2 10  0  1 10\n",
      "  0  4  6  9 10 10  3 10  4  4  3  1  8  2  1  1  4  9  1  4  1  9  0  7\n",
      "  0  5  5  1  0  9  0  4 10  3  0  4 10  7  4  3  3  8  1  2  5  9 10  4\n",
      "  2  0  4  5 10  4  3  9  4  3 10  9  9  8  4  0  1  1  4  1  0  3  9  3\n",
      " 10  0  1  0  0  1  4  4  3  0  0  4  5  3  3  3  0  0  4  4  4  1  4  2\n",
      "  3  3  4  0  1  9  5 10  1  0  1  2  0  1  0  0  3  2  2  0  1  2  9  0\n",
      " 10  0  9  4 10  2  2  4  0  4  0  0  1  7  1  0  2  2  0  7  4  9  4  4\n",
      "  3  5  0 10  9  5  1  0  2  4  5  1  2 10  2  4  4  0  1  1 10  8  1 10\n",
      "  9  3 10  4 10  4  9 10  8 10  0  0 10  1  3  1  5  9  1  1  4  7  2  1\n",
      "  2  9  1  2  0  2  7  3  0  0  9  3 10  0  6  9  6  1  0  3  0  9  1  1\n",
      "  1  3  9  1  4  0  4  1  5  1  1  9  5  4  5  9  3  0  4  1  4  4  9  3\n",
      "  5  3  1  0  5  4 10  2  1  4  7  0  4 10  3  6  4  4  5  0 10  1  3  3\n",
      "  6  0  1  7  0  4  3 10  5  5  4  4  0  1  4  2  8  4  4 10  1  0  0  0\n",
      "  9 10  5  1 10  3  1  9  2  4  0  4  5  8  5 10  1  0  4  1  3  2  9  7\n",
      "  2  4  1 10  4  4  4  4  3  1  1 10  9  9  0  1  9  7  0  8  4  4  0  4\n",
      "  2  4  4  4  2  1 10  4  4 10 10  4  7  4  9  0  0  8  4  4  4  1  1  3\n",
      "  9  4  2  3 10  0  0  3  4  4  2  1  4  2  3  9  1  3  1  2  9  7  1  4\n",
      "  4  1  9  6  9  1  4  9  4  5  0  9  5  3  4  0  4  4  4  4  1  3 10  1\n",
      "  4  2  3  2  4  3  2  6  1 10  4  4  3  1  3 10  1  1  2  2  9  1  4  1\n",
      "  1  4  1  3  1  4  4  4 10  2  2  4  5  3  3  0  0  4  3  4  4  9  1  9\n",
      "  0  9  3  7  1  4  2  9 10 10  0  4  1  9  2 10  0  9  1  0  1  7 10  1\n",
      "  1  9  4  0  3  0  0  0  2 10  6  6  8  9  0  0  1  0  8  5  9  4  4  3\n",
      "  4 10  0  0  0  4  5  2  3  4  0  4  5  9  4  3  2  0  4  4 10  0  0  4\n",
      " 10  4  9  4  4  2  1  6  0  3  4  1  9  1  4  1  5  9  0  4  4  0  0  4\n",
      "  1  4  4  4  5  4  9  4  4  4  1  4 10  3  5  5  4  2  0  0  1 10  4  2\n",
      "  1  9  9  0  8  5  0  2  1 10  0  1  4  1  8  0  4  7  4  7  4  4 10  0\n",
      "  0  0  9  2  4  3  4 10  3  4 10  0  0  3  1  0  0  5  2  1 10  4  6  4\n",
      "  8  9  9  2  0  7  7  1  0  9  3  1 10  4  0  3  1  3  1  0  2  8  0 10\n",
      "  5  9  4  1  9  9 10  1  1  0 10 10  4  0  0  1  9  1  0  8  1  5  4  5\n",
      "  1  6  4  4  4  8  1  4  9  4  1  3  2  1  1  9  1  1  2  0  4  9  5  1\n",
      "  9  1  0  6  0  3  7  1  0  2  5  4  1  6  1  5 10  1  8  3  4  1  1  3\n",
      "  9  6  4  0  3  9  2  1  3  7  0  4  5  0  0  1  9  7  2  1  5  2  4  0\n",
      "  4  9  5  9 10  4  7  9  1  4  1  4 10  1  5  0  4  2  0  0  7  4  1 10\n",
      "  4  5  1  1 10  1 10  0  4  8  0  4 10  7  4  8  4  2  1  1  3  1  1  5\n",
      "  1  6  4  1  3  5  1  1  0  0  1  1  4  9  4  9  0  4  4  3  5  7  1  4\n",
      "  8  0  8  0  0  1  5  3 10  3  5  4  1  9  1  0 10 10 10  4  4  9  9  5\n",
      " 10  8  0  0  4  4  9  4  9  4  5  0  1  1 10  5  1  2  0  5  3  1  4  4\n",
      "  0  1  2  2  9  4  0  4  4  7 10  4  1  0  8  0 10  2 10  4  0  4  3  1\n",
      "  4  0  0  2  2  9  0  4  0  4  9  8 10  2  2  3  0  4  0  0  1  8  2  4\n",
      "  4  0  4  0  0  2  9  5  4  3 10  0  8  1  4  0  0  4  4  4 10  3  4 10\n",
      "  1  4  6  9  5  0  2  0  9  9  4  4  1  2  4  4  0  4  5  3  1  3  5  4\n",
      "  5  4  0  9  1  4 10  5  1  3  0 10 10  0  4  4  4  0  1  5  3  4  0  9\n",
      "  9  7  9  5  0  9 10  3  1  2  4  1 10  0  0  2  7  4  2  0  2  4  1  7\n",
      "  4  1  0  0  1 10  0  1  9  5  0  4  4  4  0  4  2  6  0  4  4  9  5  4\n",
      "  5  0  2  1  0  1  1  0 10  4  4  4  0  0  1  7  4 10  4  4  7  4  5  0\n",
      "  2  0  0  9  1  3  0  4  4 10  0  5  7  4  4  0  4  5  1  0 10  1  6  0\n",
      "  2  7  9  0  1  5  7  1  2  6  3  0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy\n",
    "\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "print(result.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_samples,silhouette_score\n",
    "from matplotlib import cm\n",
    "\n",
    "def plotSilhouette(X, y_km):\n",
    "    cluster_labels = np.unique(y_km.labels_)\n",
    "    n_clusters = cluster_labels.shape[0]\n",
    "    silhouette_vals = silhouette_score(X, y_km.labels_,metric='euclidean')\n",
    "    print(silhouette_vals)\n",
    "#     y_ax_lower, y_ax_upper = 0,0\n",
    "#     yticks = []\n",
    "    \n",
    "#     for i , c in enumerate(cluster_labels):\n",
    "#         c_silhouette_vals = silhouette_vals[y_km.labels_ == c]\n",
    "#         c_silhouette_vals.sort()\n",
    "#         y_ax_upper += len(c_silhouette_vals)\n",
    "#         color = cm.jet(i/n_clusters)\n",
    "        \n",
    "#         plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0,edgecolor='none', color=color)\n",
    "#         yticks.append((y_ax_lower + y_ax_upper)/2)\n",
    "#         y_ax_lower += len(c_silhouette_vals)\n",
    "    \n",
    "#     silhouette_avg = np.mean(silhouette_vals)\n",
    "#     plt.axvline(silhouette_avg, color='red', linestyle='--')\n",
    "#     plt.yticks(yticks, cluster_labels+1)\n",
    "#     plt.ylabel('cluster')\n",
    "#     plt.xlabel('silhouette score')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11032789613567383\n"
     ]
    }
   ],
   "source": [
    "plotSilhouette(features,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save(\"insect_16_0.19706.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
