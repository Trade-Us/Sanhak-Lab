{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from readFile import split_into_values, toRPdata\n",
    "# columns 와 value는 사용자 입력\n",
    "# df = pd.read_csv('resources/AXISX_resample.csv')\n",
    "df = pd.read_csv('resources/CLAMP_resample.csv')\n",
    "columns = ['chip', 'wire', 'segment']\n",
    "value = ['value']\n",
    "#df = pd.read_csv('resources/Dataset1.csv')\n",
    "#columns = ['Process', 'Step']\n",
    "#value = ['Value']\n",
    "\n",
    "df = df.loc[:, columns + value] #('chip', 'wire', 'value')는 사용자 입력\n",
    "size = 256\n",
    "result = split_into_values(df, columns)\n",
    "\n",
    "# dataframe -> list\n",
    "result_list = result.values.tolist()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def MinMax(data):\n",
    "    MMS = MinMaxScaler().fit(data)\n",
    "    scaled = MMS.transform(data)\n",
    "    return scaled\n",
    "\n",
    "# result_list transpose\n",
    "result_T = [list(x) for x in zip(*result_list)]\n",
    "\n",
    "# minmax 정규화\n",
    "result_scaled = MinMax(result_T)\n",
    "\n",
    "# 다시 result transpose 해서 원래대로\n",
    "result_scaled = [list(x) for x in zip(*result_scaled)]\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "\n",
    "# 2. 시계열 셋 크기 변경\n",
    "#result_ = TimeSeriesResampler(sz=size).fit_transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1140, 256, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 시계열 셋 크기 변경\n",
    "result_ = TimeSeriesResampler(sz=size).fit_transform(result_scaled)\n",
    "\n",
    "result_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1140, 28, 28, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = result_.reshape(result_.shape[0], 1, size)\n",
    "X = toRPdata(data, threshold='point', percentage=30)\n",
    "#X = toRPdata(data)\n",
    "    \n",
    "X_scaled = np.expand_dims(X, axis=3)\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a3bad89b00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(X[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you will use a batch size of 128 using a higher batch size of 256 or 512 is also preferable\n",
    "batch_size = 128\n",
    "learning_rate = 0.0001\n",
    "epochs = 5000\n",
    "optimizer='Adam'\n",
    "loss='mse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 2)         290       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 14, 14, 2)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 7, 7, 2)           38        \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 7, 7, 2)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 14, 14, 2)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 14, 14, 16)        304       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 28, 28, 1)         145       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 937\n",
      "Trainable params: 937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from utils import split_data, normalization_tool\n",
    "from agent import Autoencoder_Agent\n",
    "from utils import optimizer_set\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, Dense\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D\n",
    "\n",
    "#lr 바뀔때는 실행하지 말것! 새로운 rp 적용할때는 한번 적용!\n",
    "#X_train, X_test, Y_train, Y_test = split_data(X_scaled, X_scaled) #데이터 분리\n",
    "\n",
    "optimizer = optimizer_set(optimizer, learning_rate)\n",
    "model = Sequential()\n",
    " \n",
    "#1st convolution layer\n",
    "model.add(Conv2D(16, (3, 3) #16 is number of filters and (3, 3) is the size of the filter.\n",
    ", padding='same', input_shape=(28,28,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "\n",
    "#2nd convolution layer\n",
    "model.add(Conv2D(2,(3, 3), padding='same')) # apply 2 filters sized of (3x3)\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "\n",
    "#here compressed version\n",
    "\n",
    "#3rd convolution layer\n",
    "model.add(Conv2D(2,(3, 3), padding='same')) # apply 2 filters sized of (3x3)\n",
    "model.add(Activation('relu'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "\n",
    "#4rd convolution layer\n",
    "model.add(Conv2D(16,(3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(1,(3, 3), padding='same'))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss) #사용자 지정 파라미터(optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100, min_delta=0.0001)\n",
    "mc = ModelCheckpoint(f'CLAMP28_0.001_Autoencoder_weight.h5', monitor='val_loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2503\n",
      "Epoch 00001: val_loss improved from inf to 0.25002, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.2503 - val_loss: 0.2500\n",
      "Epoch 2/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2499\n",
      "Epoch 00002: val_loss improved from 0.25002 to 0.24965, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.2499 - val_loss: 0.2497\n",
      "Epoch 3/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2495\n",
      "Epoch 00003: val_loss improved from 0.24965 to 0.24931, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.2495 - val_loss: 0.2493\n",
      "Epoch 4/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2492\n",
      "Epoch 00004: val_loss improved from 0.24931 to 0.24897, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.2492 - val_loss: 0.2490\n",
      "Epoch 5/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2488\n",
      "Epoch 00005: val_loss improved from 0.24897 to 0.24864, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.2488 - val_loss: 0.2486\n",
      "Epoch 6/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2485\n",
      "Epoch 00006: val_loss improved from 0.24864 to 0.24829, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.2485 - val_loss: 0.2483\n",
      "Epoch 7/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2482\n",
      "Epoch 00007: val_loss improved from 0.24829 to 0.24794, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.2482 - val_loss: 0.2479\n",
      "Epoch 8/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2478\n",
      "Epoch 00008: val_loss improved from 0.24794 to 0.24759, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.2478 - val_loss: 0.2476\n",
      "Epoch 9/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2474\n",
      "Epoch 00009: val_loss improved from 0.24759 to 0.24723, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.2474 - val_loss: 0.2472\n",
      "Epoch 10/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2471\n",
      "Epoch 00010: val_loss improved from 0.24723 to 0.24686, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.2471 - val_loss: 0.2469\n",
      "Epoch 11/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2467\n",
      "Epoch 00011: val_loss improved from 0.24686 to 0.24648, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.2467 - val_loss: 0.2465\n",
      "Epoch 12/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2463\n",
      "Epoch 00012: val_loss improved from 0.24648 to 0.24610, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.2463 - val_loss: 0.2461\n",
      "Epoch 13/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2459\n",
      "Epoch 00013: val_loss improved from 0.24610 to 0.24570, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.2459 - val_loss: 0.2457\n",
      "Epoch 14/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2455\n",
      "Epoch 00014: val_loss improved from 0.24570 to 0.24530, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.2455 - val_loss: 0.2453\n",
      "Epoch 15/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2451\n",
      "Epoch 00015: val_loss improved from 0.24530 to 0.24489, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.2451 - val_loss: 0.2449\n",
      "Epoch 16/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2447\n",
      "Epoch 00016: val_loss improved from 0.24489 to 0.24446, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 0.2447 - val_loss: 0.2445\n",
      "Epoch 17/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2443\n",
      "Epoch 00017: val_loss improved from 0.24446 to 0.24403, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.2443 - val_loss: 0.2440\n",
      "Epoch 18/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2438\n",
      "Epoch 00018: val_loss improved from 0.24403 to 0.24358, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.2438 - val_loss: 0.2436\n",
      "Epoch 19/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2434\n",
      "Epoch 00019: val_loss improved from 0.24358 to 0.24312, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 0.2434 - val_loss: 0.2431\n",
      "Epoch 20/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2429\n",
      "Epoch 00020: val_loss improved from 0.24312 to 0.24265, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.2429 - val_loss: 0.2426\n",
      "Epoch 21/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2424\n",
      "Epoch 00021: val_loss improved from 0.24265 to 0.24216, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.2424 - val_loss: 0.2422\n",
      "Epoch 22/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2420\n",
      "Epoch 00022: val_loss improved from 0.24216 to 0.24167, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.2420 - val_loss: 0.2417\n",
      "Epoch 23/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2415\n",
      "Epoch 00023: val_loss improved from 0.24167 to 0.24116, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.2415 - val_loss: 0.2412\n",
      "Epoch 24/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2409\n",
      "Epoch 00024: val_loss improved from 0.24116 to 0.24064, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.2409 - val_loss: 0.2406\n",
      "Epoch 25/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2404\n",
      "Epoch 00025: val_loss improved from 0.24064 to 0.24012, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.2404 - val_loss: 0.2401\n",
      "Epoch 26/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2399\n",
      "Epoch 00026: val_loss improved from 0.24012 to 0.23958, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.2399 - val_loss: 0.2396\n",
      "Epoch 27/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2393\n",
      "Epoch 00027: val_loss improved from 0.23958 to 0.23902, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.2393 - val_loss: 0.2390\n",
      "Epoch 28/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2388\n",
      "Epoch 00028: val_loss improved from 0.23902 to 0.23845, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.2388 - val_loss: 0.2385\n",
      "Epoch 29/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2382\n",
      "Epoch 00029: val_loss improved from 0.23845 to 0.23787, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.2382 - val_loss: 0.2379\n",
      "Epoch 30/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2376\n",
      "Epoch 00030: val_loss improved from 0.23787 to 0.23727, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.2376 - val_loss: 0.2373\n",
      "Epoch 31/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2370\n",
      "Epoch 00031: val_loss improved from 0.23727 to 0.23665, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.2370 - val_loss: 0.2367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2364\n",
      "Epoch 00032: val_loss improved from 0.23665 to 0.23602, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.2364 - val_loss: 0.2360\n",
      "Epoch 33/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2358\n",
      "Epoch 00033: val_loss improved from 0.23602 to 0.23536, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.2358 - val_loss: 0.2354\n",
      "Epoch 34/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2351\n",
      "Epoch 00034: val_loss improved from 0.23536 to 0.23466, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.2351 - val_loss: 0.2347\n",
      "Epoch 35/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2344\n",
      "Epoch 00035: val_loss improved from 0.23466 to 0.23393, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.2344 - val_loss: 0.2339\n",
      "Epoch 36/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2337\n",
      "Epoch 00036: val_loss improved from 0.23393 to 0.23316, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.2337 - val_loss: 0.2332\n",
      "Epoch 37/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2329\n",
      "Epoch 00037: val_loss improved from 0.23316 to 0.23233, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.2329 - val_loss: 0.2323\n",
      "Epoch 38/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2320\n",
      "Epoch 00038: val_loss improved from 0.23233 to 0.23136, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.2320 - val_loss: 0.2314\n",
      "Epoch 39/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2308\n",
      "Epoch 00039: val_loss improved from 0.23136 to 0.22977, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.2308 - val_loss: 0.2298\n",
      "Epoch 40/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2290\n",
      "Epoch 00040: val_loss improved from 0.22977 to 0.22748, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.2290 - val_loss: 0.2275\n",
      "Epoch 41/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2265\n",
      "Epoch 00041: val_loss improved from 0.22748 to 0.22506, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.2265 - val_loss: 0.2251\n",
      "Epoch 42/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2242\n",
      "Epoch 00042: val_loss improved from 0.22506 to 0.22323, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.2242 - val_loss: 0.2232\n",
      "Epoch 43/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2225\n",
      "Epoch 00043: val_loss improved from 0.22323 to 0.22169, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.2225 - val_loss: 0.2217\n",
      "Epoch 44/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2209\n",
      "Epoch 00044: val_loss improved from 0.22169 to 0.22015, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.2209 - val_loss: 0.2201\n",
      "Epoch 45/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2194\n",
      "Epoch 00045: val_loss improved from 0.22015 to 0.21856, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.2194 - val_loss: 0.2186\n",
      "Epoch 46/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2178\n",
      "Epoch 00046: val_loss improved from 0.21856 to 0.21692, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.2178 - val_loss: 0.2169\n",
      "Epoch 47/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2162\n",
      "Epoch 00047: val_loss improved from 0.21692 to 0.21534, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.2162 - val_loss: 0.2153\n",
      "Epoch 48/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2146\n",
      "Epoch 00048: val_loss improved from 0.21534 to 0.21381, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.2146 - val_loss: 0.2138\n",
      "Epoch 49/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2132\n",
      "Epoch 00049: val_loss improved from 0.21381 to 0.21234, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.2132 - val_loss: 0.2123\n",
      "Epoch 50/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2117\n",
      "Epoch 00050: val_loss improved from 0.21234 to 0.21091, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.2117 - val_loss: 0.2109\n",
      "Epoch 51/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2103\n",
      "Epoch 00051: val_loss improved from 0.21091 to 0.20951, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.2103 - val_loss: 0.2095\n",
      "Epoch 52/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2089\n",
      "Epoch 00052: val_loss improved from 0.20951 to 0.20813, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.2089 - val_loss: 0.2081\n",
      "Epoch 53/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2076\n",
      "Epoch 00053: val_loss improved from 0.20813 to 0.20678, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.2076 - val_loss: 0.2068\n",
      "Epoch 54/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2063\n",
      "Epoch 00054: val_loss improved from 0.20678 to 0.20547, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.2063 - val_loss: 0.2055\n",
      "Epoch 55/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2050\n",
      "Epoch 00055: val_loss improved from 0.20547 to 0.20420, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.2050 - val_loss: 0.2042\n",
      "Epoch 56/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2037\n",
      "Epoch 00056: val_loss improved from 0.20420 to 0.20297, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.2037 - val_loss: 0.2030\n",
      "Epoch 57/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2025\n",
      "Epoch 00057: val_loss improved from 0.20297 to 0.20178, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.2025 - val_loss: 0.2018\n",
      "Epoch 58/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2014\n",
      "Epoch 00058: val_loss improved from 0.20178 to 0.20064, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.2014 - val_loss: 0.2006\n",
      "Epoch 59/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.2003\n",
      "Epoch 00059: val_loss improved from 0.20064 to 0.19956, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.2003 - val_loss: 0.1996\n",
      "Epoch 60/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1992\n",
      "Epoch 00060: val_loss improved from 0.19956 to 0.19852, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1992 - val_loss: 0.1985\n",
      "Epoch 61/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1982- ETA: 0s - loss: 0.19\n",
      "Epoch 00061: val_loss improved from 0.19852 to 0.19755, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1982 - val_loss: 0.1975\n",
      "Epoch 62/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1972\n",
      "Epoch 00062: val_loss improved from 0.19755 to 0.19663, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1972 - val_loss: 0.1966\n",
      "Epoch 63/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1964\n",
      "Epoch 00063: val_loss improved from 0.19663 to 0.19576, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1964 - val_loss: 0.1958\n",
      "Epoch 64/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1955\n",
      "Epoch 00064: val_loss improved from 0.19576 to 0.19492, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1955 - val_loss: 0.1949\n",
      "Epoch 65/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1947\n",
      "Epoch 00065: val_loss improved from 0.19492 to 0.19413, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1947 - val_loss: 0.1941\n",
      "Epoch 66/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1939\n",
      "Epoch 00066: val_loss improved from 0.19413 to 0.19337, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1939 - val_loss: 0.1934\n",
      "Epoch 67/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1932\n",
      "Epoch 00067: val_loss improved from 0.19337 to 0.19264, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1932 - val_loss: 0.1926\n",
      "Epoch 68/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1925\n",
      "Epoch 00068: val_loss improved from 0.19264 to 0.19194, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1925 - val_loss: 0.1919\n",
      "Epoch 69/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1918\n",
      "Epoch 00069: val_loss improved from 0.19194 to 0.19127, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1918 - val_loss: 0.1913\n",
      "Epoch 70/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1912\n",
      "Epoch 00070: val_loss improved from 0.19127 to 0.19062, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1912 - val_loss: 0.1906\n",
      "Epoch 71/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1906\n",
      "Epoch 00071: val_loss improved from 0.19062 to 0.19001, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1906 - val_loss: 0.1900\n",
      "Epoch 72/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1900\n",
      "Epoch 00072: val_loss improved from 0.19001 to 0.18941, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1900 - val_loss: 0.1894\n",
      "Epoch 73/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1894\n",
      "Epoch 00073: val_loss improved from 0.18941 to 0.18883, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1894 - val_loss: 0.1888\n",
      "Epoch 74/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1888\n",
      "Epoch 00074: val_loss improved from 0.18883 to 0.18825, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1888 - val_loss: 0.1882\n",
      "Epoch 75/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1883\n",
      "Epoch 00075: val_loss improved from 0.18825 to 0.18767, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1883 - val_loss: 0.1877\n",
      "Epoch 76/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1877\n",
      "Epoch 00076: val_loss improved from 0.18767 to 0.18711, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1877 - val_loss: 0.1871\n",
      "Epoch 77/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1871\n",
      "Epoch 00077: val_loss improved from 0.18711 to 0.18655, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1871 - val_loss: 0.1865\n",
      "Epoch 78/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1866\n",
      "Epoch 00078: val_loss improved from 0.18655 to 0.18600, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1866 - val_loss: 0.1860\n",
      "Epoch 79/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1861\n",
      "Epoch 00079: val_loss improved from 0.18600 to 0.18546, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1861 - val_loss: 0.1855\n",
      "Epoch 80/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1855\n",
      "Epoch 00080: val_loss improved from 0.18546 to 0.18490, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1855 - val_loss: 0.1849\n",
      "Epoch 81/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1850\n",
      "Epoch 00081: val_loss improved from 0.18490 to 0.18436, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1850 - val_loss: 0.1844\n",
      "Epoch 82/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1845\n",
      "Epoch 00082: val_loss improved from 0.18436 to 0.18381, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1845 - val_loss: 0.1838\n",
      "Epoch 83/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1839\n",
      "Epoch 00083: val_loss improved from 0.18381 to 0.18326, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1839 - val_loss: 0.1833\n",
      "Epoch 84/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1834\n",
      "Epoch 00084: val_loss improved from 0.18326 to 0.18272, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1834 - val_loss: 0.1827\n",
      "Epoch 85/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1828\n",
      "Epoch 00085: val_loss improved from 0.18272 to 0.18217, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1828 - val_loss: 0.1822\n",
      "Epoch 86/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1823\n",
      "Epoch 00086: val_loss improved from 0.18217 to 0.18161, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1823 - val_loss: 0.1816\n",
      "Epoch 87/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1818\n",
      "Epoch 00087: val_loss improved from 0.18161 to 0.18105, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1818 - val_loss: 0.1811\n",
      "Epoch 88/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1812\n",
      "Epoch 00088: val_loss improved from 0.18105 to 0.18050, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1812 - val_loss: 0.1805\n",
      "Epoch 89/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1807\n",
      "Epoch 00089: val_loss improved from 0.18050 to 0.17994, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1807 - val_loss: 0.1799\n",
      "Epoch 90/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1801\n",
      "Epoch 00090: val_loss improved from 0.17994 to 0.17937, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1801 - val_loss: 0.1794\n",
      "Epoch 91/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1795\n",
      "Epoch 00091: val_loss improved from 0.17937 to 0.17879, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1795 - val_loss: 0.1788\n",
      "Epoch 92/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1790\n",
      "Epoch 00092: val_loss improved from 0.17879 to 0.17821, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1790 - val_loss: 0.1782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1784\n",
      "Epoch 00093: val_loss improved from 0.17821 to 0.17761, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1784 - val_loss: 0.1776\n",
      "Epoch 94/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1778\n",
      "Epoch 00094: val_loss improved from 0.17761 to 0.17700, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1778 - val_loss: 0.1770\n",
      "Epoch 95/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1772\n",
      "Epoch 00095: val_loss improved from 0.17700 to 0.17638, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1772 - val_loss: 0.1764\n",
      "Epoch 96/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1766\n",
      "Epoch 00096: val_loss improved from 0.17638 to 0.17571, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1766 - val_loss: 0.1757\n",
      "Epoch 97/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1759\n",
      "Epoch 00097: val_loss improved from 0.17571 to 0.17502, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1759 - val_loss: 0.1750\n",
      "Epoch 98/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1752\n",
      "Epoch 00098: val_loss improved from 0.17502 to 0.17431, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1752 - val_loss: 0.1743\n",
      "Epoch 99/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1745\n",
      "Epoch 00099: val_loss improved from 0.17431 to 0.17359, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1745 - val_loss: 0.1736\n",
      "Epoch 100/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1738\n",
      "Epoch 00100: val_loss improved from 0.17359 to 0.17286, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1738 - val_loss: 0.1729\n",
      "Epoch 101/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1731\n",
      "Epoch 00101: val_loss improved from 0.17286 to 0.17213, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1731 - val_loss: 0.1721\n",
      "Epoch 102/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1723\n",
      "Epoch 00102: val_loss improved from 0.17213 to 0.17138, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1723 - val_loss: 0.1714\n",
      "Epoch 103/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1716\n",
      "Epoch 00103: val_loss improved from 0.17138 to 0.17062, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1716 - val_loss: 0.1706\n",
      "Epoch 104/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1708\n",
      "Epoch 00104: val_loss improved from 0.17062 to 0.16984, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1708 - val_loss: 0.1698\n",
      "Epoch 105/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1701\n",
      "Epoch 00105: val_loss improved from 0.16984 to 0.16907, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1701 - val_loss: 0.1691\n",
      "Epoch 106/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1693\n",
      "Epoch 00106: val_loss improved from 0.16907 to 0.16830, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1693 - val_loss: 0.1683\n",
      "Epoch 107/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1685\n",
      "Epoch 00107: val_loss improved from 0.16830 to 0.16752, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1685 - val_loss: 0.1675\n",
      "Epoch 108/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1678\n",
      "Epoch 00108: val_loss improved from 0.16752 to 0.16675, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1678 - val_loss: 0.1667\n",
      "Epoch 109/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1670\n",
      "Epoch 00109: val_loss improved from 0.16675 to 0.16596, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1670 - val_loss: 0.1660\n",
      "Epoch 110/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1662\n",
      "Epoch 00110: val_loss improved from 0.16596 to 0.16519, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1662 - val_loss: 0.1652\n",
      "Epoch 111/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1655\n",
      "Epoch 00111: val_loss improved from 0.16519 to 0.16443, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1655 - val_loss: 0.1644\n",
      "Epoch 112/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1647\n",
      "Epoch 00112: val_loss improved from 0.16443 to 0.16367, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1647 - val_loss: 0.1637\n",
      "Epoch 113/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1640\n",
      "Epoch 00113: val_loss improved from 0.16367 to 0.16292, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1640 - val_loss: 0.1629\n",
      "Epoch 114/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1633\n",
      "Epoch 00114: val_loss improved from 0.16292 to 0.16218, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1633 - val_loss: 0.1622\n",
      "Epoch 115/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1625\n",
      "Epoch 00115: val_loss improved from 0.16218 to 0.16146, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1625 - val_loss: 0.1615\n",
      "Epoch 116/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1618\n",
      "Epoch 00116: val_loss improved from 0.16146 to 0.16074, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1618 - val_loss: 0.1607\n",
      "Epoch 117/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1611\n",
      "Epoch 00117: val_loss improved from 0.16074 to 0.16004, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1611 - val_loss: 0.1600\n",
      "Epoch 118/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1604\n",
      "Epoch 00118: val_loss improved from 0.16004 to 0.15933, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1604 - val_loss: 0.1593\n",
      "Epoch 119/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1597\n",
      "Epoch 00119: val_loss improved from 0.15933 to 0.15864, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1597 - val_loss: 0.1586\n",
      "Epoch 120/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1590\n",
      "Epoch 00120: val_loss improved from 0.15864 to 0.15796, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1590 - val_loss: 0.1580\n",
      "Epoch 121/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1584\n",
      "Epoch 00121: val_loss improved from 0.15796 to 0.15728, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1584 - val_loss: 0.1573\n",
      "Epoch 122/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1577\n",
      "Epoch 00122: val_loss improved from 0.15728 to 0.15661, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1577 - val_loss: 0.1566\n",
      "Epoch 123/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1570\n",
      "Epoch 00123: val_loss improved from 0.15661 to 0.15595, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1570 - val_loss: 0.1560\n",
      "Epoch 124/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1564\n",
      "Epoch 00124: val_loss improved from 0.15595 to 0.15530, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1564 - val_loss: 0.1553\n",
      "Epoch 125/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1558\n",
      "Epoch 00125: val_loss improved from 0.15530 to 0.15467, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1558 - val_loss: 0.1547\n",
      "Epoch 126/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1551\n",
      "Epoch 00126: val_loss improved from 0.15467 to 0.15404, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1551 - val_loss: 0.1540\n",
      "Epoch 127/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1545\n",
      "Epoch 00127: val_loss improved from 0.15404 to 0.15343, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1545 - val_loss: 0.1534\n",
      "Epoch 128/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1539\n",
      "Epoch 00128: val_loss improved from 0.15343 to 0.15284, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1539 - val_loss: 0.1528\n",
      "Epoch 129/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1533\n",
      "Epoch 00129: val_loss improved from 0.15284 to 0.15225, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1533 - val_loss: 0.1522\n",
      "Epoch 130/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1527\n",
      "Epoch 00130: val_loss improved from 0.15225 to 0.15167, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1527 - val_loss: 0.1517\n",
      "Epoch 131/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1522\n",
      "Epoch 00131: val_loss improved from 0.15167 to 0.15111, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1522 - val_loss: 0.1511\n",
      "Epoch 132/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1516\n",
      "Epoch 00132: val_loss improved from 0.15111 to 0.15055, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1516 - val_loss: 0.1506\n",
      "Epoch 133/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1511\n",
      "Epoch 00133: val_loss improved from 0.15055 to 0.15001, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1511 - val_loss: 0.1500\n",
      "Epoch 134/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1505\n",
      "Epoch 00134: val_loss improved from 0.15001 to 0.14948, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1505 - val_loss: 0.1495\n",
      "Epoch 135/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1500\n",
      "Epoch 00135: val_loss improved from 0.14948 to 0.14896, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1500 - val_loss: 0.1490\n",
      "Epoch 136/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1495\n",
      "Epoch 00136: val_loss improved from 0.14896 to 0.14845, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1495 - val_loss: 0.1485\n",
      "Epoch 137/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1490\n",
      "Epoch 00137: val_loss improved from 0.14845 to 0.14796, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1490 - val_loss: 0.1480\n",
      "Epoch 138/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1485\n",
      "Epoch 00138: val_loss improved from 0.14796 to 0.14748, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1485 - val_loss: 0.1475\n",
      "Epoch 139/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1480\n",
      "Epoch 00139: val_loss improved from 0.14748 to 0.14701, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1480 - val_loss: 0.1470\n",
      "Epoch 140/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1475\n",
      "Epoch 00140: val_loss improved from 0.14701 to 0.14654, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.1475 - val_loss: 0.1465\n",
      "Epoch 141/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1471\n",
      "Epoch 00141: val_loss improved from 0.14654 to 0.14609, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1471 - val_loss: 0.1461\n",
      "Epoch 142/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1466\n",
      "Epoch 00142: val_loss improved from 0.14609 to 0.14565, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1466 - val_loss: 0.1457\n",
      "Epoch 143/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1462\n",
      "Epoch 00143: val_loss improved from 0.14565 to 0.14522, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1462 - val_loss: 0.1452\n",
      "Epoch 144/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1457\n",
      "Epoch 00144: val_loss improved from 0.14522 to 0.14479, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1457 - val_loss: 0.1448\n",
      "Epoch 145/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1453\n",
      "Epoch 00145: val_loss improved from 0.14479 to 0.14438, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1453 - val_loss: 0.1444\n",
      "Epoch 146/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1449\n",
      "Epoch 00146: val_loss improved from 0.14438 to 0.14398, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1449 - val_loss: 0.1440\n",
      "Epoch 147/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1445\n",
      "Epoch 00147: val_loss improved from 0.14398 to 0.14360, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1445 - val_loss: 0.1436\n",
      "Epoch 148/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1441\n",
      "Epoch 00148: val_loss improved from 0.14360 to 0.14321, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1441 - val_loss: 0.1432\n",
      "Epoch 149/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1437\n",
      "Epoch 00149: val_loss improved from 0.14321 to 0.14284, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1437 - val_loss: 0.1428\n",
      "Epoch 150/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1433\n",
      "Epoch 00150: val_loss improved from 0.14284 to 0.14248, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1433 - val_loss: 0.1425\n",
      "Epoch 151/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1430\n",
      "Epoch 00151: val_loss improved from 0.14248 to 0.14212, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1430 - val_loss: 0.1421\n",
      "Epoch 152/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1426\n",
      "Epoch 00152: val_loss improved from 0.14212 to 0.14177, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1426 - val_loss: 0.1418\n",
      "Epoch 153/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1423\n",
      "Epoch 00153: val_loss improved from 0.14177 to 0.14142, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1423 - val_loss: 0.1414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1419\n",
      "Epoch 00154: val_loss improved from 0.14142 to 0.14108, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1419 - val_loss: 0.1411\n",
      "Epoch 155/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1416\n",
      "Epoch 00155: val_loss improved from 0.14108 to 0.14074, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1416 - val_loss: 0.1407\n",
      "Epoch 156/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1412\n",
      "Epoch 00156: val_loss improved from 0.14074 to 0.14042, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1412 - val_loss: 0.1404\n",
      "Epoch 157/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1409\n",
      "Epoch 00157: val_loss improved from 0.14042 to 0.14010, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1409 - val_loss: 0.1401\n",
      "Epoch 158/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1406\n",
      "Epoch 00158: val_loss improved from 0.14010 to 0.13979, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1406 - val_loss: 0.1398\n",
      "Epoch 159/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1402\n",
      "Epoch 00159: val_loss improved from 0.13979 to 0.13949, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1402 - val_loss: 0.1395\n",
      "Epoch 160/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1399\n",
      "Epoch 00160: val_loss improved from 0.13949 to 0.13919, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1399 - val_loss: 0.1392\n",
      "Epoch 161/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1396\n",
      "Epoch 00161: val_loss improved from 0.13919 to 0.13890, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1396 - val_loss: 0.1389\n",
      "Epoch 162/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1394\n",
      "Epoch 00162: val_loss improved from 0.13890 to 0.13862, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1394 - val_loss: 0.1386\n",
      "Epoch 163/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1391\n",
      "Epoch 00163: val_loss improved from 0.13862 to 0.13835, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1391 - val_loss: 0.1383\n",
      "Epoch 164/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1388\n",
      "Epoch 00164: val_loss improved from 0.13835 to 0.13807, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1388 - val_loss: 0.1381\n",
      "Epoch 165/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1385\n",
      "Epoch 00165: val_loss improved from 0.13807 to 0.13780, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1385 - val_loss: 0.1378\n",
      "Epoch 166/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1382\n",
      "Epoch 00166: val_loss improved from 0.13780 to 0.13753, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1382 - val_loss: 0.1375\n",
      "Epoch 167/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1380\n",
      "Epoch 00167: val_loss improved from 0.13753 to 0.13727, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1380 - val_loss: 0.1373\n",
      "Epoch 168/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1377\n",
      "Epoch 00168: val_loss improved from 0.13727 to 0.13701, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1377 - val_loss: 0.1370\n",
      "Epoch 169/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1375- ETA: 0s - loss: 0.\n",
      "Epoch 00169: val_loss improved from 0.13701 to 0.13676, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1375 - val_loss: 0.1368\n",
      "Epoch 170/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1372\n",
      "Epoch 00170: val_loss improved from 0.13676 to 0.13652, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1372 - val_loss: 0.1365\n",
      "Epoch 171/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1370\n",
      "Epoch 00171: val_loss improved from 0.13652 to 0.13628, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1370 - val_loss: 0.1363\n",
      "Epoch 172/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1367\n",
      "Epoch 00172: val_loss improved from 0.13628 to 0.13606, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1367 - val_loss: 0.1361\n",
      "Epoch 173/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1365\n",
      "Epoch 00173: val_loss improved from 0.13606 to 0.13583, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1365 - val_loss: 0.1358\n",
      "Epoch 174/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1363\n",
      "Epoch 00174: val_loss improved from 0.13583 to 0.13562, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1363 - val_loss: 0.1356\n",
      "Epoch 175/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1361\n",
      "Epoch 00175: val_loss improved from 0.13562 to 0.13541, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1361 - val_loss: 0.1354\n",
      "Epoch 176/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1359\n",
      "Epoch 00176: val_loss improved from 0.13541 to 0.13520, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1359 - val_loss: 0.1352\n",
      "Epoch 177/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1356\n",
      "Epoch 00177: val_loss improved from 0.13520 to 0.13500, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1356 - val_loss: 0.1350\n",
      "Epoch 178/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1354\n",
      "Epoch 00178: val_loss improved from 0.13500 to 0.13481, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1354 - val_loss: 0.1348\n",
      "Epoch 179/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1352\n",
      "Epoch 00179: val_loss improved from 0.13481 to 0.13462, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1352 - val_loss: 0.1346\n",
      "Epoch 180/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1350\n",
      "Epoch 00180: val_loss improved from 0.13462 to 0.13443, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1350 - val_loss: 0.1344\n",
      "Epoch 181/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1348\n",
      "Epoch 00181: val_loss improved from 0.13443 to 0.13425, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1348 - val_loss: 0.1343\n",
      "Epoch 182/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1347\n",
      "Epoch 00182: val_loss improved from 0.13425 to 0.13407, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1347 - val_loss: 0.1341\n",
      "Epoch 183/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1345\n",
      "Epoch 00183: val_loss improved from 0.13407 to 0.13390, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1345 - val_loss: 0.1339\n",
      "Epoch 184/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1343\n",
      "Epoch 00184: val_loss improved from 0.13390 to 0.13373, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1343 - val_loss: 0.1337\n",
      "Epoch 185/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1341\n",
      "Epoch 00185: val_loss improved from 0.13373 to 0.13356, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1341 - val_loss: 0.1336\n",
      "Epoch 186/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1340\n",
      "Epoch 00186: val_loss improved from 0.13356 to 0.13340, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1340 - val_loss: 0.1334\n",
      "Epoch 187/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1338\n",
      "Epoch 00187: val_loss improved from 0.13340 to 0.13324, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1338 - val_loss: 0.1332\n",
      "Epoch 188/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1336\n",
      "Epoch 00188: val_loss improved from 0.13324 to 0.13308, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1336 - val_loss: 0.1331\n",
      "Epoch 189/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1335\n",
      "Epoch 00189: val_loss improved from 0.13308 to 0.13293, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1335 - val_loss: 0.1329\n",
      "Epoch 190/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1333\n",
      "Epoch 00190: val_loss improved from 0.13293 to 0.13278, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1333 - val_loss: 0.1328\n",
      "Epoch 191/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1332\n",
      "Epoch 00191: val_loss improved from 0.13278 to 0.13263, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1332 - val_loss: 0.1326\n",
      "Epoch 192/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1330\n",
      "Epoch 00192: val_loss improved from 0.13263 to 0.13249, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1330 - val_loss: 0.1325\n",
      "Epoch 193/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1329\n",
      "Epoch 00193: val_loss improved from 0.13249 to 0.13234, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1329 - val_loss: 0.1323\n",
      "Epoch 194/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1327\n",
      "Epoch 00194: val_loss improved from 0.13234 to 0.13220, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1327 - val_loss: 0.1322\n",
      "Epoch 195/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1326\n",
      "Epoch 00195: val_loss improved from 0.13220 to 0.13206, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1326 - val_loss: 0.1321\n",
      "Epoch 196/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1324\n",
      "Epoch 00196: val_loss improved from 0.13206 to 0.13192, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1324 - val_loss: 0.1319\n",
      "Epoch 197/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1323\n",
      "Epoch 00197: val_loss improved from 0.13192 to 0.13179, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1323 - val_loss: 0.1318\n",
      "Epoch 198/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1322\n",
      "Epoch 00198: val_loss improved from 0.13179 to 0.13165, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1322 - val_loss: 0.1317\n",
      "Epoch 199/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1320\n",
      "Epoch 00199: val_loss improved from 0.13165 to 0.13152, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1320 - val_loss: 0.1315\n",
      "Epoch 200/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1319\n",
      "Epoch 00200: val_loss improved from 0.13152 to 0.13140, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1319 - val_loss: 0.1314\n",
      "Epoch 201/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1318\n",
      "Epoch 00201: val_loss improved from 0.13140 to 0.13127, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1318 - val_loss: 0.1313\n",
      "Epoch 202/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1316\n",
      "Epoch 00202: val_loss improved from 0.13127 to 0.13114, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1316 - val_loss: 0.1311\n",
      "Epoch 203/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1315\n",
      "Epoch 00203: val_loss improved from 0.13114 to 0.13102, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1315 - val_loss: 0.1310\n",
      "Epoch 204/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1314\n",
      "Epoch 00204: val_loss improved from 0.13102 to 0.13089, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1314 - val_loss: 0.1309\n",
      "Epoch 205/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1313\n",
      "Epoch 00205: val_loss improved from 0.13089 to 0.13077, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1313 - val_loss: 0.1308\n",
      "Epoch 206/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1311\n",
      "Epoch 00206: val_loss improved from 0.13077 to 0.13065, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1311 - val_loss: 0.1307\n",
      "Epoch 207/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1310\n",
      "Epoch 00207: val_loss improved from 0.13065 to 0.13054, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1310 - val_loss: 0.1305\n",
      "Epoch 208/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1309\n",
      "Epoch 00208: val_loss improved from 0.13054 to 0.13042, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1309 - val_loss: 0.1304\n",
      "Epoch 209/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1308\n",
      "Epoch 00209: val_loss improved from 0.13042 to 0.13030, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1308 - val_loss: 0.1303\n",
      "Epoch 210/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1307\n",
      "Epoch 00210: val_loss improved from 0.13030 to 0.13019, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1307 - val_loss: 0.1302\n",
      "Epoch 211/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1305\n",
      "Epoch 00211: val_loss improved from 0.13019 to 0.13008, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1305 - val_loss: 0.1301\n",
      "Epoch 212/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1304\n",
      "Epoch 00212: val_loss improved from 0.13008 to 0.12996, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1304 - val_loss: 0.1300\n",
      "Epoch 213/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1303\n",
      "Epoch 00213: val_loss improved from 0.12996 to 0.12985, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1303 - val_loss: 0.1299\n",
      "Epoch 214/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1302\n",
      "Epoch 00214: val_loss improved from 0.12985 to 0.12974, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1302 - val_loss: 0.1297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1301\n",
      "Epoch 00215: val_loss improved from 0.12974 to 0.12964, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1301 - val_loss: 0.1296\n",
      "Epoch 216/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1300\n",
      "Epoch 00216: val_loss improved from 0.12964 to 0.12953, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1300 - val_loss: 0.1295\n",
      "Epoch 217/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1299\n",
      "Epoch 00217: val_loss improved from 0.12953 to 0.12942, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1299 - val_loss: 0.1294\n",
      "Epoch 218/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1297\n",
      "Epoch 00218: val_loss improved from 0.12942 to 0.12932, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1297 - val_loss: 0.1293\n",
      "Epoch 219/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1296\n",
      "Epoch 00219: val_loss improved from 0.12932 to 0.12922, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1296 - val_loss: 0.1292\n",
      "Epoch 220/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1295\n",
      "Epoch 00220: val_loss improved from 0.12922 to 0.12912, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1295 - val_loss: 0.1291\n",
      "Epoch 221/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1294\n",
      "Epoch 00221: val_loss improved from 0.12912 to 0.12901, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1294 - val_loss: 0.1290\n",
      "Epoch 222/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1293\n",
      "Epoch 00222: val_loss improved from 0.12901 to 0.12891, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1293 - val_loss: 0.1289\n",
      "Epoch 223/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1292\n",
      "Epoch 00223: val_loss improved from 0.12891 to 0.12881, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1292 - val_loss: 0.1288\n",
      "Epoch 224/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1291\n",
      "Epoch 00224: val_loss improved from 0.12881 to 0.12871, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1291 - val_loss: 0.1287\n",
      "Epoch 225/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1290\n",
      "Epoch 00225: val_loss improved from 0.12871 to 0.12861, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1290 - val_loss: 0.1286\n",
      "Epoch 226/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1289\n",
      "Epoch 00226: val_loss improved from 0.12861 to 0.12851, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1289 - val_loss: 0.1285\n",
      "Epoch 227/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1288\n",
      "Epoch 00227: val_loss improved from 0.12851 to 0.12841, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1288 - val_loss: 0.1284\n",
      "Epoch 228/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1287\n",
      "Epoch 00228: val_loss improved from 0.12841 to 0.12831, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1287 - val_loss: 0.1283\n",
      "Epoch 229/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1286\n",
      "Epoch 00229: val_loss improved from 0.12831 to 0.12821, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1286 - val_loss: 0.1282\n",
      "Epoch 230/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1285\n",
      "Epoch 00230: val_loss improved from 0.12821 to 0.12810, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.1285 - val_loss: 0.1281\n",
      "Epoch 231/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1284\n",
      "Epoch 00231: val_loss improved from 0.12810 to 0.12800, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1284 - val_loss: 0.1280\n",
      "Epoch 232/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1283\n",
      "Epoch 00232: val_loss improved from 0.12800 to 0.12789, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1283 - val_loss: 0.1279\n",
      "Epoch 233/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1282\n",
      "Epoch 00233: val_loss improved from 0.12789 to 0.12779, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1282 - val_loss: 0.1278\n",
      "Epoch 234/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1281\n",
      "Epoch 00234: val_loss improved from 0.12779 to 0.12769, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1281 - val_loss: 0.1277\n",
      "Epoch 235/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1280\n",
      "Epoch 00235: val_loss improved from 0.12769 to 0.12758, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1280 - val_loss: 0.1276\n",
      "Epoch 236/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1279\n",
      "Epoch 00236: val_loss improved from 0.12758 to 0.12748, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1279 - val_loss: 0.1275\n",
      "Epoch 237/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1278\n",
      "Epoch 00237: val_loss improved from 0.12748 to 0.12737, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1278 - val_loss: 0.1274\n",
      "Epoch 238/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1277\n",
      "Epoch 00238: val_loss improved from 0.12737 to 0.12727, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1277 - val_loss: 0.1273\n",
      "Epoch 239/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1276\n",
      "Epoch 00239: val_loss improved from 0.12727 to 0.12716, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1276 - val_loss: 0.1272\n",
      "Epoch 240/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1274\n",
      "Epoch 00240: val_loss improved from 0.12716 to 0.12706, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1274 - val_loss: 0.1271\n",
      "Epoch 241/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1273\n",
      "Epoch 00241: val_loss improved from 0.12706 to 0.12695, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1273 - val_loss: 0.1269\n",
      "Epoch 242/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1272\n",
      "Epoch 00242: val_loss improved from 0.12695 to 0.12683, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1272 - val_loss: 0.1268\n",
      "Epoch 243/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1271\n",
      "Epoch 00243: val_loss improved from 0.12683 to 0.12672, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1271 - val_loss: 0.1267\n",
      "Epoch 244/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1270\n",
      "Epoch 00244: val_loss improved from 0.12672 to 0.12660, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1270 - val_loss: 0.1266\n",
      "Epoch 245/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1269\n",
      "Epoch 00245: val_loss improved from 0.12660 to 0.12649, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1269 - val_loss: 0.1265\n",
      "Epoch 246/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1268\n",
      "Epoch 00246: val_loss improved from 0.12649 to 0.12638, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1268 - val_loss: 0.1264\n",
      "Epoch 247/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1267\n",
      "Epoch 00247: val_loss improved from 0.12638 to 0.12627, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1267 - val_loss: 0.1263\n",
      "Epoch 248/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1266\n",
      "Epoch 00248: val_loss improved from 0.12627 to 0.12615, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1266 - val_loss: 0.1262\n",
      "Epoch 249/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1265\n",
      "Epoch 00249: val_loss improved from 0.12615 to 0.12604, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1265 - val_loss: 0.1260\n",
      "Epoch 250/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1263\n",
      "Epoch 00250: val_loss improved from 0.12604 to 0.12593, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1263 - val_loss: 0.1259\n",
      "Epoch 251/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1262\n",
      "Epoch 00251: val_loss improved from 0.12593 to 0.12582, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1262 - val_loss: 0.1258\n",
      "Epoch 252/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1261\n",
      "Epoch 00252: val_loss improved from 0.12582 to 0.12571, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1261 - val_loss: 0.1257\n",
      "Epoch 253/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1260\n",
      "Epoch 00253: val_loss improved from 0.12571 to 0.12560, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1260 - val_loss: 0.1256\n",
      "Epoch 254/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1259\n",
      "Epoch 00254: val_loss improved from 0.12560 to 0.12550, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1259 - val_loss: 0.1255\n",
      "Epoch 255/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1258\n",
      "Epoch 00255: val_loss improved from 0.12550 to 0.12539, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1258 - val_loss: 0.1254\n",
      "Epoch 256/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1257\n",
      "Epoch 00256: val_loss improved from 0.12539 to 0.12529, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1257 - val_loss: 0.1253\n",
      "Epoch 257/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1256\n",
      "Epoch 00257: val_loss improved from 0.12529 to 0.12518, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1256 - val_loss: 0.1252\n",
      "Epoch 258/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1255\n",
      "Epoch 00258: val_loss improved from 0.12518 to 0.12508, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1255 - val_loss: 0.1251\n",
      "Epoch 259/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1254\n",
      "Epoch 00259: val_loss improved from 0.12508 to 0.12499, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1254 - val_loss: 0.1250\n",
      "Epoch 260/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1253\n",
      "Epoch 00260: val_loss improved from 0.12499 to 0.12489, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1253 - val_loss: 0.1249\n",
      "Epoch 261/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1252\n",
      "Epoch 00261: val_loss improved from 0.12489 to 0.12479, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1252 - val_loss: 0.1248\n",
      "Epoch 262/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00262: val_loss improved from 0.12479 to 0.12471, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1251 - val_loss: 0.1247\n",
      "Epoch 263/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1251\n",
      "Epoch 00263: val_loss improved from 0.12471 to 0.12462, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1251 - val_loss: 0.1246\n",
      "Epoch 264/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1250\n",
      "Epoch 00264: val_loss improved from 0.12462 to 0.12453, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1250 - val_loss: 0.1245\n",
      "Epoch 265/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1249\n",
      "Epoch 00265: val_loss improved from 0.12453 to 0.12445, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1249 - val_loss: 0.1244\n",
      "Epoch 266/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1248\n",
      "Epoch 00266: val_loss improved from 0.12445 to 0.12437, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1248 - val_loss: 0.1244\n",
      "Epoch 267/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1247\n",
      "Epoch 00267: val_loss improved from 0.12437 to 0.12428, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1247 - val_loss: 0.1243\n",
      "Epoch 268/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1246\n",
      "Epoch 00268: val_loss improved from 0.12428 to 0.12421, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1246 - val_loss: 0.1242\n",
      "Epoch 269/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1246\n",
      "Epoch 00269: val_loss improved from 0.12421 to 0.12412, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1246 - val_loss: 0.1241\n",
      "Epoch 270/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1245\n",
      "Epoch 00270: val_loss improved from 0.12412 to 0.12404, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1245 - val_loss: 0.1240\n",
      "Epoch 271/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1244\n",
      "Epoch 00271: val_loss improved from 0.12404 to 0.12397, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1244 - val_loss: 0.1240\n",
      "Epoch 272/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.123 - ETA: 0s - loss: 0.1243\n",
      "Epoch 00272: val_loss improved from 0.12397 to 0.12389, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1243 - val_loss: 0.1239\n",
      "Epoch 273/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1243\n",
      "Epoch 00273: val_loss improved from 0.12389 to 0.12381, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1243 - val_loss: 0.1238\n",
      "Epoch 274/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1242\n",
      "Epoch 00274: val_loss improved from 0.12381 to 0.12374, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1242 - val_loss: 0.1237\n",
      "Epoch 275/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.1241\n",
      "Epoch 00275: val_loss improved from 0.12374 to 0.12366, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1241 - val_loss: 0.1237\n",
      "Epoch 276/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00276: val_loss improved from 0.12366 to 0.12359, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1240 - val_loss: 0.1236\n",
      "Epoch 277/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1240\n",
      "Epoch 00277: val_loss improved from 0.12359 to 0.12352, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1240 - val_loss: 0.1235\n",
      "Epoch 278/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1239\n",
      "Epoch 00278: val_loss improved from 0.12352 to 0.12345, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1239 - val_loss: 0.1234\n",
      "Epoch 279/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1238\n",
      "Epoch 00279: val_loss improved from 0.12345 to 0.12338, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1238 - val_loss: 0.1234\n",
      "Epoch 280/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00280: val_loss improved from 0.12338 to 0.12331, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1237 - val_loss: 0.1233\n",
      "Epoch 281/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1237\n",
      "Epoch 00281: val_loss improved from 0.12331 to 0.12324, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1237 - val_loss: 0.1232\n",
      "Epoch 282/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1236\n",
      "Epoch 00282: val_loss improved from 0.12324 to 0.12317, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1236 - val_loss: 0.1232\n",
      "Epoch 283/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1235\n",
      "Epoch 00283: val_loss improved from 0.12317 to 0.12310, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1235 - val_loss: 0.1231\n",
      "Epoch 284/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1235\n",
      "Epoch 00284: val_loss improved from 0.12310 to 0.12303, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1235 - val_loss: 0.1230\n",
      "Epoch 285/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1234\n",
      "Epoch 00285: val_loss improved from 0.12303 to 0.12296, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1234 - val_loss: 0.1230\n",
      "Epoch 286/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1233\n",
      "Epoch 00286: val_loss improved from 0.12296 to 0.12290, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1233 - val_loss: 0.1229\n",
      "Epoch 287/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1233\n",
      "Epoch 00287: val_loss improved from 0.12290 to 0.12283, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1233 - val_loss: 0.1228\n",
      "Epoch 288/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1232\n",
      "Epoch 00288: val_loss improved from 0.12283 to 0.12277, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1232 - val_loss: 0.1228\n",
      "Epoch 289/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00289: val_loss improved from 0.12277 to 0.12271, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1231 - val_loss: 0.1227\n",
      "Epoch 290/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1231\n",
      "Epoch 00290: val_loss improved from 0.12271 to 0.12264, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1231 - val_loss: 0.1226\n",
      "Epoch 291/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1230\n",
      "Epoch 00291: val_loss improved from 0.12264 to 0.12258, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1230 - val_loss: 0.1226\n",
      "Epoch 292/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1230\n",
      "Epoch 00292: val_loss improved from 0.12258 to 0.12251, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1230 - val_loss: 0.1225\n",
      "Epoch 293/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1229\n",
      "Epoch 00293: val_loss improved from 0.12251 to 0.12245, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1229 - val_loss: 0.1225\n",
      "Epoch 294/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1228\n",
      "Epoch 00294: val_loss improved from 0.12245 to 0.12239, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1228 - val_loss: 0.1224\n",
      "Epoch 295/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1228\n",
      "Epoch 00295: val_loss improved from 0.12239 to 0.12233, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1228 - val_loss: 0.1223\n",
      "Epoch 296/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1227\n",
      "Epoch 00296: val_loss improved from 0.12233 to 0.12227, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1227 - val_loss: 0.1223\n",
      "Epoch 297/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1227\n",
      "Epoch 00297: val_loss improved from 0.12227 to 0.12221, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1227 - val_loss: 0.1222\n",
      "Epoch 298/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1226\n",
      "Epoch 00298: val_loss improved from 0.12221 to 0.12215, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1226 - val_loss: 0.1221\n",
      "Epoch 299/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1225\n",
      "Epoch 00299: val_loss improved from 0.12215 to 0.12208, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1225 - val_loss: 0.1221\n",
      "Epoch 300/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1225\n",
      "Epoch 00300: val_loss improved from 0.12208 to 0.12202, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1225 - val_loss: 0.1220\n",
      "Epoch 301/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1224\n",
      "Epoch 00301: val_loss improved from 0.12202 to 0.12196, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1224 - val_loss: 0.1220\n",
      "Epoch 302/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1224\n",
      "Epoch 00302: val_loss improved from 0.12196 to 0.12191, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1224 - val_loss: 0.1219\n",
      "Epoch 303/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1223\n",
      "Epoch 00303: val_loss improved from 0.12191 to 0.12185, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1223 - val_loss: 0.1218\n",
      "Epoch 304/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1223\n",
      "Epoch 00304: val_loss improved from 0.12185 to 0.12179, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1223 - val_loss: 0.1218\n",
      "Epoch 305/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1222\n",
      "Epoch 00305: val_loss improved from 0.12179 to 0.12173, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1222 - val_loss: 0.1217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1221\n",
      "Epoch 00306: val_loss improved from 0.12173 to 0.12168, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1221 - val_loss: 0.1217\n",
      "Epoch 307/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1221\n",
      "Epoch 00307: val_loss improved from 0.12168 to 0.12161, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1221 - val_loss: 0.1216\n",
      "Epoch 308/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1220\n",
      "Epoch 00308: val_loss improved from 0.12161 to 0.12155, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1220 - val_loss: 0.1216\n",
      "Epoch 309/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1220\n",
      "Epoch 00309: val_loss improved from 0.12155 to 0.12150, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1220 - val_loss: 0.1215\n",
      "Epoch 310/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1219\n",
      "Epoch 00310: val_loss improved from 0.12150 to 0.12144, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1219 - val_loss: 0.1214\n",
      "Epoch 311/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1219\n",
      "Epoch 00311: val_loss improved from 0.12144 to 0.12139, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1219 - val_loss: 0.1214\n",
      "Epoch 312/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1218\n",
      "Epoch 00312: val_loss improved from 0.12139 to 0.12133, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1218 - val_loss: 0.1213\n",
      "Epoch 313/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1218\n",
      "Epoch 00313: val_loss improved from 0.12133 to 0.12127, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1218 - val_loss: 0.1213\n",
      "Epoch 314/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1217\n",
      "Epoch 00314: val_loss improved from 0.12127 to 0.12123, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1217 - val_loss: 0.1212\n",
      "Epoch 315/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1217\n",
      "Epoch 00315: val_loss improved from 0.12123 to 0.12117, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1217 - val_loss: 0.1212\n",
      "Epoch 316/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1216\n",
      "Epoch 00316: val_loss improved from 0.12117 to 0.12111, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1216 - val_loss: 0.1211\n",
      "Epoch 317/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1216\n",
      "Epoch 00317: val_loss improved from 0.12111 to 0.12107, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1216 - val_loss: 0.1211\n",
      "Epoch 318/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1215\n",
      "Epoch 00318: val_loss improved from 0.12107 to 0.12101, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1215 - val_loss: 0.1210\n",
      "Epoch 319/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1215\n",
      "Epoch 00319: val_loss improved from 0.12101 to 0.12097, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1215 - val_loss: 0.1210\n",
      "Epoch 320/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1214\n",
      "Epoch 00320: val_loss improved from 0.12097 to 0.12090, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1214 - val_loss: 0.1209\n",
      "Epoch 321/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1214\n",
      "Epoch 00321: val_loss improved from 0.12090 to 0.12084, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1214 - val_loss: 0.1208\n",
      "Epoch 322/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1213\n",
      "Epoch 00322: val_loss improved from 0.12084 to 0.12080, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1213 - val_loss: 0.1208\n",
      "Epoch 323/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1212\n",
      "Epoch 00323: val_loss improved from 0.12080 to 0.12074, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1212 - val_loss: 0.1207\n",
      "Epoch 324/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1212\n",
      "Epoch 00324: val_loss improved from 0.12074 to 0.12069, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1212 - val_loss: 0.1207\n",
      "Epoch 325/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1212\n",
      "Epoch 00325: val_loss improved from 0.12069 to 0.12065, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1212 - val_loss: 0.1207\n",
      "Epoch 326/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1211\n",
      "Epoch 00326: val_loss improved from 0.12065 to 0.12060, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1211 - val_loss: 0.1206\n",
      "Epoch 327/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1211\n",
      "Epoch 00327: val_loss improved from 0.12060 to 0.12054, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1211 - val_loss: 0.1205\n",
      "Epoch 328/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1210\n",
      "Epoch 00328: val_loss improved from 0.12054 to 0.12050, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1210 - val_loss: 0.1205\n",
      "Epoch 329/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1210\n",
      "Epoch 00329: val_loss improved from 0.12050 to 0.12045, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1210 - val_loss: 0.1204\n",
      "Epoch 330/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1209\n",
      "Epoch 00330: val_loss improved from 0.12045 to 0.12040, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1209 - val_loss: 0.1204\n",
      "Epoch 331/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1209\n",
      "Epoch 00331: val_loss improved from 0.12040 to 0.12035, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1209 - val_loss: 0.1204\n",
      "Epoch 332/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1208\n",
      "Epoch 00332: val_loss improved from 0.12035 to 0.12031, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1208 - val_loss: 0.1203\n",
      "Epoch 333/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1208\n",
      "Epoch 00333: val_loss improved from 0.12031 to 0.12026, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1208 - val_loss: 0.1203\n",
      "Epoch 334/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1207\n",
      "Epoch 00334: val_loss improved from 0.12026 to 0.12021, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.1207 - val_loss: 0.1202\n",
      "Epoch 335/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1207\n",
      "Epoch 00335: val_loss improved from 0.12021 to 0.12017, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1207 - val_loss: 0.1202\n",
      "Epoch 336/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1206\n",
      "Epoch 00336: val_loss improved from 0.12017 to 0.12012, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1206 - val_loss: 0.1201\n",
      "Epoch 337/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1206\n",
      "Epoch 00337: val_loss improved from 0.12012 to 0.12008, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1206 - val_loss: 0.1201\n",
      "Epoch 338/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1206\n",
      "Epoch 00338: val_loss improved from 0.12008 to 0.12003, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1206 - val_loss: 0.1200\n",
      "Epoch 339/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1205\n",
      "Epoch 00339: val_loss improved from 0.12003 to 0.11999, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1205 - val_loss: 0.1200\n",
      "Epoch 340/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1205- ETA: 0s - loss: 0.\n",
      "Epoch 00340: val_loss improved from 0.11999 to 0.11994, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1205 - val_loss: 0.1199\n",
      "Epoch 341/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1204\n",
      "Epoch 00341: val_loss improved from 0.11994 to 0.11990, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1204 - val_loss: 0.1199\n",
      "Epoch 342/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1204\n",
      "Epoch 00342: val_loss improved from 0.11990 to 0.11985, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1204 - val_loss: 0.1199\n",
      "Epoch 343/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1203\n",
      "Epoch 00343: val_loss improved from 0.11985 to 0.11981, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1203 - val_loss: 0.1198\n",
      "Epoch 344/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1203\n",
      "Epoch 00344: val_loss improved from 0.11981 to 0.11977, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1203 - val_loss: 0.1198\n",
      "Epoch 345/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1203\n",
      "Epoch 00345: val_loss improved from 0.11977 to 0.11972, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1203 - val_loss: 0.1197\n",
      "Epoch 346/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1202\n",
      "Epoch 00346: val_loss improved from 0.11972 to 0.11969, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1202 - val_loss: 0.1197\n",
      "Epoch 347/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1202\n",
      "Epoch 00347: val_loss improved from 0.11969 to 0.11964, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1202 - val_loss: 0.1196\n",
      "Epoch 348/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1201\n",
      "Epoch 00348: val_loss improved from 0.11964 to 0.11960, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1201 - val_loss: 0.1196\n",
      "Epoch 349/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1201\n",
      "Epoch 00349: val_loss improved from 0.11960 to 0.11956, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1201 - val_loss: 0.1196\n",
      "Epoch 350/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1201\n",
      "Epoch 00350: val_loss improved from 0.11956 to 0.11952, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1201 - val_loss: 0.1195\n",
      "Epoch 351/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1200\n",
      "Epoch 00351: val_loss improved from 0.11952 to 0.11948, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1200 - val_loss: 0.1195\n",
      "Epoch 352/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1200\n",
      "Epoch 00352: val_loss improved from 0.11948 to 0.11944, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1200 - val_loss: 0.1194\n",
      "Epoch 353/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1199\n",
      "Epoch 00353: val_loss improved from 0.11944 to 0.11940, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1199 - val_loss: 0.1194\n",
      "Epoch 354/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1199\n",
      "Epoch 00354: val_loss improved from 0.11940 to 0.11936, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1199 - val_loss: 0.1194\n",
      "Epoch 355/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1199\n",
      "Epoch 00355: val_loss improved from 0.11936 to 0.11932, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1199 - val_loss: 0.1193\n",
      "Epoch 356/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1198\n",
      "Epoch 00356: val_loss improved from 0.11932 to 0.11928, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1198 - val_loss: 0.1193\n",
      "Epoch 357/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1198\n",
      "Epoch 00357: val_loss improved from 0.11928 to 0.11924, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1198 - val_loss: 0.1192\n",
      "Epoch 358/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1197\n",
      "Epoch 00358: val_loss improved from 0.11924 to 0.11920, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1197 - val_loss: 0.1192\n",
      "Epoch 359/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1197- ETA: 0s - loss: 0.119\n",
      "Epoch 00359: val_loss improved from 0.11920 to 0.11916, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1197 - val_loss: 0.1192\n",
      "Epoch 360/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1197\n",
      "Epoch 00360: val_loss improved from 0.11916 to 0.11913, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1197 - val_loss: 0.1191\n",
      "Epoch 361/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1196\n",
      "Epoch 00361: val_loss improved from 0.11913 to 0.11909, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1196 - val_loss: 0.1191\n",
      "Epoch 362/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1196\n",
      "Epoch 00362: val_loss improved from 0.11909 to 0.11905, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1196 - val_loss: 0.1191\n",
      "Epoch 363/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1196\n",
      "Epoch 00363: val_loss improved from 0.11905 to 0.11902, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1196 - val_loss: 0.1190\n",
      "Epoch 364/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1195\n",
      "Epoch 00364: val_loss improved from 0.11902 to 0.11898, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1195 - val_loss: 0.1190\n",
      "Epoch 365/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1195\n",
      "Epoch 00365: val_loss improved from 0.11898 to 0.11895, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1195 - val_loss: 0.1189\n",
      "Epoch 366/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.1195\n",
      "Epoch 00366: val_loss improved from 0.11895 to 0.11891, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1195 - val_loss: 0.1189\n",
      "Epoch 367/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1194\n",
      "Epoch 00367: val_loss improved from 0.11891 to 0.11887, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 0.1194 - val_loss: 0.1189\n",
      "Epoch 368/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1194\n",
      "Epoch 00368: val_loss improved from 0.11887 to 0.11884, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.1194 - val_loss: 0.1188\n",
      "Epoch 369/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1193\n",
      "Epoch 00369: val_loss improved from 0.11884 to 0.11880, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1193 - val_loss: 0.1188\n",
      "Epoch 370/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1193\n",
      "Epoch 00370: val_loss improved from 0.11880 to 0.11876, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1193 - val_loss: 0.1188\n",
      "Epoch 371/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1193\n",
      "Epoch 00371: val_loss improved from 0.11876 to 0.11873, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1193 - val_loss: 0.1187\n",
      "Epoch 372/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1192\n",
      "Epoch 00372: val_loss improved from 0.11873 to 0.11870, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 0.1192 - val_loss: 0.1187\n",
      "Epoch 373/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1192\n",
      "Epoch 00373: val_loss improved from 0.11870 to 0.11866, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1192 - val_loss: 0.1187\n",
      "Epoch 374/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1192\n",
      "Epoch 00374: val_loss improved from 0.11866 to 0.11863, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.1192 - val_loss: 0.1186\n",
      "Epoch 375/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1191\n",
      "Epoch 00375: val_loss improved from 0.11863 to 0.11859, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 112ms/step - loss: 0.1191 - val_loss: 0.1186\n",
      "Epoch 376/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1191\n",
      "Epoch 00376: val_loss improved from 0.11859 to 0.11856, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.1191 - val_loss: 0.1186\n",
      "Epoch 377/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1191\n",
      "Epoch 00377: val_loss improved from 0.11856 to 0.11852, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 0.1191 - val_loss: 0.1185\n",
      "Epoch 378/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1190\n",
      "Epoch 00378: val_loss improved from 0.11852 to 0.11850, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.1190 - val_loss: 0.1185\n",
      "Epoch 379/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1190\n",
      "Epoch 00379: val_loss improved from 0.11850 to 0.11846, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1190 - val_loss: 0.1185\n",
      "Epoch 380/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1190\n",
      "Epoch 00380: val_loss improved from 0.11846 to 0.11843, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1190 - val_loss: 0.1184\n",
      "Epoch 381/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1189\n",
      "Epoch 00381: val_loss improved from 0.11843 to 0.11840, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1189 - val_loss: 0.1184\n",
      "Epoch 382/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1189\n",
      "Epoch 00382: val_loss improved from 0.11840 to 0.11837, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1189 - val_loss: 0.1184\n",
      "Epoch 383/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1189\n",
      "Epoch 00383: val_loss improved from 0.11837 to 0.11833, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1189 - val_loss: 0.1183\n",
      "Epoch 384/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1189\n",
      "Epoch 00384: val_loss improved from 0.11833 to 0.11831, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1189 - val_loss: 0.1183\n",
      "Epoch 385/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1188\n",
      "Epoch 00385: val_loss improved from 0.11831 to 0.11827, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1188 - val_loss: 0.1183\n",
      "Epoch 386/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1188\n",
      "Epoch 00386: val_loss improved from 0.11827 to 0.11824, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1188 - val_loss: 0.1182\n",
      "Epoch 387/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1188\n",
      "Epoch 00387: val_loss improved from 0.11824 to 0.11821, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1188 - val_loss: 0.1182\n",
      "Epoch 388/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1187\n",
      "Epoch 00388: val_loss improved from 0.11821 to 0.11818, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1187 - val_loss: 0.1182\n",
      "Epoch 389/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1187\n",
      "Epoch 00389: val_loss improved from 0.11818 to 0.11815, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1187 - val_loss: 0.1182\n",
      "Epoch 390/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1187\n",
      "Epoch 00390: val_loss improved from 0.11815 to 0.11813, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1187 - val_loss: 0.1181\n",
      "Epoch 391/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1186- ETA: 0s - loss: 0.119\n",
      "Epoch 00391: val_loss improved from 0.11813 to 0.11809, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1186 - val_loss: 0.1181\n",
      "Epoch 392/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1186\n",
      "Epoch 00392: val_loss improved from 0.11809 to 0.11806, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1186 - val_loss: 0.1181\n",
      "Epoch 393/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1186\n",
      "Epoch 00393: val_loss improved from 0.11806 to 0.11803, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1186 - val_loss: 0.1180\n",
      "Epoch 394/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1186\n",
      "Epoch 00394: val_loss improved from 0.11803 to 0.11801, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1186 - val_loss: 0.1180\n",
      "Epoch 395/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1185\n",
      "Epoch 00395: val_loss improved from 0.11801 to 0.11797, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1185 - val_loss: 0.1180\n",
      "Epoch 396/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1185\n",
      "Epoch 00396: val_loss improved from 0.11797 to 0.11795, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1185 - val_loss: 0.1179\n",
      "Epoch 397/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1185\n",
      "Epoch 00397: val_loss improved from 0.11795 to 0.11792, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1185 - val_loss: 0.1179\n",
      "Epoch 398/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1184\n",
      "Epoch 00398: val_loss improved from 0.11792 to 0.11789, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1184 - val_loss: 0.1179\n",
      "Epoch 399/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1184\n",
      "Epoch 00399: val_loss improved from 0.11789 to 0.11786, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1184 - val_loss: 0.1179\n",
      "Epoch 400/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1184\n",
      "Epoch 00400: val_loss improved from 0.11786 to 0.11783, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1184 - val_loss: 0.1178\n",
      "Epoch 401/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1184\n",
      "Epoch 00401: val_loss improved from 0.11783 to 0.11781, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1184 - val_loss: 0.1178\n",
      "Epoch 402/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1183\n",
      "Epoch 00402: val_loss improved from 0.11781 to 0.11778, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1183 - val_loss: 0.1178\n",
      "Epoch 403/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1183\n",
      "Epoch 00403: val_loss improved from 0.11778 to 0.11775, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1183 - val_loss: 0.1177\n",
      "Epoch 404/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1183\n",
      "Epoch 00404: val_loss improved from 0.11775 to 0.11773, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1183 - val_loss: 0.1177\n",
      "Epoch 405/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1182\n",
      "Epoch 00405: val_loss improved from 0.11773 to 0.11770, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1182 - val_loss: 0.1177\n",
      "Epoch 406/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1182\n",
      "Epoch 00406: val_loss improved from 0.11770 to 0.11767, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1182 - val_loss: 0.1177\n",
      "Epoch 407/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1182\n",
      "Epoch 00407: val_loss improved from 0.11767 to 0.11764, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1182 - val_loss: 0.1176\n",
      "Epoch 408/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1182\n",
      "Epoch 00408: val_loss improved from 0.11764 to 0.11762, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1182 - val_loss: 0.1176\n",
      "Epoch 409/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1181\n",
      "Epoch 00409: val_loss improved from 0.11762 to 0.11759, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1181 - val_loss: 0.1176\n",
      "Epoch 410/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1181\n",
      "Epoch 00410: val_loss improved from 0.11759 to 0.11756, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1181 - val_loss: 0.1176\n",
      "Epoch 411/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1181\n",
      "Epoch 00411: val_loss improved from 0.11756 to 0.11753, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1181 - val_loss: 0.1175\n",
      "Epoch 412/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1181\n",
      "Epoch 00412: val_loss improved from 0.11753 to 0.11751, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1181 - val_loss: 0.1175\n",
      "Epoch 413/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1180\n",
      "Epoch 00413: val_loss improved from 0.11751 to 0.11748, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1180 - val_loss: 0.1175\n",
      "Epoch 414/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1180\n",
      "Epoch 00414: val_loss improved from 0.11748 to 0.11747, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1180 - val_loss: 0.1175\n",
      "Epoch 415/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1180\n",
      "Epoch 00415: val_loss improved from 0.11747 to 0.11743, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1180 - val_loss: 0.1174\n",
      "Epoch 416/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1180\n",
      "Epoch 00416: val_loss improved from 0.11743 to 0.11741, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1180 - val_loss: 0.1174\n",
      "Epoch 417/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1179\n",
      "Epoch 00417: val_loss improved from 0.11741 to 0.11739, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1179 - val_loss: 0.1174\n",
      "Epoch 418/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1179\n",
      "Epoch 00418: val_loss improved from 0.11739 to 0.11735, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1179 - val_loss: 0.1174\n",
      "Epoch 419/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1179\n",
      "Epoch 00419: val_loss improved from 0.11735 to 0.11733, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1179 - val_loss: 0.1173\n",
      "Epoch 420/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1179\n",
      "Epoch 00420: val_loss did not improve from 0.11733\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1179 - val_loss: 0.1173\n",
      "Epoch 421/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1178\n",
      "Epoch 00421: val_loss improved from 0.11733 to 0.11728, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1178 - val_loss: 0.1173\n",
      "Epoch 422/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1178\n",
      "Epoch 00422: val_loss improved from 0.11728 to 0.11726, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1178 - val_loss: 0.1173\n",
      "Epoch 423/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1178\n",
      "Epoch 00423: val_loss improved from 0.11726 to 0.11724, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1178 - val_loss: 0.1172\n",
      "Epoch 424/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1178\n",
      "Epoch 00424: val_loss improved from 0.11724 to 0.11721, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1178 - val_loss: 0.1172\n",
      "Epoch 425/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1177\n",
      "Epoch 00425: val_loss improved from 0.11721 to 0.11720, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1177 - val_loss: 0.1172\n",
      "Epoch 426/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1177\n",
      "Epoch 00426: val_loss improved from 0.11720 to 0.11716, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1177 - val_loss: 0.1172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1177\n",
      "Epoch 00427: val_loss improved from 0.11716 to 0.11715, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1177 - val_loss: 0.1171\n",
      "Epoch 428/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1177\n",
      "Epoch 00428: val_loss improved from 0.11715 to 0.11711, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1177 - val_loss: 0.1171\n",
      "Epoch 429/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1176\n",
      "Epoch 00429: val_loss improved from 0.11711 to 0.11709, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1176 - val_loss: 0.1171\n",
      "Epoch 430/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1176- ETA: 0s - loss: 0\n",
      "Epoch 00430: val_loss improved from 0.11709 to 0.11707, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1176 - val_loss: 0.1171\n",
      "Epoch 431/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1176\n",
      "Epoch 00431: val_loss improved from 0.11707 to 0.11704, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1176 - val_loss: 0.1170\n",
      "Epoch 432/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1176\n",
      "Epoch 00432: val_loss improved from 0.11704 to 0.11702, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1176 - val_loss: 0.1170\n",
      "Epoch 433/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1175\n",
      "Epoch 00433: val_loss improved from 0.11702 to 0.11700, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1175 - val_loss: 0.1170\n",
      "Epoch 434/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1175\n",
      "Epoch 00434: val_loss improved from 0.11700 to 0.11697, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1175 - val_loss: 0.1170\n",
      "Epoch 435/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1175\n",
      "Epoch 00435: val_loss improved from 0.11697 to 0.11695, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1175 - val_loss: 0.1170\n",
      "Epoch 436/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1175\n",
      "Epoch 00436: val_loss improved from 0.11695 to 0.11693, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1175 - val_loss: 0.1169\n",
      "Epoch 437/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1175\n",
      "Epoch 00437: val_loss improved from 0.11693 to 0.11691, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1175 - val_loss: 0.1169\n",
      "Epoch 438/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1174\n",
      "Epoch 00438: val_loss did not improve from 0.11691\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1174 - val_loss: 0.1169\n",
      "Epoch 439/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1174\n",
      "Epoch 00439: val_loss improved from 0.11691 to 0.11686, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1174 - val_loss: 0.1169\n",
      "Epoch 440/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1174\n",
      "Epoch 00440: val_loss improved from 0.11686 to 0.11684, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1174 - val_loss: 0.1168\n",
      "Epoch 441/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1174\n",
      "Epoch 00441: val_loss improved from 0.11684 to 0.11681, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1174 - val_loss: 0.1168\n",
      "Epoch 442/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1173\n",
      "Epoch 00442: val_loss improved from 0.11681 to 0.11679, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1173 - val_loss: 0.1168\n",
      "Epoch 443/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1173\n",
      "Epoch 00443: val_loss improved from 0.11679 to 0.11677, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1173 - val_loss: 0.1168\n",
      "Epoch 444/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1173\n",
      "Epoch 00444: val_loss improved from 0.11677 to 0.11675, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1173 - val_loss: 0.1167\n",
      "Epoch 445/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1173\n",
      "Epoch 00445: val_loss improved from 0.11675 to 0.11672, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1173 - val_loss: 0.1167\n",
      "Epoch 446/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1172\n",
      "Epoch 00446: val_loss improved from 0.11672 to 0.11671, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1172 - val_loss: 0.1167\n",
      "Epoch 447/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1172\n",
      "Epoch 00447: val_loss improved from 0.11671 to 0.11668, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1172 - val_loss: 0.1167\n",
      "Epoch 448/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1172\n",
      "Epoch 00448: val_loss improved from 0.11668 to 0.11666, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1172 - val_loss: 0.1167\n",
      "Epoch 449/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1172\n",
      "Epoch 00449: val_loss improved from 0.11666 to 0.11665, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1172 - val_loss: 0.1166\n",
      "Epoch 450/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1172- ETA: 0s - loss: 0.1\n",
      "Epoch 00450: val_loss improved from 0.11665 to 0.11662, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1172 - val_loss: 0.1166\n",
      "Epoch 451/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1171\n",
      "Epoch 00451: val_loss improved from 0.11662 to 0.11661, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1171 - val_loss: 0.1166\n",
      "Epoch 452/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1171\n",
      "Epoch 00452: val_loss improved from 0.11661 to 0.11657, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1171 - val_loss: 0.1166\n",
      "Epoch 453/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1171\n",
      "Epoch 00453: val_loss improved from 0.11657 to 0.11655, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1171 - val_loss: 0.1166\n",
      "Epoch 454/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1171\n",
      "Epoch 00454: val_loss improved from 0.11655 to 0.11653, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1171 - val_loss: 0.1165\n",
      "Epoch 455/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1170\n",
      "Epoch 00455: val_loss improved from 0.11653 to 0.11651, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1170 - val_loss: 0.1165\n",
      "Epoch 456/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1170\n",
      "Epoch 00456: val_loss improved from 0.11651 to 0.11649, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1170 - val_loss: 0.1165\n",
      "Epoch 457/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1170\n",
      "Epoch 00457: val_loss improved from 0.11649 to 0.11646, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1170 - val_loss: 0.1165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 458/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1170\n",
      "Epoch 00458: val_loss improved from 0.11646 to 0.11644, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1170 - val_loss: 0.1164\n",
      "Epoch 459/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1170\n",
      "Epoch 00459: val_loss improved from 0.11644 to 0.11642, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1170 - val_loss: 0.1164\n",
      "Epoch 460/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1169\n",
      "Epoch 00460: val_loss improved from 0.11642 to 0.11640, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1169 - val_loss: 0.1164\n",
      "Epoch 461/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1169\n",
      "Epoch 00461: val_loss improved from 0.11640 to 0.11638, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1169 - val_loss: 0.1164\n",
      "Epoch 462/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1169\n",
      "Epoch 00462: val_loss did not improve from 0.11638\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1169 - val_loss: 0.1164\n",
      "Epoch 463/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1169\n",
      "Epoch 00463: val_loss improved from 0.11638 to 0.11633, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1169 - val_loss: 0.1163\n",
      "Epoch 464/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1169\n",
      "Epoch 00464: val_loss improved from 0.11633 to 0.11631, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1169 - val_loss: 0.1163\n",
      "Epoch 465/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00465: val_loss improved from 0.11631 to 0.11629, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1168 - val_loss: 0.1163\n",
      "Epoch 466/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00466: val_loss improved from 0.11629 to 0.11627, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1168 - val_loss: 0.1163\n",
      "Epoch 467/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00467: val_loss improved from 0.11627 to 0.11625, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1168 - val_loss: 0.1162\n",
      "Epoch 468/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1168\n",
      "Epoch 00468: val_loss improved from 0.11625 to 0.11624, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1168 - val_loss: 0.1162\n",
      "Epoch 469/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00469: val_loss improved from 0.11624 to 0.11621, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1167 - val_loss: 0.1162\n",
      "Epoch 470/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00470: val_loss improved from 0.11621 to 0.11618, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1167 - val_loss: 0.1162\n",
      "Epoch 471/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00471: val_loss improved from 0.11618 to 0.11616, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1167 - val_loss: 0.1162\n",
      "Epoch 472/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00472: val_loss improved from 0.11616 to 0.11615, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1167 - val_loss: 0.1161\n",
      "Epoch 473/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1167\n",
      "Epoch 00473: val_loss improved from 0.11615 to 0.11612, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1167 - val_loss: 0.1161\n",
      "Epoch 474/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1166\n",
      "Epoch 00474: val_loss improved from 0.11612 to 0.11612, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1166 - val_loss: 0.1161\n",
      "Epoch 475/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1166\n",
      "Epoch 00475: val_loss improved from 0.11612 to 0.11608, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1166 - val_loss: 0.1161\n",
      "Epoch 476/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1166\n",
      "Epoch 00476: val_loss improved from 0.11608 to 0.11605, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1166 - val_loss: 0.1161\n",
      "Epoch 477/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1166\n",
      "Epoch 00477: val_loss improved from 0.11605 to 0.11603, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1166 - val_loss: 0.1160\n",
      "Epoch 478/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1166\n",
      "Epoch 00478: val_loss improved from 0.11603 to 0.11603, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1166 - val_loss: 0.1160\n",
      "Epoch 479/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1165\n",
      "Epoch 00479: val_loss improved from 0.11603 to 0.11599, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1165 - val_loss: 0.1160\n",
      "Epoch 480/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1165\n",
      "Epoch 00480: val_loss improved from 0.11599 to 0.11598, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1165 - val_loss: 0.1160\n",
      "Epoch 481/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1165\n",
      "Epoch 00481: val_loss improved from 0.11598 to 0.11595, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1165 - val_loss: 0.1160\n",
      "Epoch 482/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1165\n",
      "Epoch 00482: val_loss improved from 0.11595 to 0.11594, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1165 - val_loss: 0.1159\n",
      "Epoch 483/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1165\n",
      "Epoch 00483: val_loss improved from 0.11594 to 0.11591, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1165 - val_loss: 0.1159\n",
      "Epoch 484/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1164\n",
      "Epoch 00484: val_loss improved from 0.11591 to 0.11589, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1164 - val_loss: 0.1159\n",
      "Epoch 485/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1164\n",
      "Epoch 00485: val_loss improved from 0.11589 to 0.11587, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1164 - val_loss: 0.1159\n",
      "Epoch 486/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1164\n",
      "Epoch 00486: val_loss improved from 0.11587 to 0.11585, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1164 - val_loss: 0.1158\n",
      "Epoch 487/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1164\n",
      "Epoch 00487: val_loss improved from 0.11585 to 0.11583, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1164 - val_loss: 0.1158\n",
      "Epoch 488/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1163\n",
      "Epoch 00488: val_loss improved from 0.11583 to 0.11580, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1163 - val_loss: 0.1158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 489/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1163\n",
      "Epoch 00489: val_loss did not improve from 0.11580\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1163 - val_loss: 0.1158\n",
      "Epoch 490/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1163\n",
      "Epoch 00490: val_loss improved from 0.11580 to 0.11576, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1163 - val_loss: 0.1158\n",
      "Epoch 491/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1163\n",
      "Epoch 00491: val_loss improved from 0.11576 to 0.11574, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1163 - val_loss: 0.1157\n",
      "Epoch 492/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1163\n",
      "Epoch 00492: val_loss improved from 0.11574 to 0.11572, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1163 - val_loss: 0.1157\n",
      "Epoch 493/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1162\n",
      "Epoch 00493: val_loss improved from 0.11572 to 0.11570, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1162 - val_loss: 0.1157\n",
      "Epoch 494/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1162\n",
      "Epoch 00494: val_loss improved from 0.11570 to 0.11568, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1162 - val_loss: 0.1157\n",
      "Epoch 495/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1162\n",
      "Epoch 00495: val_loss improved from 0.11568 to 0.11566, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1162 - val_loss: 0.1157\n",
      "Epoch 496/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1162\n",
      "Epoch 00496: val_loss improved from 0.11566 to 0.11564, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1162 - val_loss: 0.1156\n",
      "Epoch 497/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1162\n",
      "Epoch 00497: val_loss improved from 0.11564 to 0.11561, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1162 - val_loss: 0.1156\n",
      "Epoch 498/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1161- ETA: 0s - loss: 0.11\n",
      "Epoch 00498: val_loss improved from 0.11561 to 0.11560, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1161 - val_loss: 0.1156\n",
      "Epoch 499/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1161\n",
      "Epoch 00499: val_loss improved from 0.11560 to 0.11557, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1161 - val_loss: 0.1156\n",
      "Epoch 500/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1161\n",
      "Epoch 00500: val_loss did not improve from 0.11557\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1161 - val_loss: 0.1156\n",
      "Epoch 501/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1161\n",
      "Epoch 00501: val_loss improved from 0.11557 to 0.11553, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1161 - val_loss: 0.1155\n",
      "Epoch 502/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1161\n",
      "Epoch 00502: val_loss improved from 0.11553 to 0.11551, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1161 - val_loss: 0.1155\n",
      "Epoch 503/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1160\n",
      "Epoch 00503: val_loss improved from 0.11551 to 0.11550, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1160 - val_loss: 0.1155\n",
      "Epoch 504/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1160\n",
      "Epoch 00504: val_loss improved from 0.11550 to 0.11547, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1160 - val_loss: 0.1155\n",
      "Epoch 505/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1160\n",
      "Epoch 00505: val_loss improved from 0.11547 to 0.11545, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1160 - val_loss: 0.1155\n",
      "Epoch 506/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1160\n",
      "Epoch 00506: val_loss improved from 0.11545 to 0.11543, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1160 - val_loss: 0.1154\n",
      "Epoch 507/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1159\n",
      "Epoch 00507: val_loss improved from 0.11543 to 0.11541, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1159 - val_loss: 0.1154\n",
      "Epoch 508/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1159\n",
      "Epoch 00508: val_loss improved from 0.11541 to 0.11539, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1159 - val_loss: 0.1154\n",
      "Epoch 509/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1159\n",
      "Epoch 00509: val_loss improved from 0.11539 to 0.11537, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1159 - val_loss: 0.1154\n",
      "Epoch 510/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1159\n",
      "Epoch 00510: val_loss improved from 0.11537 to 0.11534, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1159 - val_loss: 0.1153\n",
      "Epoch 511/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1159\n",
      "Epoch 00511: val_loss improved from 0.11534 to 0.11533, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1159 - val_loss: 0.1153\n",
      "Epoch 512/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1158\n",
      "Epoch 00512: val_loss improved from 0.11533 to 0.11531, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1158 - val_loss: 0.1153\n",
      "Epoch 513/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1158\n",
      "Epoch 00513: val_loss improved from 0.11531 to 0.11528, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1158 - val_loss: 0.1153\n",
      "Epoch 514/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1158\n",
      "Epoch 00514: val_loss improved from 0.11528 to 0.11527, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1158 - val_loss: 0.1153\n",
      "Epoch 515/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1158\n",
      "Epoch 00515: val_loss improved from 0.11527 to 0.11524, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1158 - val_loss: 0.1152\n",
      "Epoch 516/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1158\n",
      "Epoch 00516: val_loss improved from 0.11524 to 0.11522, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1158 - val_loss: 0.1152\n",
      "Epoch 517/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1157\n",
      "Epoch 00517: val_loss improved from 0.11522 to 0.11521, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1157 - val_loss: 0.1152\n",
      "Epoch 518/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1157\n",
      "Epoch 00518: val_loss improved from 0.11521 to 0.11518, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1157 - val_loss: 0.1152\n",
      "Epoch 519/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1157\n",
      "Epoch 00519: val_loss improved from 0.11518 to 0.11516, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1157 - val_loss: 0.1152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 520/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1157\n",
      "Epoch 00520: val_loss improved from 0.11516 to 0.11515, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1157 - val_loss: 0.1151\n",
      "Epoch 521/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1157\n",
      "Epoch 00521: val_loss improved from 0.11515 to 0.11511, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1157 - val_loss: 0.1151\n",
      "Epoch 522/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1156\n",
      "Epoch 00522: val_loss improved from 0.11511 to 0.11510, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1156 - val_loss: 0.1151\n",
      "Epoch 523/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1156\n",
      "Epoch 00523: val_loss improved from 0.11510 to 0.11508, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1156 - val_loss: 0.1151\n",
      "Epoch 524/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1156\n",
      "Epoch 00524: val_loss improved from 0.11508 to 0.11505, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1156 - val_loss: 0.1151\n",
      "Epoch 525/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1156\n",
      "Epoch 00525: val_loss improved from 0.11505 to 0.11503, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1156 - val_loss: 0.1150\n",
      "Epoch 526/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1156\n",
      "Epoch 00526: val_loss improved from 0.11503 to 0.11503, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1156 - val_loss: 0.1150\n",
      "Epoch 527/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1155\n",
      "Epoch 00527: val_loss improved from 0.11503 to 0.11499, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1155 - val_loss: 0.1150\n",
      "Epoch 528/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1155\n",
      "Epoch 00528: val_loss improved from 0.11499 to 0.11497, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1155 - val_loss: 0.1150\n",
      "Epoch 529/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1155\n",
      "Epoch 00529: val_loss improved from 0.11497 to 0.11496, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1155 - val_loss: 0.1150\n",
      "Epoch 530/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1155\n",
      "Epoch 00530: val_loss improved from 0.11496 to 0.11493, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1155 - val_loss: 0.1149\n",
      "Epoch 531/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1155\n",
      "Epoch 00531: val_loss improved from 0.11493 to 0.11491, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1155 - val_loss: 0.1149\n",
      "Epoch 532/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1154\n",
      "Epoch 00532: val_loss improved from 0.11491 to 0.11489, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1154 - val_loss: 0.1149\n",
      "Epoch 533/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1154\n",
      "Epoch 00533: val_loss improved from 0.11489 to 0.11487, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1154 - val_loss: 0.1149\n",
      "Epoch 534/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1154\n",
      "Epoch 00534: val_loss improved from 0.11487 to 0.11485, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1154 - val_loss: 0.1149\n",
      "Epoch 535/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1154\n",
      "Epoch 00535: val_loss improved from 0.11485 to 0.11483, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1154 - val_loss: 0.1148\n",
      "Epoch 536/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1154\n",
      "Epoch 00536: val_loss improved from 0.11483 to 0.11481, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1154 - val_loss: 0.1148\n",
      "Epoch 537/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1153\n",
      "Epoch 00537: val_loss improved from 0.11481 to 0.11480, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1153 - val_loss: 0.1148\n",
      "Epoch 538/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1153\n",
      "Epoch 00538: val_loss improved from 0.11480 to 0.11477, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1153 - val_loss: 0.1148\n",
      "Epoch 539/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1153\n",
      "Epoch 00539: val_loss improved from 0.11477 to 0.11476, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1153 - val_loss: 0.1148\n",
      "Epoch 540/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1153\n",
      "Epoch 00540: val_loss improved from 0.11476 to 0.11473, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1153 - val_loss: 0.1147\n",
      "Epoch 541/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1153\n",
      "Epoch 00541: val_loss improved from 0.11473 to 0.11471, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1153 - val_loss: 0.1147\n",
      "Epoch 542/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1152\n",
      "Epoch 00542: val_loss improved from 0.11471 to 0.11469, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1152 - val_loss: 0.1147\n",
      "Epoch 543/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1152\n",
      "Epoch 00543: val_loss improved from 0.11469 to 0.11467, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1152 - val_loss: 0.1147\n",
      "Epoch 544/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1152\n",
      "Epoch 00544: val_loss improved from 0.11467 to 0.11467, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1152 - val_loss: 0.1147\n",
      "Epoch 545/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1152\n",
      "Epoch 00545: val_loss improved from 0.11467 to 0.11463, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1152 - val_loss: 0.1146\n",
      "Epoch 546/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1152\n",
      "Epoch 00546: val_loss improved from 0.11463 to 0.11461, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1152 - val_loss: 0.1146\n",
      "Epoch 547/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1151\n",
      "Epoch 00547: val_loss improved from 0.11461 to 0.11459, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1151 - val_loss: 0.1146\n",
      "Epoch 548/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1151\n",
      "Epoch 00548: val_loss improved from 0.11459 to 0.11457, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1151 - val_loss: 0.1146\n",
      "Epoch 549/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1151\n",
      "Epoch 00549: val_loss improved from 0.11457 to 0.11456, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1151 - val_loss: 0.1146\n",
      "Epoch 550/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1151\n",
      "Epoch 00550: val_loss improved from 0.11456 to 0.11453, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1151 - val_loss: 0.1145\n",
      "Epoch 551/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1150\n",
      "Epoch 00551: val_loss improved from 0.11453 to 0.11451, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1150 - val_loss: 0.1145\n",
      "Epoch 552/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1150\n",
      "Epoch 00552: val_loss improved from 0.11451 to 0.11450, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1150 - val_loss: 0.1145\n",
      "Epoch 553/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1150\n",
      "Epoch 00553: val_loss improved from 0.11450 to 0.11447, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1150 - val_loss: 0.1145\n",
      "Epoch 554/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1150\n",
      "Epoch 00554: val_loss improved from 0.11447 to 0.11445, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1150 - val_loss: 0.1144\n",
      "Epoch 555/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1150\n",
      "Epoch 00555: val_loss improved from 0.11445 to 0.11443, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1150 - val_loss: 0.1144\n",
      "Epoch 556/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1149\n",
      "Epoch 00556: val_loss improved from 0.11443 to 0.11440, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1149 - val_loss: 0.1144\n",
      "Epoch 557/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1149\n",
      "Epoch 00557: val_loss improved from 0.11440 to 0.11438, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1149 - val_loss: 0.1144\n",
      "Epoch 558/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1149\n",
      "Epoch 00558: val_loss improved from 0.11438 to 0.11436, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1149 - val_loss: 0.1144\n",
      "Epoch 559/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1149\n",
      "Epoch 00559: val_loss improved from 0.11436 to 0.11434, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1149 - val_loss: 0.1143\n",
      "Epoch 560/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1149\n",
      "Epoch 00560: val_loss improved from 0.11434 to 0.11433, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1149 - val_loss: 0.1143\n",
      "Epoch 561/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1148\n",
      "Epoch 00561: val_loss improved from 0.11433 to 0.11430, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1148 - val_loss: 0.1143\n",
      "Epoch 562/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1148\n",
      "Epoch 00562: val_loss improved from 0.11430 to 0.11428, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1148 - val_loss: 0.1143\n",
      "Epoch 563/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1148\n",
      "Epoch 00563: val_loss improved from 0.11428 to 0.11426, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1148 - val_loss: 0.1143\n",
      "Epoch 564/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1148\n",
      "Epoch 00564: val_loss improved from 0.11426 to 0.11424, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1148 - val_loss: 0.1142\n",
      "Epoch 565/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1148\n",
      "Epoch 00565: val_loss improved from 0.11424 to 0.11422, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1148 - val_loss: 0.1142\n",
      "Epoch 566/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1147\n",
      "Epoch 00566: val_loss improved from 0.11422 to 0.11419, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1147 - val_loss: 0.1142\n",
      "Epoch 567/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1147\n",
      "Epoch 00567: val_loss improved from 0.11419 to 0.11417, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1147 - val_loss: 0.1142\n",
      "Epoch 568/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1147\n",
      "Epoch 00568: val_loss improved from 0.11417 to 0.11414, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1147 - val_loss: 0.1141\n",
      "Epoch 569/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1147\n",
      "Epoch 00569: val_loss improved from 0.11414 to 0.11412, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1147 - val_loss: 0.1141\n",
      "Epoch 570/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1146\n",
      "Epoch 00570: val_loss improved from 0.11412 to 0.11409, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1146 - val_loss: 0.1141\n",
      "Epoch 571/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1146\n",
      "Epoch 00571: val_loss improved from 0.11409 to 0.11406, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1146 - val_loss: 0.1141\n",
      "Epoch 572/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1146\n",
      "Epoch 00572: val_loss improved from 0.11406 to 0.11405, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1146 - val_loss: 0.1141\n",
      "Epoch 573/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1146\n",
      "Epoch 00573: val_loss improved from 0.11405 to 0.11402, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1146 - val_loss: 0.1140\n",
      "Epoch 574/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1145\n",
      "Epoch 00574: val_loss improved from 0.11402 to 0.11400, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1145 - val_loss: 0.1140\n",
      "Epoch 575/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1145\n",
      "Epoch 00575: val_loss improved from 0.11400 to 0.11397, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1145 - val_loss: 0.1140\n",
      "Epoch 576/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1145\n",
      "Epoch 00576: val_loss improved from 0.11397 to 0.11396, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1145 - val_loss: 0.1140\n",
      "Epoch 577/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1145\n",
      "Epoch 00577: val_loss improved from 0.11396 to 0.11393, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1145 - val_loss: 0.1139\n",
      "Epoch 578/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1145\n",
      "Epoch 00578: val_loss improved from 0.11393 to 0.11392, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1145 - val_loss: 0.1139\n",
      "Epoch 579/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1144\n",
      "Epoch 00579: val_loss improved from 0.11392 to 0.11389, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1144 - val_loss: 0.1139\n",
      "Epoch 580/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1144\n",
      "Epoch 00580: val_loss improved from 0.11389 to 0.11385, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1144 - val_loss: 0.1139\n",
      "Epoch 581/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1144\n",
      "Epoch 00581: val_loss improved from 0.11385 to 0.11384, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1144 - val_loss: 0.1138\n",
      "Epoch 582/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1144\n",
      "Epoch 00582: val_loss improved from 0.11384 to 0.11381, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1144 - val_loss: 0.1138\n",
      "Epoch 583/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1143\n",
      "Epoch 00583: val_loss improved from 0.11381 to 0.11379, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1143 - val_loss: 0.1138\n",
      "Epoch 584/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1143\n",
      "Epoch 00584: val_loss improved from 0.11379 to 0.11377, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1143 - val_loss: 0.1138\n",
      "Epoch 585/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1143\n",
      "Epoch 00585: val_loss improved from 0.11377 to 0.11374, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1143 - val_loss: 0.1137\n",
      "Epoch 586/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1143\n",
      "Epoch 00586: val_loss improved from 0.11374 to 0.11373, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1143 - val_loss: 0.1137\n",
      "Epoch 587/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1143\n",
      "Epoch 00587: val_loss improved from 0.11373 to 0.11370, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1143 - val_loss: 0.1137\n",
      "Epoch 588/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1142\n",
      "Epoch 00588: val_loss improved from 0.11370 to 0.11367, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1142 - val_loss: 0.1137\n",
      "Epoch 589/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1142\n",
      "Epoch 00589: val_loss improved from 0.11367 to 0.11366, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1142 - val_loss: 0.1137\n",
      "Epoch 590/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1142\n",
      "Epoch 00590: val_loss improved from 0.11366 to 0.11363, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1142 - val_loss: 0.1136\n",
      "Epoch 591/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1142\n",
      "Epoch 00591: val_loss improved from 0.11363 to 0.11360, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1142 - val_loss: 0.1136\n",
      "Epoch 592/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1141\n",
      "Epoch 00592: val_loss improved from 0.11360 to 0.11359, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.1141 - val_loss: 0.1136\n",
      "Epoch 593/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1141\n",
      "Epoch 00593: val_loss improved from 0.11359 to 0.11356, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.1141 - val_loss: 0.1136\n",
      "Epoch 594/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1141\n",
      "Epoch 00594: val_loss improved from 0.11356 to 0.11356, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1141 - val_loss: 0.1136\n",
      "Epoch 595/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1141\n",
      "Epoch 00595: val_loss improved from 0.11356 to 0.11352, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1141 - val_loss: 0.1135\n",
      "Epoch 596/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1140\n",
      "Epoch 00596: val_loss improved from 0.11352 to 0.11349, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1140 - val_loss: 0.1135\n",
      "Epoch 597/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1140\n",
      "Epoch 00597: val_loss improved from 0.11349 to 0.11349, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1140 - val_loss: 0.1135\n",
      "Epoch 598/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1140\n",
      "Epoch 00598: val_loss improved from 0.11349 to 0.11344, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1140 - val_loss: 0.1134\n",
      "Epoch 599/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1140\n",
      "Epoch 00599: val_loss improved from 0.11344 to 0.11343, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1140 - val_loss: 0.1134\n",
      "Epoch 600/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1140\n",
      "Epoch 00600: val_loss improved from 0.11343 to 0.11340, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1140 - val_loss: 0.1134\n",
      "Epoch 601/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1139\n",
      "Epoch 00601: val_loss improved from 0.11340 to 0.11338, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1139 - val_loss: 0.1134\n",
      "Epoch 602/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1139\n",
      "Epoch 00602: val_loss improved from 0.11338 to 0.11336, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1139 - val_loss: 0.1134\n",
      "Epoch 603/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1139\n",
      "Epoch 00603: val_loss improved from 0.11336 to 0.11334, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1139 - val_loss: 0.1133\n",
      "Epoch 604/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1139\n",
      "Epoch 00604: val_loss improved from 0.11334 to 0.11331, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1139 - val_loss: 0.1133\n",
      "Epoch 605/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1138\n",
      "Epoch 00605: val_loss improved from 0.11331 to 0.11328, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1138 - val_loss: 0.1133\n",
      "Epoch 606/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1138\n",
      "Epoch 00606: val_loss improved from 0.11328 to 0.11327, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1138 - val_loss: 0.1133\n",
      "Epoch 607/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1138\n",
      "Epoch 00607: val_loss improved from 0.11327 to 0.11324, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1138 - val_loss: 0.1132\n",
      "Epoch 608/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1138\n",
      "Epoch 00608: val_loss improved from 0.11324 to 0.11321, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1138 - val_loss: 0.1132\n",
      "Epoch 609/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1138\n",
      "Epoch 00609: val_loss improved from 0.11321 to 0.11320, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1138 - val_loss: 0.1132\n",
      "Epoch 610/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1137\n",
      "Epoch 00610: val_loss improved from 0.11320 to 0.11317, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1137 - val_loss: 0.1132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 611/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1137\n",
      "Epoch 00611: val_loss improved from 0.11317 to 0.11315, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1137 - val_loss: 0.1131\n",
      "Epoch 612/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1137\n",
      "Epoch 00612: val_loss improved from 0.11315 to 0.11313, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1137 - val_loss: 0.1131\n",
      "Epoch 613/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1137\n",
      "Epoch 00613: val_loss improved from 0.11313 to 0.11311, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1137 - val_loss: 0.1131\n",
      "Epoch 614/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1136\n",
      "Epoch 00614: val_loss improved from 0.11311 to 0.11308, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1136 - val_loss: 0.1131\n",
      "Epoch 615/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1136\n",
      "Epoch 00615: val_loss improved from 0.11308 to 0.11305, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1136 - val_loss: 0.1131\n",
      "Epoch 616/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1136\n",
      "Epoch 00616: val_loss improved from 0.11305 to 0.11305, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1136 - val_loss: 0.1131\n",
      "Epoch 617/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1136\n",
      "Epoch 00617: val_loss improved from 0.11305 to 0.11301, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1136 - val_loss: 0.1130\n",
      "Epoch 618/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1135\n",
      "Epoch 00618: val_loss improved from 0.11301 to 0.11299, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1135 - val_loss: 0.1130\n",
      "Epoch 619/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1135\n",
      "Epoch 00619: val_loss improved from 0.11299 to 0.11296, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1135 - val_loss: 0.1130\n",
      "Epoch 620/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1135\n",
      "Epoch 00620: val_loss improved from 0.11296 to 0.11294, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1135 - val_loss: 0.1129\n",
      "Epoch 621/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1135\n",
      "Epoch 00621: val_loss improved from 0.11294 to 0.11292, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1135 - val_loss: 0.1129\n",
      "Epoch 622/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1135\n",
      "Epoch 00622: val_loss improved from 0.11292 to 0.11289, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1135 - val_loss: 0.1129\n",
      "Epoch 623/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1134\n",
      "Epoch 00623: val_loss improved from 0.11289 to 0.11288, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1134 - val_loss: 0.1129\n",
      "Epoch 624/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1134\n",
      "Epoch 00624: val_loss improved from 0.11288 to 0.11285, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1134 - val_loss: 0.1128\n",
      "Epoch 625/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1134\n",
      "Epoch 00625: val_loss improved from 0.11285 to 0.11283, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1134 - val_loss: 0.1128\n",
      "Epoch 626/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1134\n",
      "Epoch 00626: val_loss improved from 0.11283 to 0.11281, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1134 - val_loss: 0.1128\n",
      "Epoch 627/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1133\n",
      "Epoch 00627: val_loss improved from 0.11281 to 0.11278, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1133 - val_loss: 0.1128\n",
      "Epoch 628/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1133\n",
      "Epoch 00628: val_loss improved from 0.11278 to 0.11276, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1133 - val_loss: 0.1128\n",
      "Epoch 629/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1133\n",
      "Epoch 00629: val_loss improved from 0.11276 to 0.11273, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1133 - val_loss: 0.1127\n",
      "Epoch 630/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1133\n",
      "Epoch 00630: val_loss did not improve from 0.11273\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1133 - val_loss: 0.1127\n",
      "Epoch 631/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1133\n",
      "Epoch 00631: val_loss improved from 0.11273 to 0.11269, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1133 - val_loss: 0.1127\n",
      "Epoch 632/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1132\n",
      "Epoch 00632: val_loss improved from 0.11269 to 0.11267, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1132 - val_loss: 0.1127\n",
      "Epoch 633/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1132\n",
      "Epoch 00633: val_loss improved from 0.11267 to 0.11265, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1132 - val_loss: 0.1126\n",
      "Epoch 634/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1132\n",
      "Epoch 00634: val_loss improved from 0.11265 to 0.11262, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1132 - val_loss: 0.1126\n",
      "Epoch 635/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1132\n",
      "Epoch 00635: val_loss improved from 0.11262 to 0.11260, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1132 - val_loss: 0.1126\n",
      "Epoch 636/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1131\n",
      "Epoch 00636: val_loss improved from 0.11260 to 0.11258, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1131 - val_loss: 0.1126\n",
      "Epoch 637/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1131\n",
      "Epoch 00637: val_loss improved from 0.11258 to 0.11256, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1131 - val_loss: 0.1126\n",
      "Epoch 638/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1131\n",
      "Epoch 00638: val_loss improved from 0.11256 to 0.11254, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1131 - val_loss: 0.1125\n",
      "Epoch 639/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1131\n",
      "Epoch 00639: val_loss improved from 0.11254 to 0.11252, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1131 - val_loss: 0.1125\n",
      "Epoch 640/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1131\n",
      "Epoch 00640: val_loss improved from 0.11252 to 0.11249, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1131 - val_loss: 0.1125\n",
      "Epoch 641/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1130\n",
      "Epoch 00641: val_loss improved from 0.11249 to 0.11247, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1130 - val_loss: 0.1125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 642/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1130\n",
      "Epoch 00642: val_loss improved from 0.11247 to 0.11245, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1130 - val_loss: 0.1125\n",
      "Epoch 643/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1130\n",
      "Epoch 00643: val_loss improved from 0.11245 to 0.11242, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1130 - val_loss: 0.1124\n",
      "Epoch 644/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1130\n",
      "Epoch 00644: val_loss improved from 0.11242 to 0.11240, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1130 - val_loss: 0.1124\n",
      "Epoch 645/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1130\n",
      "Epoch 00645: val_loss improved from 0.11240 to 0.11240, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1130 - val_loss: 0.1124\n",
      "Epoch 646/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1129\n",
      "Epoch 00646: val_loss improved from 0.11240 to 0.11236, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1129 - val_loss: 0.1124\n",
      "Epoch 647/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1129\n",
      "Epoch 00647: val_loss improved from 0.11236 to 0.11234, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1129 - val_loss: 0.1123\n",
      "Epoch 648/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1129\n",
      "Epoch 00648: val_loss improved from 0.11234 to 0.11233, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1129 - val_loss: 0.1123\n",
      "Epoch 649/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1129\n",
      "Epoch 00649: val_loss improved from 0.11233 to 0.11230, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1129 - val_loss: 0.1123\n",
      "Epoch 650/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1128\n",
      "Epoch 00650: val_loss improved from 0.11230 to 0.11227, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1128 - val_loss: 0.1123\n",
      "Epoch 651/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1128\n",
      "Epoch 00651: val_loss improved from 0.11227 to 0.11226, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1128 - val_loss: 0.1123\n",
      "Epoch 652/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1128\n",
      "Epoch 00652: val_loss improved from 0.11226 to 0.11224, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1128 - val_loss: 0.1122\n",
      "Epoch 653/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1128\n",
      "Epoch 00653: val_loss improved from 0.11224 to 0.11221, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1128 - val_loss: 0.1122\n",
      "Epoch 654/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1128\n",
      "Epoch 00654: val_loss improved from 0.11221 to 0.11220, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1128 - val_loss: 0.1122\n",
      "Epoch 655/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1127- ETA: 0s - loss: 0.111\n",
      "Epoch 00655: val_loss improved from 0.11220 to 0.11218, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1127 - val_loss: 0.1122\n",
      "Epoch 656/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1127\n",
      "Epoch 00656: val_loss improved from 0.11218 to 0.11216, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1127 - val_loss: 0.1122\n",
      "Epoch 657/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1127\n",
      "Epoch 00657: val_loss improved from 0.11216 to 0.11214, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1127 - val_loss: 0.1121\n",
      "Epoch 658/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1127\n",
      "Epoch 00658: val_loss improved from 0.11214 to 0.11211, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1127 - val_loss: 0.1121\n",
      "Epoch 659/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1127\n",
      "Epoch 00659: val_loss improved from 0.11211 to 0.11209, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1127 - val_loss: 0.1121\n",
      "Epoch 660/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1126\n",
      "Epoch 00660: val_loss improved from 0.11209 to 0.11208, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1126 - val_loss: 0.1121\n",
      "Epoch 661/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1126\n",
      "Epoch 00661: val_loss improved from 0.11208 to 0.11206, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1126 - val_loss: 0.1121\n",
      "Epoch 662/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1126\n",
      "Epoch 00662: val_loss improved from 0.11206 to 0.11203, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1126 - val_loss: 0.1120\n",
      "Epoch 663/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1126\n",
      "Epoch 00663: val_loss improved from 0.11203 to 0.11201, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1126 - val_loss: 0.1120\n",
      "Epoch 664/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1126\n",
      "Epoch 00664: val_loss improved from 0.11201 to 0.11200, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1126 - val_loss: 0.1120\n",
      "Epoch 665/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1125\n",
      "Epoch 00665: val_loss improved from 0.11200 to 0.11197, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1125 - val_loss: 0.1120\n",
      "Epoch 666/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1125\n",
      "Epoch 00666: val_loss improved from 0.11197 to 0.11195, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1125 - val_loss: 0.1120\n",
      "Epoch 667/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1125\n",
      "Epoch 00667: val_loss improved from 0.11195 to 0.11193, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1125 - val_loss: 0.1119\n",
      "Epoch 668/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1125\n",
      "Epoch 00668: val_loss did not improve from 0.11193\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1125 - val_loss: 0.1119\n",
      "Epoch 669/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1125- ETA: 0s - loss: 0.1\n",
      "Epoch 00669: val_loss improved from 0.11193 to 0.11189, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1125 - val_loss: 0.1119\n",
      "Epoch 670/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1124\n",
      "Epoch 00670: val_loss improved from 0.11189 to 0.11187, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1124 - val_loss: 0.1119\n",
      "Epoch 671/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1124\n",
      "Epoch 00671: val_loss improved from 0.11187 to 0.11185, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1124 - val_loss: 0.1119\n",
      "Epoch 672/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1124\n",
      "Epoch 00672: val_loss improved from 0.11185 to 0.11183, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1124 - val_loss: 0.1118\n",
      "Epoch 673/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1124\n",
      "Epoch 00673: val_loss improved from 0.11183 to 0.11181, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1124 - val_loss: 0.1118\n",
      "Epoch 674/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1124\n",
      "Epoch 00674: val_loss did not improve from 0.11181\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1124 - val_loss: 0.1118\n",
      "Epoch 675/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1123\n",
      "Epoch 00675: val_loss improved from 0.11181 to 0.11177, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1123 - val_loss: 0.1118\n",
      "Epoch 676/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1123\n",
      "Epoch 00676: val_loss improved from 0.11177 to 0.11175, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1123 - val_loss: 0.1118\n",
      "Epoch 677/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1123\n",
      "Epoch 00677: val_loss improved from 0.11175 to 0.11173, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1123 - val_loss: 0.1117\n",
      "Epoch 678/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1123\n",
      "Epoch 00678: val_loss improved from 0.11173 to 0.11172, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1123 - val_loss: 0.1117\n",
      "Epoch 679/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1122\n",
      "Epoch 00679: val_loss improved from 0.11172 to 0.11169, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1122 - val_loss: 0.1117\n",
      "Epoch 680/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1122\n",
      "Epoch 00680: val_loss improved from 0.11169 to 0.11168, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1122 - val_loss: 0.1117\n",
      "Epoch 681/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1122\n",
      "Epoch 00681: val_loss improved from 0.11168 to 0.11166, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1122 - val_loss: 0.1117\n",
      "Epoch 682/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1122\n",
      "Epoch 00682: val_loss improved from 0.11166 to 0.11164, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1122 - val_loss: 0.1116\n",
      "Epoch 683/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1122\n",
      "Epoch 00683: val_loss improved from 0.11164 to 0.11162, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1122 - val_loss: 0.1116\n",
      "Epoch 684/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1122\n",
      "Epoch 00684: val_loss improved from 0.11162 to 0.11160, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1122 - val_loss: 0.1116\n",
      "Epoch 685/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1121\n",
      "Epoch 00685: val_loss improved from 0.11160 to 0.11158, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1121 - val_loss: 0.1116\n",
      "Epoch 686/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1121\n",
      "Epoch 00686: val_loss improved from 0.11158 to 0.11156, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1121 - val_loss: 0.1116\n",
      "Epoch 687/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1121\n",
      "Epoch 00687: val_loss improved from 0.11156 to 0.11156, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1121 - val_loss: 0.1116\n",
      "Epoch 688/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1121\n",
      "Epoch 00688: val_loss improved from 0.11156 to 0.11152, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1121 - val_loss: 0.1115\n",
      "Epoch 689/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1121\n",
      "Epoch 00689: val_loss improved from 0.11152 to 0.11150, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1121 - val_loss: 0.1115\n",
      "Epoch 690/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1120\n",
      "Epoch 00690: val_loss improved from 0.11150 to 0.11149, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1120 - val_loss: 0.1115\n",
      "Epoch 691/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1120\n",
      "Epoch 00691: val_loss improved from 0.11149 to 0.11148, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1120 - val_loss: 0.1115\n",
      "Epoch 692/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1120\n",
      "Epoch 00692: val_loss improved from 0.11148 to 0.11145, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1120 - val_loss: 0.1114\n",
      "Epoch 693/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1120\n",
      "Epoch 00693: val_loss improved from 0.11145 to 0.11143, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1120 - val_loss: 0.1114\n",
      "Epoch 694/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1120\n",
      "Epoch 00694: val_loss improved from 0.11143 to 0.11143, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1120 - val_loss: 0.1114\n",
      "Epoch 695/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1119\n",
      "Epoch 00695: val_loss improved from 0.11143 to 0.11139, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1119 - val_loss: 0.1114\n",
      "Epoch 696/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1119\n",
      "Epoch 00696: val_loss improved from 0.11139 to 0.11137, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1119 - val_loss: 0.1114\n",
      "Epoch 697/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1119\n",
      "Epoch 00697: val_loss improved from 0.11137 to 0.11137, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1119 - val_loss: 0.1114\n",
      "Epoch 698/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1119\n",
      "Epoch 00698: val_loss improved from 0.11137 to 0.11134, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1119 - val_loss: 0.1113\n",
      "Epoch 699/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1119\n",
      "Epoch 00699: val_loss improved from 0.11134 to 0.11132, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1119 - val_loss: 0.1113\n",
      "Epoch 700/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1118\n",
      "Epoch 00700: val_loss improved from 0.11132 to 0.11131, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1118 - val_loss: 0.1113\n",
      "Epoch 701/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1118\n",
      "Epoch 00701: val_loss improved from 0.11131 to 0.11128, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1118 - val_loss: 0.1113\n",
      "Epoch 702/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1118\n",
      "Epoch 00702: val_loss improved from 0.11128 to 0.11127, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1118 - val_loss: 0.1113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 703/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1118\n",
      "Epoch 00703: val_loss improved from 0.11127 to 0.11126, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1118 - val_loss: 0.1113\n",
      "Epoch 704/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1118\n",
      "Epoch 00704: val_loss improved from 0.11126 to 0.11124, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1118 - val_loss: 0.1112\n",
      "Epoch 705/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1118\n",
      "Epoch 00705: val_loss improved from 0.11124 to 0.11122, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1118 - val_loss: 0.1112\n",
      "Epoch 706/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1117\n",
      "Epoch 00706: val_loss improved from 0.11122 to 0.11120, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1117 - val_loss: 0.1112\n",
      "Epoch 707/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1117\n",
      "Epoch 00707: val_loss did not improve from 0.11120\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1117 - val_loss: 0.1112\n",
      "Epoch 708/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1117\n",
      "Epoch 00708: val_loss improved from 0.11120 to 0.11116, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1117 - val_loss: 0.1112\n",
      "Epoch 709/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1117\n",
      "Epoch 00709: val_loss improved from 0.11116 to 0.11114, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1117 - val_loss: 0.1111\n",
      "Epoch 710/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1117\n",
      "Epoch 00710: val_loss improved from 0.11114 to 0.11113, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1117 - val_loss: 0.1111\n",
      "Epoch 711/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1117\n",
      "Epoch 00711: val_loss improved from 0.11113 to 0.11110, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1117 - val_loss: 0.1111\n",
      "Epoch 712/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1116\n",
      "Epoch 00712: val_loss improved from 0.11110 to 0.11109, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1116 - val_loss: 0.1111\n",
      "Epoch 713/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1116\n",
      "Epoch 00713: val_loss improved from 0.11109 to 0.11108, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1116 - val_loss: 0.1111\n",
      "Epoch 714/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1116\n",
      "Epoch 00714: val_loss improved from 0.11108 to 0.11105, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1116 - val_loss: 0.1110\n",
      "Epoch 715/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1116\n",
      "Epoch 00715: val_loss improved from 0.11105 to 0.11103, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1116 - val_loss: 0.1110\n",
      "Epoch 716/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1116\n",
      "Epoch 00716: val_loss improved from 0.11103 to 0.11102, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1116 - val_loss: 0.1110\n",
      "Epoch 717/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1115\n",
      "Epoch 00717: val_loss improved from 0.11102 to 0.11100, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1115 - val_loss: 0.1110\n",
      "Epoch 718/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1115\n",
      "Epoch 00718: val_loss improved from 0.11100 to 0.11098, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1115 - val_loss: 0.1110\n",
      "Epoch 719/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1115\n",
      "Epoch 00719: val_loss improved from 0.11098 to 0.11096, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1115 - val_loss: 0.1110\n",
      "Epoch 720/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1115\n",
      "Epoch 00720: val_loss did not improve from 0.11096\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1115 - val_loss: 0.1110\n",
      "Epoch 721/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1115\n",
      "Epoch 00721: val_loss improved from 0.11096 to 0.11092, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1115 - val_loss: 0.1109\n",
      "Epoch 722/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1114\n",
      "Epoch 00722: val_loss improved from 0.11092 to 0.11091, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1114 - val_loss: 0.1109\n",
      "Epoch 723/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1114\n",
      "Epoch 00723: val_loss did not improve from 0.11091\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1114 - val_loss: 0.1109\n",
      "Epoch 724/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1114\n",
      "Epoch 00724: val_loss improved from 0.11091 to 0.11087, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1114 - val_loss: 0.1109\n",
      "Epoch 725/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1114\n",
      "Epoch 00725: val_loss improved from 0.11087 to 0.11086, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1114 - val_loss: 0.1109\n",
      "Epoch 726/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1114\n",
      "Epoch 00726: val_loss did not improve from 0.11086\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1114 - val_loss: 0.1109\n",
      "Epoch 727/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1114\n",
      "Epoch 00727: val_loss improved from 0.11086 to 0.11082, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1114 - val_loss: 0.1108\n",
      "Epoch 728/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1113\n",
      "Epoch 00728: val_loss improved from 0.11082 to 0.11081, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1113 - val_loss: 0.1108\n",
      "Epoch 729/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1113\n",
      "Epoch 00729: val_loss improved from 0.11081 to 0.11079, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1113 - val_loss: 0.1108\n",
      "Epoch 730/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1113\n",
      "Epoch 00730: val_loss improved from 0.11079 to 0.11079, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1113 - val_loss: 0.1108\n",
      "Epoch 731/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1113\n",
      "Epoch 00731: val_loss improved from 0.11079 to 0.11076, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1113 - val_loss: 0.1108\n",
      "Epoch 732/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1113\n",
      "Epoch 00732: val_loss improved from 0.11076 to 0.11074, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1113 - val_loss: 0.1107\n",
      "Epoch 733/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1113\n",
      "Epoch 00733: val_loss improved from 0.11074 to 0.11073, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1113 - val_loss: 0.1107\n",
      "Epoch 734/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1112\n",
      "Epoch 00734: val_loss improved from 0.11073 to 0.11072, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1112 - val_loss: 0.1107\n",
      "Epoch 735/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1112\n",
      "Epoch 00735: val_loss improved from 0.11072 to 0.11069, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1112 - val_loss: 0.1107\n",
      "Epoch 736/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1112\n",
      "Epoch 00736: val_loss improved from 0.11069 to 0.11068, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1112 - val_loss: 0.1107\n",
      "Epoch 737/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1112\n",
      "Epoch 00737: val_loss improved from 0.11068 to 0.11066, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1112 - val_loss: 0.1107\n",
      "Epoch 738/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1112\n",
      "Epoch 00738: val_loss improved from 0.11066 to 0.11065, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1112 - val_loss: 0.1107\n",
      "Epoch 739/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1112\n",
      "Epoch 00739: val_loss improved from 0.11065 to 0.11063, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1112 - val_loss: 0.1106\n",
      "Epoch 740/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1112\n",
      "Epoch 00740: val_loss improved from 0.11063 to 0.11062, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1112 - val_loss: 0.1106\n",
      "Epoch 741/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00741: val_loss improved from 0.11062 to 0.11062, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1111 - val_loss: 0.1106\n",
      "Epoch 742/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00742: val_loss improved from 0.11062 to 0.11058, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1111 - val_loss: 0.1106\n",
      "Epoch 743/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00743: val_loss improved from 0.11058 to 0.11057, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1111 - val_loss: 0.1106\n",
      "Epoch 744/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00744: val_loss improved from 0.11057 to 0.11057, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1111 - val_loss: 0.1106\n",
      "Epoch 745/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00745: val_loss improved from 0.11057 to 0.11054, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1111 - val_loss: 0.1105\n",
      "Epoch 746/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1111\n",
      "Epoch 00746: val_loss improved from 0.11054 to 0.11054, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1111 - val_loss: 0.1105\n",
      "Epoch 747/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1110\n",
      "Epoch 00747: val_loss improved from 0.11054 to 0.11051, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1110 - val_loss: 0.1105\n",
      "Epoch 748/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1110\n",
      "Epoch 00748: val_loss improved from 0.11051 to 0.11050, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1110 - val_loss: 0.1105\n",
      "Epoch 749/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1110\n",
      "Epoch 00749: val_loss improved from 0.11050 to 0.11048, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1110 - val_loss: 0.1105\n",
      "Epoch 750/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1110\n",
      "Epoch 00750: val_loss improved from 0.11048 to 0.11046, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1110 - val_loss: 0.1105\n",
      "Epoch 751/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1110\n",
      "Epoch 00751: val_loss improved from 0.11046 to 0.11045, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1110 - val_loss: 0.1105\n",
      "Epoch 752/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1110\n",
      "Epoch 00752: val_loss improved from 0.11045 to 0.11043, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1110 - val_loss: 0.1104\n",
      "Epoch 753/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1109\n",
      "Epoch 00753: val_loss improved from 0.11043 to 0.11042, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1109 - val_loss: 0.1104\n",
      "Epoch 754/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1109\n",
      "Epoch 00754: val_loss improved from 0.11042 to 0.11041, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1109 - val_loss: 0.1104\n",
      "Epoch 755/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1109\n",
      "Epoch 00755: val_loss improved from 0.11041 to 0.11039, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1109 - val_loss: 0.1104\n",
      "Epoch 756/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1109\n",
      "Epoch 00756: val_loss improved from 0.11039 to 0.11039, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1109 - val_loss: 0.1104\n",
      "Epoch 757/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1109\n",
      "Epoch 00757: val_loss improved from 0.11039 to 0.11036, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1109 - val_loss: 0.1104\n",
      "Epoch 758/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1109\n",
      "Epoch 00758: val_loss improved from 0.11036 to 0.11034, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1109 - val_loss: 0.1103\n",
      "Epoch 759/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1109\n",
      "Epoch 00759: val_loss improved from 0.11034 to 0.11033, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1109 - val_loss: 0.1103\n",
      "Epoch 760/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1108\n",
      "Epoch 00760: val_loss improved from 0.11033 to 0.11031, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1108 - val_loss: 0.1103\n",
      "Epoch 761/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1108\n",
      "Epoch 00761: val_loss improved from 0.11031 to 0.11030, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1108 - val_loss: 0.1103\n",
      "Epoch 762/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1108\n",
      "Epoch 00762: val_loss improved from 0.11030 to 0.11029, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1108 - val_loss: 0.1103\n",
      "Epoch 763/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1108\n",
      "Epoch 00763: val_loss improved from 0.11029 to 0.11027, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1108 - val_loss: 0.1103\n",
      "Epoch 764/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1108\n",
      "Epoch 00764: val_loss improved from 0.11027 to 0.11026, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1108 - val_loss: 0.1103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 765/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1108\n",
      "Epoch 00765: val_loss improved from 0.11026 to 0.11024, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1108 - val_loss: 0.1102\n",
      "Epoch 766/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1107\n",
      "Epoch 00766: val_loss improved from 0.11024 to 0.11023, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1107 - val_loss: 0.1102\n",
      "Epoch 767/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1107\n",
      "Epoch 00767: val_loss improved from 0.11023 to 0.11022, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1107 - val_loss: 0.1102\n",
      "Epoch 768/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1107\n",
      "Epoch 00768: val_loss improved from 0.11022 to 0.11019, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1107 - val_loss: 0.1102\n",
      "Epoch 769/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1107\n",
      "Epoch 00769: val_loss improved from 0.11019 to 0.11018, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1107 - val_loss: 0.1102\n",
      "Epoch 770/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1107\n",
      "Epoch 00770: val_loss improved from 0.11018 to 0.11016, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1107 - val_loss: 0.1102\n",
      "Epoch 771/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1107\n",
      "Epoch 00771: val_loss improved from 0.11016 to 0.11015, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1107 - val_loss: 0.1102\n",
      "Epoch 772/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1107\n",
      "Epoch 00772: val_loss improved from 0.11015 to 0.11013, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1107 - val_loss: 0.1101\n",
      "Epoch 773/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00773: val_loss did not improve from 0.11013\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1106 - val_loss: 0.1101\n",
      "Epoch 774/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00774: val_loss improved from 0.11013 to 0.11010, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1106 - val_loss: 0.1101\n",
      "Epoch 775/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00775: val_loss improved from 0.11010 to 0.11009, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1106 - val_loss: 0.1101\n",
      "Epoch 776/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00776: val_loss improved from 0.11009 to 0.11007, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1106 - val_loss: 0.1101\n",
      "Epoch 777/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00777: val_loss improved from 0.11007 to 0.11007, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1106 - val_loss: 0.1101\n",
      "Epoch 778/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1106\n",
      "Epoch 00778: val_loss improved from 0.11007 to 0.11005, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1106 - val_loss: 0.1101\n",
      "Epoch 779/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00779: val_loss improved from 0.11005 to 0.11003, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1105 - val_loss: 0.1100\n",
      "Epoch 780/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00780: val_loss improved from 0.11003 to 0.11003, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1105 - val_loss: 0.1100\n",
      "Epoch 781/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00781: val_loss improved from 0.11003 to 0.11000, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1105 - val_loss: 0.1100\n",
      "Epoch 782/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00782: val_loss improved from 0.11000 to 0.10999, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1105 - val_loss: 0.1100\n",
      "Epoch 783/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00783: val_loss improved from 0.10999 to 0.10997, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1105 - val_loss: 0.1100\n",
      "Epoch 784/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00784: val_loss improved from 0.10997 to 0.10996, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1105 - val_loss: 0.1100\n",
      "Epoch 785/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1105\n",
      "Epoch 00785: val_loss improved from 0.10996 to 0.10995, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1105 - val_loss: 0.1099\n",
      "Epoch 786/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1104\n",
      "Epoch 00786: val_loss improved from 0.10995 to 0.10994, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1104 - val_loss: 0.1099\n",
      "Epoch 787/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1104\n",
      "Epoch 00787: val_loss improved from 0.10994 to 0.10991, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1104 - val_loss: 0.1099\n",
      "Epoch 788/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1104\n",
      "Epoch 00788: val_loss improved from 0.10991 to 0.10991, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1104 - val_loss: 0.1099\n",
      "Epoch 789/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1104\n",
      "Epoch 00789: val_loss improved from 0.10991 to 0.10988, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1104 - val_loss: 0.1099\n",
      "Epoch 790/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1104\n",
      "Epoch 00790: val_loss improved from 0.10988 to 0.10987, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1104 - val_loss: 0.1099\n",
      "Epoch 791/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1104\n",
      "Epoch 00791: val_loss improved from 0.10987 to 0.10986, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1104 - val_loss: 0.1099\n",
      "Epoch 792/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1104\n",
      "Epoch 00792: val_loss improved from 0.10986 to 0.10984, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1104 - val_loss: 0.1098\n",
      "Epoch 793/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1103\n",
      "Epoch 00793: val_loss improved from 0.10984 to 0.10983, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1103 - val_loss: 0.1098\n",
      "Epoch 794/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1103\n",
      "Epoch 00794: val_loss improved from 0.10983 to 0.10981, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1103 - val_loss: 0.1098\n",
      "Epoch 795/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1103\n",
      "Epoch 00795: val_loss improved from 0.10981 to 0.10980, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1103 - val_loss: 0.1098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 796/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1103\n",
      "Epoch 00796: val_loss improved from 0.10980 to 0.10979, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1103 - val_loss: 0.1098\n",
      "Epoch 797/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1103\n",
      "Epoch 00797: val_loss improved from 0.10979 to 0.10977, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1103 - val_loss: 0.1098\n",
      "Epoch 798/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1103\n",
      "Epoch 00798: val_loss improved from 0.10977 to 0.10976, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1103 - val_loss: 0.1098\n",
      "Epoch 799/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1103\n",
      "Epoch 00799: val_loss improved from 0.10976 to 0.10975, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1103 - val_loss: 0.1097\n",
      "Epoch 800/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1102\n",
      "Epoch 00800: val_loss improved from 0.10975 to 0.10974, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1102 - val_loss: 0.1097\n",
      "Epoch 801/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1102\n",
      "Epoch 00801: val_loss improved from 0.10974 to 0.10972, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1102 - val_loss: 0.1097\n",
      "Epoch 802/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1102\n",
      "Epoch 00802: val_loss improved from 0.10972 to 0.10972, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1102 - val_loss: 0.1097\n",
      "Epoch 803/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1102\n",
      "Epoch 00803: val_loss improved from 0.10972 to 0.10969, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1102 - val_loss: 0.1097\n",
      "Epoch 804/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1102\n",
      "Epoch 00804: val_loss improved from 0.10969 to 0.10967, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1102 - val_loss: 0.1097\n",
      "Epoch 805/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1102\n",
      "Epoch 00805: val_loss improved from 0.10967 to 0.10967, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1102 - val_loss: 0.1097\n",
      "Epoch 806/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1102\n",
      "Epoch 00806: val_loss improved from 0.10967 to 0.10965, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1102 - val_loss: 0.1096\n",
      "Epoch 807/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00807: val_loss improved from 0.10965 to 0.10963, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1101 - val_loss: 0.1096\n",
      "Epoch 808/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00808: val_loss did not improve from 0.10963\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1101 - val_loss: 0.1096\n",
      "Epoch 809/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00809: val_loss improved from 0.10963 to 0.10960, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1101 - val_loss: 0.1096\n",
      "Epoch 810/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00810: val_loss did not improve from 0.10960\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1101 - val_loss: 0.1096\n",
      "Epoch 811/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00811: val_loss improved from 0.10960 to 0.10959, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1101 - val_loss: 0.1096\n",
      "Epoch 812/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00812: val_loss improved from 0.10959 to 0.10956, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1101 - val_loss: 0.1096\n",
      "Epoch 813/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00813: val_loss improved from 0.10956 to 0.10955, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1101 - val_loss: 0.1096\n",
      "Epoch 814/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1101\n",
      "Epoch 00814: val_loss improved from 0.10955 to 0.10954, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1101 - val_loss: 0.1095\n",
      "Epoch 815/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1100\n",
      "Epoch 00815: val_loss did not improve from 0.10954\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1100 - val_loss: 0.1095\n",
      "Epoch 816/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1100\n",
      "Epoch 00816: val_loss improved from 0.10954 to 0.10952, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1100 - val_loss: 0.1095\n",
      "Epoch 817/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1100\n",
      "Epoch 00817: val_loss improved from 0.10952 to 0.10950, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1100 - val_loss: 0.1095\n",
      "Epoch 818/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1100\n",
      "Epoch 00818: val_loss improved from 0.10950 to 0.10949, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1100 - val_loss: 0.1095\n",
      "Epoch 819/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1100\n",
      "Epoch 00819: val_loss improved from 0.10949 to 0.10948, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1100 - val_loss: 0.1095\n",
      "Epoch 820/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1100\n",
      "Epoch 00820: val_loss improved from 0.10948 to 0.10947, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1100 - val_loss: 0.1095\n",
      "Epoch 821/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1099- ETA: 0s - loss: 0.\n",
      "Epoch 00821: val_loss improved from 0.10947 to 0.10945, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1099 - val_loss: 0.1095\n",
      "Epoch 822/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1099\n",
      "Epoch 00822: val_loss improved from 0.10945 to 0.10943, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1099 - val_loss: 0.1094\n",
      "Epoch 823/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1099\n",
      "Epoch 00823: val_loss improved from 0.10943 to 0.10942, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1099 - val_loss: 0.1094\n",
      "Epoch 824/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1099\n",
      "Epoch 00824: val_loss improved from 0.10942 to 0.10941, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1099 - val_loss: 0.1094\n",
      "Epoch 825/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1099\n",
      "Epoch 00825: val_loss improved from 0.10941 to 0.10939, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1099 - val_loss: 0.1094\n",
      "Epoch 826/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1099\n",
      "Epoch 00826: val_loss improved from 0.10939 to 0.10938, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1099 - val_loss: 0.1094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 827/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1099\n",
      "Epoch 00827: val_loss did not improve from 0.10938\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1099 - val_loss: 0.1094\n",
      "Epoch 828/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1099\n",
      "Epoch 00828: val_loss improved from 0.10938 to 0.10935, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1099 - val_loss: 0.1094\n",
      "Epoch 829/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1098\n",
      "Epoch 00829: val_loss improved from 0.10935 to 0.10934, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1098 - val_loss: 0.1093\n",
      "Epoch 830/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1098\n",
      "Epoch 00830: val_loss improved from 0.10934 to 0.10933, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1098 - val_loss: 0.1093\n",
      "Epoch 831/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1098\n",
      "Epoch 00831: val_loss did not improve from 0.10933\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1098 - val_loss: 0.1093\n",
      "Epoch 832/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1098\n",
      "Epoch 00832: val_loss improved from 0.10933 to 0.10930, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1098 - val_loss: 0.1093\n",
      "Epoch 833/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1098\n",
      "Epoch 00833: val_loss improved from 0.10930 to 0.10928, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1098 - val_loss: 0.1093\n",
      "Epoch 834/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1098\n",
      "Epoch 00834: val_loss improved from 0.10928 to 0.10928, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1098 - val_loss: 0.1093\n",
      "Epoch 835/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1098\n",
      "Epoch 00835: val_loss improved from 0.10928 to 0.10926, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1098 - val_loss: 0.1093\n",
      "Epoch 836/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1097\n",
      "Epoch 00836: val_loss improved from 0.10926 to 0.10925, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1097 - val_loss: 0.1093\n",
      "Epoch 837/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1097- ETA: 0s - loss: 0.109\n",
      "Epoch 00837: val_loss improved from 0.10925 to 0.10923, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1097 - val_loss: 0.1092\n",
      "Epoch 838/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1097\n",
      "Epoch 00838: val_loss improved from 0.10923 to 0.10922, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1097 - val_loss: 0.1092\n",
      "Epoch 839/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1097\n",
      "Epoch 00839: val_loss did not improve from 0.10922\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1097 - val_loss: 0.1092\n",
      "Epoch 840/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1097\n",
      "Epoch 00840: val_loss improved from 0.10922 to 0.10920, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1097 - val_loss: 0.1092\n",
      "Epoch 841/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1097\n",
      "Epoch 00841: val_loss improved from 0.10920 to 0.10919, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1097 - val_loss: 0.1092\n",
      "Epoch 842/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1097\n",
      "Epoch 00842: val_loss improved from 0.10919 to 0.10917, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1097 - val_loss: 0.1092\n",
      "Epoch 843/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1097\n",
      "Epoch 00843: val_loss improved from 0.10917 to 0.10916, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1097 - val_loss: 0.1092\n",
      "Epoch 844/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00844: val_loss improved from 0.10916 to 0.10916, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1096 - val_loss: 0.1092\n",
      "Epoch 845/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00845: val_loss improved from 0.10916 to 0.10914, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1096 - val_loss: 0.1091\n",
      "Epoch 846/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00846: val_loss improved from 0.10914 to 0.10912, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1096 - val_loss: 0.1091\n",
      "Epoch 847/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00847: val_loss improved from 0.10912 to 0.10911, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1096 - val_loss: 0.1091\n",
      "Epoch 848/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00848: val_loss improved from 0.10911 to 0.10909, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1096 - val_loss: 0.1091\n",
      "Epoch 849/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00849: val_loss improved from 0.10909 to 0.10908, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1096 - val_loss: 0.1091\n",
      "Epoch 850/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00850: val_loss did not improve from 0.10908\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1096 - val_loss: 0.1091\n",
      "Epoch 851/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1096\n",
      "Epoch 00851: val_loss improved from 0.10908 to 0.10906, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1096 - val_loss: 0.1091\n",
      "Epoch 852/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1095\n",
      "Epoch 00852: val_loss improved from 0.10906 to 0.10903, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1095 - val_loss: 0.1090\n",
      "Epoch 853/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1095\n",
      "Epoch 00853: val_loss improved from 0.10903 to 0.10901, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1095 - val_loss: 0.1090\n",
      "Epoch 854/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1095\n",
      "Epoch 00854: val_loss did not improve from 0.10901\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1095 - val_loss: 0.1090\n",
      "Epoch 855/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1095\n",
      "Epoch 00855: val_loss improved from 0.10901 to 0.10899, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1095 - val_loss: 0.1090\n",
      "Epoch 856/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1095\n",
      "Epoch 00856: val_loss improved from 0.10899 to 0.10897, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1095 - val_loss: 0.1090\n",
      "Epoch 857/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1095\n",
      "Epoch 00857: val_loss improved from 0.10897 to 0.10896, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 0.1095 - val_loss: 0.1090\n",
      "Epoch 858/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1094\n",
      "Epoch 00858: val_loss improved from 0.10896 to 0.10895, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1094 - val_loss: 0.1090\n",
      "Epoch 859/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1094\n",
      "Epoch 00859: val_loss improved from 0.10895 to 0.10892, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1094 - val_loss: 0.1089\n",
      "Epoch 860/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1094\n",
      "Epoch 00860: val_loss improved from 0.10892 to 0.10892, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1094 - val_loss: 0.1089\n",
      "Epoch 861/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1094\n",
      "Epoch 00861: val_loss improved from 0.10892 to 0.10889, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1094 - val_loss: 0.1089\n",
      "Epoch 862/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1094\n",
      "Epoch 00862: val_loss improved from 0.10889 to 0.10888, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1094 - val_loss: 0.1089\n",
      "Epoch 863/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1094\n",
      "Epoch 00863: val_loss did not improve from 0.10888\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1094 - val_loss: 0.1089\n",
      "Epoch 864/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1094\n",
      "Epoch 00864: val_loss improved from 0.10888 to 0.10884, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1094 - val_loss: 0.1088\n",
      "Epoch 865/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1093\n",
      "Epoch 00865: val_loss improved from 0.10884 to 0.10883, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1093 - val_loss: 0.1088\n",
      "Epoch 866/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1093\n",
      "Epoch 00866: val_loss improved from 0.10883 to 0.10881, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1093 - val_loss: 0.1088\n",
      "Epoch 867/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1093\n",
      "Epoch 00867: val_loss improved from 0.10881 to 0.10880, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1093 - val_loss: 0.1088\n",
      "Epoch 868/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1093\n",
      "Epoch 00868: val_loss improved from 0.10880 to 0.10880, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1093 - val_loss: 0.1088\n",
      "Epoch 869/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1093\n",
      "Epoch 00869: val_loss improved from 0.10880 to 0.10877, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1093 - val_loss: 0.1088\n",
      "Epoch 870/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1093\n",
      "Epoch 00870: val_loss improved from 0.10877 to 0.10876, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1093 - val_loss: 0.1088\n",
      "Epoch 871/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1093\n",
      "Epoch 00871: val_loss improved from 0.10876 to 0.10874, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1093 - val_loss: 0.1087\n",
      "Epoch 872/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1092\n",
      "Epoch 00872: val_loss improved from 0.10874 to 0.10874, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1092 - val_loss: 0.1087\n",
      "Epoch 873/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1092\n",
      "Epoch 00873: val_loss improved from 0.10874 to 0.10872, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1092 - val_loss: 0.1087\n",
      "Epoch 874/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1092\n",
      "Epoch 00874: val_loss improved from 0.10872 to 0.10870, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1092 - val_loss: 0.1087\n",
      "Epoch 875/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1092\n",
      "Epoch 00875: val_loss did not improve from 0.10870\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1092 - val_loss: 0.1087\n",
      "Epoch 876/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1092\n",
      "Epoch 00876: val_loss improved from 0.10870 to 0.10868, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1092 - val_loss: 0.1087\n",
      "Epoch 877/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1092\n",
      "Epoch 00877: val_loss improved from 0.10868 to 0.10866, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1092 - val_loss: 0.1087\n",
      "Epoch 878/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1092\n",
      "Epoch 00878: val_loss improved from 0.10866 to 0.10865, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1092 - val_loss: 0.1087\n",
      "Epoch 879/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1091\n",
      "Epoch 00879: val_loss improved from 0.10865 to 0.10864, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1091 - val_loss: 0.1086\n",
      "Epoch 880/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1091\n",
      "Epoch 00880: val_loss improved from 0.10864 to 0.10862, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1091 - val_loss: 0.1086\n",
      "Epoch 881/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1091\n",
      "Epoch 00881: val_loss improved from 0.10862 to 0.10861, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1091 - val_loss: 0.1086\n",
      "Epoch 882/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1091\n",
      "Epoch 00882: val_loss improved from 0.10861 to 0.10860, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1091 - val_loss: 0.1086\n",
      "Epoch 883/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1091\n",
      "Epoch 00883: val_loss improved from 0.10860 to 0.10858, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1091 - val_loss: 0.1086\n",
      "Epoch 884/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1091\n",
      "Epoch 00884: val_loss improved from 0.10858 to 0.10857, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1091 - val_loss: 0.1086\n",
      "Epoch 885/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1091\n",
      "Epoch 00885: val_loss improved from 0.10857 to 0.10856, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1091 - val_loss: 0.1086\n",
      "Epoch 886/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00886: val_loss improved from 0.10856 to 0.10855, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1090 - val_loss: 0.1085\n",
      "Epoch 887/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00887: val_loss improved from 0.10855 to 0.10852, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1090 - val_loss: 0.1085\n",
      "Epoch 888/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00888: val_loss improved from 0.10852 to 0.10852, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1090 - val_loss: 0.1085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 889/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00889: val_loss improved from 0.10852 to 0.10852, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1090 - val_loss: 0.1085\n",
      "Epoch 890/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1090- ETA: 0s - loss: 0.109\n",
      "Epoch 00890: val_loss improved from 0.10852 to 0.10849, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1090 - val_loss: 0.1085\n",
      "Epoch 891/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00891: val_loss improved from 0.10849 to 0.10847, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1090 - val_loss: 0.1085\n",
      "Epoch 892/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00892: val_loss improved from 0.10847 to 0.10847, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1090 - val_loss: 0.1085\n",
      "Epoch 893/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1090\n",
      "Epoch 00893: val_loss improved from 0.10847 to 0.10846, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1090 - val_loss: 0.1085\n",
      "Epoch 894/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1089\n",
      "Epoch 00894: val_loss improved from 0.10846 to 0.10846, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1089 - val_loss: 0.1085\n",
      "Epoch 895/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1089\n",
      "Epoch 00895: val_loss improved from 0.10846 to 0.10842, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1089 - val_loss: 0.1084\n",
      "Epoch 896/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1089\n",
      "Epoch 00896: val_loss improved from 0.10842 to 0.10841, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1089 - val_loss: 0.1084\n",
      "Epoch 897/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1089\n",
      "Epoch 00897: val_loss improved from 0.10841 to 0.10840, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1089 - val_loss: 0.1084\n",
      "Epoch 898/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1089\n",
      "Epoch 00898: val_loss improved from 0.10840 to 0.10838, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1089 - val_loss: 0.1084\n",
      "Epoch 899/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1089\n",
      "Epoch 00899: val_loss improved from 0.10838 to 0.10837, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1089 - val_loss: 0.1084\n",
      "Epoch 900/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1089\n",
      "Epoch 00900: val_loss improved from 0.10837 to 0.10837, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1089 - val_loss: 0.1084\n",
      "Epoch 901/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1088\n",
      "Epoch 00901: val_loss improved from 0.10837 to 0.10834, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1088 - val_loss: 0.1083\n",
      "Epoch 902/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1088\n",
      "Epoch 00902: val_loss improved from 0.10834 to 0.10833, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1088 - val_loss: 0.1083\n",
      "Epoch 903/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1088\n",
      "Epoch 00903: val_loss improved from 0.10833 to 0.10833, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1088 - val_loss: 0.1083\n",
      "Epoch 904/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1088\n",
      "Epoch 00904: val_loss did not improve from 0.10833\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1088 - val_loss: 0.1083\n",
      "Epoch 905/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1088\n",
      "Epoch 00905: val_loss improved from 0.10833 to 0.10830, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1088 - val_loss: 0.1083\n",
      "Epoch 906/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1088\n",
      "Epoch 00906: val_loss improved from 0.10830 to 0.10829, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1088 - val_loss: 0.1083\n",
      "Epoch 907/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1088\n",
      "Epoch 00907: val_loss improved from 0.10829 to 0.10828, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1088 - val_loss: 0.1083\n",
      "Epoch 908/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1088\n",
      "Epoch 00908: val_loss improved from 0.10828 to 0.10826, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1088 - val_loss: 0.1083\n",
      "Epoch 909/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1087\n",
      "Epoch 00909: val_loss improved from 0.10826 to 0.10826, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1087 - val_loss: 0.1083\n",
      "Epoch 910/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1087\n",
      "Epoch 00910: val_loss improved from 0.10826 to 0.10823, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1087 - val_loss: 0.1082\n",
      "Epoch 911/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1087\n",
      "Epoch 00911: val_loss improved from 0.10823 to 0.10823, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1087 - val_loss: 0.1082\n",
      "Epoch 912/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1087\n",
      "Epoch 00912: val_loss improved from 0.10823 to 0.10821, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1087 - val_loss: 0.1082\n",
      "Epoch 913/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1087\n",
      "Epoch 00913: val_loss did not improve from 0.10821\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1087 - val_loss: 0.1082\n",
      "Epoch 914/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1087\n",
      "Epoch 00914: val_loss improved from 0.10821 to 0.10820, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1087 - val_loss: 0.1082\n",
      "Epoch 915/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1087\n",
      "Epoch 00915: val_loss improved from 0.10820 to 0.10817, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1087 - val_loss: 0.1082\n",
      "Epoch 916/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1087\n",
      "Epoch 00916: val_loss improved from 0.10817 to 0.10816, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1087 - val_loss: 0.1082\n",
      "Epoch 917/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00917: val_loss did not improve from 0.10816\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1086 - val_loss: 0.1082\n",
      "Epoch 918/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00918: val_loss improved from 0.10816 to 0.10813, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1086 - val_loss: 0.1081\n",
      "Epoch 919/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00919: val_loss improved from 0.10813 to 0.10813, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1086 - val_loss: 0.1081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 920/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00920: val_loss did not improve from 0.10813\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1086 - val_loss: 0.1081\n",
      "Epoch 921/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00921: val_loss improved from 0.10813 to 0.10810, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1086 - val_loss: 0.1081\n",
      "Epoch 922/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00922: val_loss improved from 0.10810 to 0.10809, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1086 - val_loss: 0.1081\n",
      "Epoch 923/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00923: val_loss improved from 0.10809 to 0.10808, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1086 - val_loss: 0.1081\n",
      "Epoch 924/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1086\n",
      "Epoch 00924: val_loss improved from 0.10808 to 0.10806, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1086 - val_loss: 0.1081\n",
      "Epoch 925/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1085\n",
      "Epoch 00925: val_loss improved from 0.10806 to 0.10805, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1085 - val_loss: 0.1081\n",
      "Epoch 926/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1085\n",
      "Epoch 00926: val_loss improved from 0.10805 to 0.10804, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1085 - val_loss: 0.1080\n",
      "Epoch 927/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1085\n",
      "Epoch 00927: val_loss improved from 0.10804 to 0.10803, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1085 - val_loss: 0.1080\n",
      "Epoch 928/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1085\n",
      "Epoch 00928: val_loss did not improve from 0.10803\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1085 - val_loss: 0.1080\n",
      "Epoch 929/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1085\n",
      "Epoch 00929: val_loss improved from 0.10803 to 0.10800, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1085 - val_loss: 0.1080\n",
      "Epoch 930/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1085\n",
      "Epoch 00930: val_loss improved from 0.10800 to 0.10799, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1085 - val_loss: 0.1080\n",
      "Epoch 931/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1085\n",
      "Epoch 00931: val_loss improved from 0.10799 to 0.10799, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1085 - val_loss: 0.1080\n",
      "Epoch 932/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1085\n",
      "Epoch 00932: val_loss improved from 0.10799 to 0.10797, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1085 - val_loss: 0.1080\n",
      "Epoch 933/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00933: val_loss improved from 0.10797 to 0.10796, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1084 - val_loss: 0.1080\n",
      "Epoch 934/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00934: val_loss improved from 0.10796 to 0.10795, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1084 - val_loss: 0.1079\n",
      "Epoch 935/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00935: val_loss improved from 0.10795 to 0.10793, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1084 - val_loss: 0.1079\n",
      "Epoch 936/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00936: val_loss improved from 0.10793 to 0.10792, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1084 - val_loss: 0.1079\n",
      "Epoch 937/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00937: val_loss improved from 0.10792 to 0.10791, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1084 - val_loss: 0.1079\n",
      "Epoch 938/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00938: val_loss improved from 0.10791 to 0.10790, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1084 - val_loss: 0.1079\n",
      "Epoch 939/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00939: val_loss improved from 0.10790 to 0.10788, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1084 - val_loss: 0.1079\n",
      "Epoch 940/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00940: val_loss improved from 0.10788 to 0.10787, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1084 - val_loss: 0.1079\n",
      "Epoch 941/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1084\n",
      "Epoch 00941: val_loss improved from 0.10787 to 0.10786, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1084 - val_loss: 0.1079\n",
      "Epoch 942/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00942: val_loss improved from 0.10786 to 0.10785, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1083 - val_loss: 0.1079\n",
      "Epoch 943/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00943: val_loss improved from 0.10785 to 0.10784, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1083 - val_loss: 0.1078\n",
      "Epoch 944/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00944: val_loss improved from 0.10784 to 0.10783, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1083 - val_loss: 0.1078\n",
      "Epoch 945/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00945: val_loss improved from 0.10783 to 0.10781, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1083 - val_loss: 0.1078\n",
      "Epoch 946/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00946: val_loss improved from 0.10781 to 0.10781, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1083 - val_loss: 0.1078\n",
      "Epoch 947/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00947: val_loss improved from 0.10781 to 0.10780, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1083 - val_loss: 0.1078\n",
      "Epoch 948/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00948: val_loss improved from 0.10780 to 0.10778, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1083 - val_loss: 0.1078\n",
      "Epoch 949/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1083\n",
      "Epoch 00949: val_loss improved from 0.10778 to 0.10777, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1083 - val_loss: 0.1078\n",
      "Epoch 950/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00950: val_loss did not improve from 0.10777\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1082 - val_loss: 0.1078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 951/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00951: val_loss improved from 0.10777 to 0.10775, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1082 - val_loss: 0.1077\n",
      "Epoch 952/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00952: val_loss did not improve from 0.10775\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1082 - val_loss: 0.1078\n",
      "Epoch 953/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00953: val_loss improved from 0.10775 to 0.10773, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1082 - val_loss: 0.1077\n",
      "Epoch 954/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00954: val_loss improved from 0.10773 to 0.10772, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1082 - val_loss: 0.1077\n",
      "Epoch 955/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00955: val_loss improved from 0.10772 to 0.10770, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1082 - val_loss: 0.1077\n",
      "Epoch 956/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00956: val_loss did not improve from 0.10770\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1082 - val_loss: 0.1077\n",
      "Epoch 957/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1082\n",
      "Epoch 00957: val_loss improved from 0.10770 to 0.10769, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1082 - val_loss: 0.1077\n",
      "Epoch 958/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00958: val_loss improved from 0.10769 to 0.10767, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1081 - val_loss: 0.1077\n",
      "Epoch 959/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00959: val_loss improved from 0.10767 to 0.10766, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1081 - val_loss: 0.1077\n",
      "Epoch 960/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00960: val_loss improved from 0.10766 to 0.10765, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1081 - val_loss: 0.1076\n",
      "Epoch 961/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00961: val_loss improved from 0.10765 to 0.10764, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1081 - val_loss: 0.1076\n",
      "Epoch 962/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00962: val_loss improved from 0.10764 to 0.10763, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1081 - val_loss: 0.1076\n",
      "Epoch 963/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00963: val_loss improved from 0.10763 to 0.10762, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1081 - val_loss: 0.1076\n",
      "Epoch 964/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00964: val_loss improved from 0.10762 to 0.10761, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1081 - val_loss: 0.1076\n",
      "Epoch 965/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1081\n",
      "Epoch 00965: val_loss improved from 0.10761 to 0.10760, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1081 - val_loss: 0.1076\n",
      "Epoch 966/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00966: val_loss improved from 0.10760 to 0.10759, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1080 - val_loss: 0.1076\n",
      "Epoch 967/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00967: val_loss improved from 0.10759 to 0.10757, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1080 - val_loss: 0.1076\n",
      "Epoch 968/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00968: val_loss improved from 0.10757 to 0.10757, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1080 - val_loss: 0.1076\n",
      "Epoch 969/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00969: val_loss improved from 0.10757 to 0.10756, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1080 - val_loss: 0.1076\n",
      "Epoch 970/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00970: val_loss improved from 0.10756 to 0.10754, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1080 - val_loss: 0.1075\n",
      "Epoch 971/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00971: val_loss improved from 0.10754 to 0.10753, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1080 - val_loss: 0.1075\n",
      "Epoch 972/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00972: val_loss did not improve from 0.10753\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1080 - val_loss: 0.1075\n",
      "Epoch 973/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00973: val_loss improved from 0.10753 to 0.10752, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1080 - val_loss: 0.1075\n",
      "Epoch 974/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1080\n",
      "Epoch 00974: val_loss improved from 0.10752 to 0.10750, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1080 - val_loss: 0.1075\n",
      "Epoch 975/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00975: val_loss improved from 0.10750 to 0.10749, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1079 - val_loss: 0.1075\n",
      "Epoch 976/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00976: val_loss improved from 0.10749 to 0.10748, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1079 - val_loss: 0.1075\n",
      "Epoch 977/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00977: val_loss did not improve from 0.10748\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1079 - val_loss: 0.1075\n",
      "Epoch 978/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00978: val_loss improved from 0.10748 to 0.10746, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1079 - val_loss: 0.1075\n",
      "Epoch 979/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00979: val_loss improved from 0.10746 to 0.10745, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1079 - val_loss: 0.1075\n",
      "Epoch 980/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00980: val_loss improved from 0.10745 to 0.10745, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1079 - val_loss: 0.1074\n",
      "Epoch 981/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00981: val_loss improved from 0.10745 to 0.10743, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1079 - val_loss: 0.1074\n",
      "Epoch 982/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00982: val_loss improved from 0.10743 to 0.10742, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1079 - val_loss: 0.1074\n",
      "Epoch 983/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1079\n",
      "Epoch 00983: val_loss improved from 0.10742 to 0.10741, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1079 - val_loss: 0.1074\n",
      "Epoch 984/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00984: val_loss did not improve from 0.10741\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1078 - val_loss: 0.1074\n",
      "Epoch 985/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00985: val_loss improved from 0.10741 to 0.10741, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1078 - val_loss: 0.1074\n",
      "Epoch 986/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00986: val_loss improved from 0.10741 to 0.10738, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1078 - val_loss: 0.1074\n",
      "Epoch 987/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00987: val_loss improved from 0.10738 to 0.10737, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1078 - val_loss: 0.1074\n",
      "Epoch 988/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00988: val_loss did not improve from 0.10737\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1078 - val_loss: 0.1074\n",
      "Epoch 989/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00989: val_loss improved from 0.10737 to 0.10735, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1078 - val_loss: 0.1073\n",
      "Epoch 990/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00990: val_loss improved from 0.10735 to 0.10734, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1078 - val_loss: 0.1073\n",
      "Epoch 991/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00991: val_loss improved from 0.10734 to 0.10733, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1078 - val_loss: 0.1073\n",
      "Epoch 992/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1078\n",
      "Epoch 00992: val_loss improved from 0.10733 to 0.10732, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1078 - val_loss: 0.1073\n",
      "Epoch 993/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00993: val_loss did not improve from 0.10732\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1077 - val_loss: 0.1073\n",
      "Epoch 994/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00994: val_loss improved from 0.10732 to 0.10730, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1077 - val_loss: 0.1073\n",
      "Epoch 995/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00995: val_loss improved from 0.10730 to 0.10729, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1077 - val_loss: 0.1073\n",
      "Epoch 996/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00996: val_loss improved from 0.10729 to 0.10728, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1077 - val_loss: 0.1073\n",
      "Epoch 997/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00997: val_loss improved from 0.10728 to 0.10728, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1077 - val_loss: 0.1073\n",
      "Epoch 998/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00998: val_loss improved from 0.10728 to 0.10727, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1077 - val_loss: 0.1073\n",
      "Epoch 999/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 00999: val_loss improved from 0.10727 to 0.10726, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1077 - val_loss: 0.1073\n",
      "Epoch 1000/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 01000: val_loss improved from 0.10726 to 0.10725, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1077 - val_loss: 0.1072\n",
      "Epoch 1001/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1077\n",
      "Epoch 01001: val_loss improved from 0.10725 to 0.10723, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1077 - val_loss: 0.1072\n",
      "Epoch 1002/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 01002: val_loss improved from 0.10723 to 0.10722, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1076 - val_loss: 0.1072\n",
      "Epoch 1003/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 01003: val_loss improved from 0.10722 to 0.10721, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1076 - val_loss: 0.1072\n",
      "Epoch 1004/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 01004: val_loss improved from 0.10721 to 0.10720, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1076 - val_loss: 0.1072\n",
      "Epoch 1005/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 01005: val_loss improved from 0.10720 to 0.10720, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1076 - val_loss: 0.1072\n",
      "Epoch 1006/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 01006: val_loss improved from 0.10720 to 0.10718, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1076 - val_loss: 0.1072\n",
      "Epoch 1007/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 01007: val_loss improved from 0.10718 to 0.10718, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1076 - val_loss: 0.1072\n",
      "Epoch 1008/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 01008: val_loss improved from 0.10718 to 0.10717, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1076 - val_loss: 0.1072\n",
      "Epoch 1009/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 01009: val_loss improved from 0.10717 to 0.10715, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1076 - val_loss: 0.1072\n",
      "Epoch 1010/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 01010: val_loss improved from 0.10715 to 0.10715, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1076 - val_loss: 0.1071\n",
      "Epoch 1011/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1076\n",
      "Epoch 01011: val_loss improved from 0.10715 to 0.10714, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1076 - val_loss: 0.1071\n",
      "Epoch 1012/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1075\n",
      "Epoch 01012: val_loss improved from 0.10714 to 0.10713, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1075 - val_loss: 0.1071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1013/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1075\n",
      "Epoch 01013: val_loss improved from 0.10713 to 0.10711, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1075 - val_loss: 0.1071\n",
      "Epoch 1014/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1075\n",
      "Epoch 01014: val_loss improved from 0.10711 to 0.10711, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1075 - val_loss: 0.1071\n",
      "Epoch 1015/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1075\n",
      "Epoch 01015: val_loss improved from 0.10711 to 0.10711, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1075 - val_loss: 0.1071\n",
      "Epoch 1016/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1075\n",
      "Epoch 01016: val_loss improved from 0.10711 to 0.10709, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1075 - val_loss: 0.1071\n",
      "Epoch 1017/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1075\n",
      "Epoch 01017: val_loss improved from 0.10709 to 0.10708, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1075 - val_loss: 0.1071\n",
      "Epoch 1018/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1075\n",
      "Epoch 01018: val_loss improved from 0.10708 to 0.10707, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1075 - val_loss: 0.1071\n",
      "Epoch 1019/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1075\n",
      "Epoch 01019: val_loss improved from 0.10707 to 0.10706, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1075 - val_loss: 0.1071\n",
      "Epoch 1020/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1075\n",
      "Epoch 01020: val_loss improved from 0.10706 to 0.10706, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1075 - val_loss: 0.1071\n",
      "Epoch 1021/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1075\n",
      "Epoch 01021: val_loss improved from 0.10706 to 0.10704, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1075 - val_loss: 0.1070\n",
      "Epoch 1022/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 01022: val_loss improved from 0.10704 to 0.10703, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1074 - val_loss: 0.1070\n",
      "Epoch 1023/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 01023: val_loss did not improve from 0.10703\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1074 - val_loss: 0.1070\n",
      "Epoch 1024/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 01024: val_loss did not improve from 0.10703\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1074 - val_loss: 0.1070\n",
      "Epoch 1025/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 01025: val_loss improved from 0.10703 to 0.10701, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1074 - val_loss: 0.1070\n",
      "Epoch 1026/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 01026: val_loss improved from 0.10701 to 0.10700, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1074 - val_loss: 0.1070\n",
      "Epoch 1027/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 01027: val_loss improved from 0.10700 to 0.10699, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1074 - val_loss: 0.1070\n",
      "Epoch 1028/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 01028: val_loss improved from 0.10699 to 0.10698, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1074 - val_loss: 0.1070\n",
      "Epoch 1029/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 01029: val_loss improved from 0.10698 to 0.10698, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1074 - val_loss: 0.1070\n",
      "Epoch 1030/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1074\n",
      "Epoch 01030: val_loss improved from 0.10698 to 0.10697, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1074 - val_loss: 0.1070\n",
      "Epoch 1031/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 01031: val_loss improved from 0.10697 to 0.10696, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1073 - val_loss: 0.1070\n",
      "Epoch 1032/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 01032: val_loss improved from 0.10696 to 0.10696, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1073 - val_loss: 0.1070\n",
      "Epoch 1033/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 01033: val_loss improved from 0.10696 to 0.10694, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1073 - val_loss: 0.1069\n",
      "Epoch 1034/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 01034: val_loss improved from 0.10694 to 0.10693, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1073 - val_loss: 0.1069\n",
      "Epoch 1035/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 01035: val_loss improved from 0.10693 to 0.10693, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1073 - val_loss: 0.1069\n",
      "Epoch 1036/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 01036: val_loss improved from 0.10693 to 0.10692, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1073 - val_loss: 0.1069\n",
      "Epoch 1037/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 01037: val_loss improved from 0.10692 to 0.10690, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1073 - val_loss: 0.1069\n",
      "Epoch 1038/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 01038: val_loss improved from 0.10690 to 0.10690, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1073 - val_loss: 0.1069\n",
      "Epoch 1039/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 01039: val_loss improved from 0.10690 to 0.10689, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.1073 - val_loss: 0.1069\n",
      "Epoch 1040/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1073\n",
      "Epoch 01040: val_loss improved from 0.10689 to 0.10688, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1073 - val_loss: 0.1069\n",
      "Epoch 1041/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1072\n",
      "Epoch 01041: val_loss improved from 0.10688 to 0.10687, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1072 - val_loss: 0.1069\n",
      "Epoch 1042/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1072\n",
      "Epoch 01042: val_loss did not improve from 0.10687\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1072 - val_loss: 0.1069\n",
      "Epoch 1043/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1072\n",
      "Epoch 01043: val_loss did not improve from 0.10687\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1072 - val_loss: 0.1069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1044/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1072\n",
      "Epoch 01044: val_loss improved from 0.10687 to 0.10684, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1072 - val_loss: 0.1068\n",
      "Epoch 1045/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1072\n",
      "Epoch 01045: val_loss improved from 0.10684 to 0.10683, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1072 - val_loss: 0.1068\n",
      "Epoch 1046/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1072\n",
      "Epoch 01046: val_loss improved from 0.10683 to 0.10682, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1072 - val_loss: 0.1068\n",
      "Epoch 1047/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1072\n",
      "Epoch 01047: val_loss did not improve from 0.10682\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1072 - val_loss: 0.1068\n",
      "Epoch 1048/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1072\n",
      "Epoch 01048: val_loss improved from 0.10682 to 0.10681, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1072 - val_loss: 0.1068\n",
      "Epoch 1049/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1072\n",
      "Epoch 01049: val_loss improved from 0.10681 to 0.10680, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1072 - val_loss: 0.1068\n",
      "Epoch 1050/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1072\n",
      "Epoch 01050: val_loss improved from 0.10680 to 0.10680, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1072 - val_loss: 0.1068\n",
      "Epoch 1051/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1071\n",
      "Epoch 01051: val_loss improved from 0.10680 to 0.10678, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1071 - val_loss: 0.1068\n",
      "Epoch 1052/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1071\n",
      "Epoch 01052: val_loss improved from 0.10678 to 0.10677, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1071 - val_loss: 0.1068\n",
      "Epoch 1053/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1071\n",
      "Epoch 01053: val_loss improved from 0.10677 to 0.10676, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1071 - val_loss: 0.1068\n",
      "Epoch 1054/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1071\n",
      "Epoch 01054: val_loss improved from 0.10676 to 0.10676, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1071 - val_loss: 0.1068\n",
      "Epoch 1055/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1071\n",
      "Epoch 01055: val_loss improved from 0.10676 to 0.10675, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1071 - val_loss: 0.1068\n",
      "Epoch 1056/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1071\n",
      "Epoch 01056: val_loss improved from 0.10675 to 0.10673, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1071 - val_loss: 0.1067\n",
      "Epoch 1057/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1071\n",
      "Epoch 01057: val_loss did not improve from 0.10673\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1071 - val_loss: 0.1067\n",
      "Epoch 1058/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1071\n",
      "Epoch 01058: val_loss improved from 0.10673 to 0.10671, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1071 - val_loss: 0.1067\n",
      "Epoch 1059/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1071\n",
      "Epoch 01059: val_loss improved from 0.10671 to 0.10671, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1071 - val_loss: 0.1067\n",
      "Epoch 1060/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1071\n",
      "Epoch 01060: val_loss improved from 0.10671 to 0.10670, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1071 - val_loss: 0.1067\n",
      "Epoch 1061/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1071\n",
      "Epoch 01061: val_loss did not improve from 0.10670\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1071 - val_loss: 0.1067\n",
      "Epoch 1062/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1070\n",
      "Epoch 01062: val_loss improved from 0.10670 to 0.10668, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1070 - val_loss: 0.1067\n",
      "Epoch 1063/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1070\n",
      "Epoch 01063: val_loss improved from 0.10668 to 0.10667, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1070 - val_loss: 0.1067\n",
      "Epoch 1064/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1070\n",
      "Epoch 01064: val_loss improved from 0.10667 to 0.10666, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1070 - val_loss: 0.1067\n",
      "Epoch 1065/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1070\n",
      "Epoch 01065: val_loss did not improve from 0.10666\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1070 - val_loss: 0.1067\n",
      "Epoch 1066/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1070\n",
      "Epoch 01066: val_loss improved from 0.10666 to 0.10665, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1070 - val_loss: 0.1067\n",
      "Epoch 1067/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1070\n",
      "Epoch 01067: val_loss improved from 0.10665 to 0.10664, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1070 - val_loss: 0.1066\n",
      "Epoch 1068/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1070\n",
      "Epoch 01068: val_loss improved from 0.10664 to 0.10663, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1070 - val_loss: 0.1066\n",
      "Epoch 1069/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1070\n",
      "Epoch 01069: val_loss improved from 0.10663 to 0.10662, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1070 - val_loss: 0.1066\n",
      "Epoch 1070/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1070\n",
      "Epoch 01070: val_loss improved from 0.10662 to 0.10661, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1070 - val_loss: 0.1066\n",
      "Epoch 1071/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1070\n",
      "Epoch 01071: val_loss improved from 0.10661 to 0.10660, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1070 - val_loss: 0.1066\n",
      "Epoch 1072/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1070\n",
      "Epoch 01072: val_loss did not improve from 0.10660\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1070 - val_loss: 0.1066\n",
      "Epoch 1073/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 01073: val_loss improved from 0.10660 to 0.10660, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1069 - val_loss: 0.1066\n",
      "Epoch 1074/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 01074: val_loss improved from 0.10660 to 0.10658, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1069 - val_loss: 0.1066\n",
      "Epoch 1075/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 01075: val_loss improved from 0.10658 to 0.10656, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1069 - val_loss: 0.1066\n",
      "Epoch 1076/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 01076: val_loss improved from 0.10656 to 0.10656, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1069 - val_loss: 0.1066\n",
      "Epoch 1077/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 01077: val_loss improved from 0.10656 to 0.10656, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1069 - val_loss: 0.1066\n",
      "Epoch 1078/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 01078: val_loss improved from 0.10656 to 0.10655, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1069 - val_loss: 0.1065\n",
      "Epoch 1079/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 01079: val_loss improved from 0.10655 to 0.10654, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1069 - val_loss: 0.1065\n",
      "Epoch 1080/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 01080: val_loss improved from 0.10654 to 0.10652, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1069 - val_loss: 0.1065\n",
      "Epoch 1081/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 01081: val_loss improved from 0.10652 to 0.10652, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1069 - val_loss: 0.1065\n",
      "Epoch 1082/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 01082: val_loss improved from 0.10652 to 0.10651, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1069 - val_loss: 0.1065\n",
      "Epoch 1083/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1069\n",
      "Epoch 01083: val_loss did not improve from 0.10651\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1069 - val_loss: 0.1065\n",
      "Epoch 1084/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 01084: val_loss improved from 0.10651 to 0.10649, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1068 - val_loss: 0.1065\n",
      "Epoch 1085/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 01085: val_loss improved from 0.10649 to 0.10648, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1068 - val_loss: 0.1065\n",
      "Epoch 1086/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 01086: val_loss improved from 0.10648 to 0.10647, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.1068 - val_loss: 0.1065\n",
      "Epoch 1087/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 01087: val_loss improved from 0.10647 to 0.10647, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1068 - val_loss: 0.1065\n",
      "Epoch 1088/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 01088: val_loss did not improve from 0.10647\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1068 - val_loss: 0.1065\n",
      "Epoch 1089/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 01089: val_loss improved from 0.10647 to 0.10645, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1068 - val_loss: 0.1065\n",
      "Epoch 1090/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 01090: val_loss improved from 0.10645 to 0.10644, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1068 - val_loss: 0.1064\n",
      "Epoch 1091/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 01091: val_loss improved from 0.10644 to 0.10643, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1068 - val_loss: 0.1064\n",
      "Epoch 1092/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 01092: val_loss improved from 0.10643 to 0.10643, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1068 - val_loss: 0.1064\n",
      "Epoch 1093/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 01093: val_loss improved from 0.10643 to 0.10642, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1068 - val_loss: 0.1064\n",
      "Epoch 1094/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 01094: val_loss improved from 0.10642 to 0.10641, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1068 - val_loss: 0.1064\n",
      "Epoch 1095/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1068\n",
      "Epoch 01095: val_loss improved from 0.10641 to 0.10640, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1068 - val_loss: 0.1064\n",
      "Epoch 1096/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 01096: val_loss did not improve from 0.10640\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1067 - val_loss: 0.1064\n",
      "Epoch 1097/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 01097: val_loss improved from 0.10640 to 0.10639, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1067 - val_loss: 0.1064\n",
      "Epoch 1098/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 01098: val_loss improved from 0.10639 to 0.10639, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1067 - val_loss: 0.1064\n",
      "Epoch 1099/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 01099: val_loss improved from 0.10639 to 0.10637, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1067 - val_loss: 0.1064\n",
      "Epoch 1100/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 01100: val_loss improved from 0.10637 to 0.10636, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1067 - val_loss: 0.1064\n",
      "Epoch 1101/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 01101: val_loss did not improve from 0.10636\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1067 - val_loss: 0.1064\n",
      "Epoch 1102/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 01102: val_loss improved from 0.10636 to 0.10636, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1067 - val_loss: 0.1064\n",
      "Epoch 1103/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 01103: val_loss improved from 0.10636 to 0.10634, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1067 - val_loss: 0.1063\n",
      "Epoch 1104/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 01104: val_loss improved from 0.10634 to 0.10633, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1067 - val_loss: 0.1063\n",
      "Epoch 1105/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 01105: val_loss improved from 0.10633 to 0.10632, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1067 - val_loss: 0.1063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1106/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 01106: val_loss improved from 0.10632 to 0.10632, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1067 - val_loss: 0.1063\n",
      "Epoch 1107/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1067\n",
      "Epoch 01107: val_loss improved from 0.10632 to 0.10630, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1067 - val_loss: 0.1063\n",
      "Epoch 1108/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 01108: val_loss improved from 0.10630 to 0.10630, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1066 - val_loss: 0.1063\n",
      "Epoch 1109/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 01109: val_loss did not improve from 0.10630\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1066 - val_loss: 0.1063\n",
      "Epoch 1110/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 01110: val_loss did not improve from 0.10630\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1066 - val_loss: 0.1063\n",
      "Epoch 1111/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 01111: val_loss improved from 0.10630 to 0.10628, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1066 - val_loss: 0.1063\n",
      "Epoch 1112/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 01112: val_loss improved from 0.10628 to 0.10626, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1066 - val_loss: 0.1063\n",
      "Epoch 1113/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 01113: val_loss improved from 0.10626 to 0.10626, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1066 - val_loss: 0.1063\n",
      "Epoch 1114/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 01114: val_loss did not improve from 0.10626\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1066 - val_loss: 0.1063\n",
      "Epoch 1115/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 01115: val_loss improved from 0.10626 to 0.10625, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1066 - val_loss: 0.1062\n",
      "Epoch 1116/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 01116: val_loss improved from 0.10625 to 0.10624, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1066 - val_loss: 0.1062\n",
      "Epoch 1117/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 01117: val_loss improved from 0.10624 to 0.10623, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1066 - val_loss: 0.1062\n",
      "Epoch 1118/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 01118: val_loss did not improve from 0.10623\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1066 - val_loss: 0.1062\n",
      "Epoch 1119/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 01119: val_loss improved from 0.10623 to 0.10623, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1066 - val_loss: 0.1062\n",
      "Epoch 1120/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1066\n",
      "Epoch 01120: val_loss improved from 0.10623 to 0.10621, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.1066 - val_loss: 0.1062\n",
      "Epoch 1121/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 01121: val_loss improved from 0.10621 to 0.10620, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1065 - val_loss: 0.1062\n",
      "Epoch 1122/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 01122: val_loss improved from 0.10620 to 0.10619, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1065 - val_loss: 0.1062\n",
      "Epoch 1123/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 01123: val_loss did not improve from 0.10619\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1065 - val_loss: 0.1062\n",
      "Epoch 1124/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 01124: val_loss improved from 0.10619 to 0.10619, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1065 - val_loss: 0.1062\n",
      "Epoch 1125/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 01125: val_loss improved from 0.10619 to 0.10617, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1065 - val_loss: 0.1062\n",
      "Epoch 1126/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1065- ETA: 0s - loss: 0.1\n",
      "Epoch 01126: val_loss improved from 0.10617 to 0.10616, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1065 - val_loss: 0.1062\n",
      "Epoch 1127/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 01127: val_loss did not improve from 0.10616\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1065 - val_loss: 0.1062\n",
      "Epoch 1128/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 01128: val_loss improved from 0.10616 to 0.10616, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1065 - val_loss: 0.1062\n",
      "Epoch 1129/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 01129: val_loss improved from 0.10616 to 0.10614, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1065 - val_loss: 0.1061\n",
      "Epoch 1130/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 01130: val_loss improved from 0.10614 to 0.10614, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1065 - val_loss: 0.1061\n",
      "Epoch 1131/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 01131: val_loss did not improve from 0.10614\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1065 - val_loss: 0.1061\n",
      "Epoch 1132/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 01132: val_loss improved from 0.10614 to 0.10612, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1065 - val_loss: 0.1061\n",
      "Epoch 1133/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1065\n",
      "Epoch 01133: val_loss improved from 0.10612 to 0.10611, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1065 - val_loss: 0.1061\n",
      "Epoch 1134/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01134: val_loss improved from 0.10611 to 0.10611, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1064 - val_loss: 0.1061\n",
      "Epoch 1135/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01135: val_loss improved from 0.10611 to 0.10610, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1064 - val_loss: 0.1061\n",
      "Epoch 1136/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01136: val_loss improved from 0.10610 to 0.10609, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1064 - val_loss: 0.1061\n",
      "Epoch 1137/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01137: val_loss improved from 0.10609 to 0.10608, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1064 - val_loss: 0.1061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1138/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01138: val_loss did not improve from 0.10608\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1064 - val_loss: 0.1061\n",
      "Epoch 1139/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01139: val_loss did not improve from 0.10608\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1064 - val_loss: 0.1061\n",
      "Epoch 1140/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01140: val_loss improved from 0.10608 to 0.10607, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1064 - val_loss: 0.1061\n",
      "Epoch 1141/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01141: val_loss improved from 0.10607 to 0.10605, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1064 - val_loss: 0.1061\n",
      "Epoch 1142/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01142: val_loss improved from 0.10605 to 0.10605, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1064 - val_loss: 0.1060\n",
      "Epoch 1143/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01143: val_loss did not improve from 0.10605\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1064 - val_loss: 0.1061\n",
      "Epoch 1144/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01144: val_loss improved from 0.10605 to 0.10604, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1064 - val_loss: 0.1060\n",
      "Epoch 1145/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01145: val_loss improved from 0.10604 to 0.10603, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1064 - val_loss: 0.1060\n",
      "Epoch 1146/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01146: val_loss improved from 0.10603 to 0.10602, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1064 - val_loss: 0.1060\n",
      "Epoch 1147/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1064\n",
      "Epoch 01147: val_loss improved from 0.10602 to 0.10602, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1064 - val_loss: 0.1060\n",
      "Epoch 1148/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 01148: val_loss improved from 0.10602 to 0.10602, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1063 - val_loss: 0.1060\n",
      "Epoch 1149/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 01149: val_loss improved from 0.10602 to 0.10600, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1063 - val_loss: 0.1060\n",
      "Epoch 1150/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 01150: val_loss improved from 0.10600 to 0.10599, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1063 - val_loss: 0.1060\n",
      "Epoch 1151/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 01151: val_loss improved from 0.10599 to 0.10599, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1063 - val_loss: 0.1060\n",
      "Epoch 1152/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 01152: val_loss improved from 0.10599 to 0.10598, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1063 - val_loss: 0.1060\n",
      "Epoch 1153/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 01153: val_loss did not improve from 0.10598\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1063 - val_loss: 0.1060\n",
      "Epoch 1154/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 01154: val_loss improved from 0.10598 to 0.10597, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1063 - val_loss: 0.1060\n",
      "Epoch 1155/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 01155: val_loss improved from 0.10597 to 0.10596, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1063 - val_loss: 0.1060\n",
      "Epoch 1156/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 01156: val_loss improved from 0.10596 to 0.10595, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1063 - val_loss: 0.1059\n",
      "Epoch 1157/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063- ETA: 0s - loss: 0.105\n",
      "Epoch 01157: val_loss did not improve from 0.10595\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1063 - val_loss: 0.1060\n",
      "Epoch 1158/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 01158: val_loss improved from 0.10595 to 0.10594, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1063 - val_loss: 0.1059\n",
      "Epoch 1159/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 01159: val_loss improved from 0.10594 to 0.10593, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1063 - val_loss: 0.1059\n",
      "Epoch 1160/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 01160: val_loss improved from 0.10593 to 0.10593, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1063 - val_loss: 0.1059\n",
      "Epoch 1161/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1063\n",
      "Epoch 01161: val_loss improved from 0.10593 to 0.10592, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1063 - val_loss: 0.1059\n",
      "Epoch 1162/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01162: val_loss improved from 0.10592 to 0.10591, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1062 - val_loss: 0.1059\n",
      "Epoch 1163/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01163: val_loss improved from 0.10591 to 0.10591, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1062 - val_loss: 0.1059\n",
      "Epoch 1164/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01164: val_loss improved from 0.10591 to 0.10590, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1062 - val_loss: 0.1059\n",
      "Epoch 1165/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01165: val_loss improved from 0.10590 to 0.10589, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1062 - val_loss: 0.1059\n",
      "Epoch 1166/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01166: val_loss improved from 0.10589 to 0.10588, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1062 - val_loss: 0.1059\n",
      "Epoch 1167/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01167: val_loss did not improve from 0.10588\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1062 - val_loss: 0.1059\n",
      "Epoch 1168/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01168: val_loss improved from 0.10588 to 0.10587, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1062 - val_loss: 0.1059\n",
      "Epoch 1169/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01169: val_loss improved from 0.10587 to 0.10587, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1062 - val_loss: 0.1059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1170/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01170: val_loss improved from 0.10587 to 0.10586, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1062 - val_loss: 0.1059\n",
      "Epoch 1171/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01171: val_loss did not improve from 0.10586\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1062 - val_loss: 0.1059\n",
      "Epoch 1172/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01172: val_loss improved from 0.10586 to 0.10584, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1062 - val_loss: 0.1058\n",
      "Epoch 1173/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01173: val_loss improved from 0.10584 to 0.10583, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1062 - val_loss: 0.1058\n",
      "Epoch 1174/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01174: val_loss improved from 0.10583 to 0.10583, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1062 - val_loss: 0.1058\n",
      "Epoch 1175/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1062\n",
      "Epoch 01175: val_loss did not improve from 0.10583\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1062 - val_loss: 0.1058\n",
      "Epoch 1176/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01176: val_loss improved from 0.10583 to 0.10582, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1061 - val_loss: 0.1058\n",
      "Epoch 1177/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01177: val_loss improved from 0.10582 to 0.10581, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1061 - val_loss: 0.1058\n",
      "Epoch 1178/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01178: val_loss improved from 0.10581 to 0.10580, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1061 - val_loss: 0.1058\n",
      "Epoch 1179/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01179: val_loss improved from 0.10580 to 0.10580, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1061 - val_loss: 0.1058\n",
      "Epoch 1180/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01180: val_loss improved from 0.10580 to 0.10579, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1061 - val_loss: 0.1058\n",
      "Epoch 1181/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01181: val_loss improved from 0.10579 to 0.10579, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1061 - val_loss: 0.1058\n",
      "Epoch 1182/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01182: val_loss improved from 0.10579 to 0.10579, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1061 - val_loss: 0.1058\n",
      "Epoch 1183/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01183: val_loss improved from 0.10579 to 0.10577, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1061 - val_loss: 0.1058\n",
      "Epoch 1184/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01184: val_loss improved from 0.10577 to 0.10577, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1061 - val_loss: 0.1058\n",
      "Epoch 1185/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01185: val_loss improved from 0.10577 to 0.10576, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1061 - val_loss: 0.1058\n",
      "Epoch 1186/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01186: val_loss improved from 0.10576 to 0.10575, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1061 - val_loss: 0.1058\n",
      "Epoch 1187/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01187: val_loss did not improve from 0.10575\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1061 - val_loss: 0.1058\n",
      "Epoch 1188/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01188: val_loss did not improve from 0.10575\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1061 - val_loss: 0.1058\n",
      "Epoch 1189/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01189: val_loss improved from 0.10575 to 0.10573, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1061 - val_loss: 0.1057\n",
      "Epoch 1190/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01190: val_loss improved from 0.10573 to 0.10573, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1061 - val_loss: 0.1057\n",
      "Epoch 1191/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1061\n",
      "Epoch 01191: val_loss improved from 0.10573 to 0.10572, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1061 - val_loss: 0.1057\n",
      "Epoch 1192/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01192: val_loss did not improve from 0.10572\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1060 - val_loss: 0.1057\n",
      "Epoch 1193/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01193: val_loss improved from 0.10572 to 0.10571, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1060 - val_loss: 0.1057\n",
      "Epoch 1194/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01194: val_loss improved from 0.10571 to 0.10570, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1060 - val_loss: 0.1057\n",
      "Epoch 1195/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01195: val_loss did not improve from 0.10570\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1060 - val_loss: 0.1057\n",
      "Epoch 1196/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01196: val_loss improved from 0.10570 to 0.10569, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1060 - val_loss: 0.1057\n",
      "Epoch 1197/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01197: val_loss improved from 0.10569 to 0.10568, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1060 - val_loss: 0.1057\n",
      "Epoch 1198/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01198: val_loss improved from 0.10568 to 0.10568, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1060 - val_loss: 0.1057\n",
      "Epoch 1199/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01199: val_loss improved from 0.10568 to 0.10567, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1060 - val_loss: 0.1057\n",
      "Epoch 1200/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01200: val_loss did not improve from 0.10567\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1060 - val_loss: 0.1057\n",
      "Epoch 1201/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01201: val_loss improved from 0.10567 to 0.10566, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1060 - val_loss: 0.1057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1202/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01202: val_loss improved from 0.10566 to 0.10565, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1060 - val_loss: 0.1057\n",
      "Epoch 1203/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01203: val_loss improved from 0.10565 to 0.10565, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1060 - val_loss: 0.1056\n",
      "Epoch 1204/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01204: val_loss improved from 0.10565 to 0.10564, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1060 - val_loss: 0.1056\n",
      "Epoch 1205/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1060\n",
      "Epoch 01205: val_loss improved from 0.10564 to 0.10563, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1060 - val_loss: 0.1056\n",
      "Epoch 1206/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01206: val_loss did not improve from 0.10563\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1207/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01207: val_loss improved from 0.10563 to 0.10562, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1208/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01208: val_loss improved from 0.10562 to 0.10562, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1209/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01209: val_loss improved from 0.10562 to 0.10561, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1210/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01210: val_loss improved from 0.10561 to 0.10560, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1211/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01211: val_loss improved from 0.10560 to 0.10559, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1212/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01212: val_loss improved from 0.10559 to 0.10559, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1213/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01213: val_loss improved from 0.10559 to 0.10559, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1214/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01214: val_loss improved from 0.10559 to 0.10558, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1215/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01215: val_loss did not improve from 0.10558\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1216/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01216: val_loss improved from 0.10558 to 0.10557, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1217/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01217: val_loss improved from 0.10557 to 0.10556, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1218/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01218: val_loss did not improve from 0.10556\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1219/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01219: val_loss did not improve from 0.10556\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1059 - val_loss: 0.1056\n",
      "Epoch 1220/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1059\n",
      "Epoch 01220: val_loss improved from 0.10556 to 0.10554, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1059 - val_loss: 0.1055\n",
      "Epoch 1221/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01221: val_loss improved from 0.10554 to 0.10553, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1058 - val_loss: 0.1055\n",
      "Epoch 1222/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01222: val_loss improved from 0.10553 to 0.10553, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1058 - val_loss: 0.1055\n",
      "Epoch 1223/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01223: val_loss did not improve from 0.10553\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1058 - val_loss: 0.1056\n",
      "Epoch 1224/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01224: val_loss improved from 0.10553 to 0.10551, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1058 - val_loss: 0.1055\n",
      "Epoch 1225/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01225: val_loss did not improve from 0.10551\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1058 - val_loss: 0.1055\n",
      "Epoch 1226/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01226: val_loss did not improve from 0.10551\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1058 - val_loss: 0.1055\n",
      "Epoch 1227/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01227: val_loss did not improve from 0.10551\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1058 - val_loss: 0.1055\n",
      "Epoch 1228/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01228: val_loss improved from 0.10551 to 0.10549, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1058 - val_loss: 0.1055\n",
      "Epoch 1229/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01229: val_loss improved from 0.10549 to 0.10548, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1058 - val_loss: 0.1055\n",
      "Epoch 1230/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.105 - ETA: 0s - loss: 0.1058\n",
      "Epoch 01230: val_loss improved from 0.10548 to 0.10548, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1058 - val_loss: 0.1055\n",
      "Epoch 1231/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01231: val_loss improved from 0.10548 to 0.10547, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1058 - val_loss: 0.1055\n",
      "Epoch 1232/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01232: val_loss improved from 0.10547 to 0.10546, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1058 - val_loss: 0.1055\n",
      "Epoch 1233/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01233: val_loss improved from 0.10546 to 0.10546, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1058 - val_loss: 0.1055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1234/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01234: val_loss improved from 0.10546 to 0.10546, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1058 - val_loss: 0.1055\n",
      "Epoch 1235/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1058\n",
      "Epoch 01235: val_loss improved from 0.10546 to 0.10545, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1058 - val_loss: 0.1055\n",
      "Epoch 1236/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01236: val_loss improved from 0.10545 to 0.10544, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1237/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01237: val_loss did not improve from 0.10544\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1238/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01238: val_loss improved from 0.10544 to 0.10543, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1239/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01239: val_loss improved from 0.10543 to 0.10542, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1240/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01240: val_loss improved from 0.10542 to 0.10542, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1241/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01241: val_loss did not improve from 0.10542\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1242/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01242: val_loss improved from 0.10542 to 0.10541, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1243/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01243: val_loss improved from 0.10541 to 0.10540, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1244/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01244: val_loss improved from 0.10540 to 0.10539, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1245/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01245: val_loss improved from 0.10539 to 0.10539, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1246/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01246: val_loss improved from 0.10539 to 0.10538, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1247/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01247: val_loss did not improve from 0.10538\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1248/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01248: val_loss did not improve from 0.10538\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1249/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01249: val_loss improved from 0.10538 to 0.10537, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1250/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01250: val_loss improved from 0.10537 to 0.10535, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1057 - val_loss: 0.1054\n",
      "Epoch 1251/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1057\n",
      "Epoch 01251: val_loss improved from 0.10535 to 0.10535, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1057 - val_loss: 0.1053\n",
      "Epoch 1252/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01252: val_loss did not improve from 0.10535\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1056 - val_loss: 0.1054\n",
      "Epoch 1253/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01253: val_loss improved from 0.10535 to 0.10534, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1254/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01254: val_loss improved from 0.10534 to 0.10533, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1255/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01255: val_loss improved from 0.10533 to 0.10532, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1256/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01256: val_loss did not improve from 0.10532\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1257/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01257: val_loss did not improve from 0.10532\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1258/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01258: val_loss improved from 0.10532 to 0.10531, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1259/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01259: val_loss improved from 0.10531 to 0.10530, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1260/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01260: val_loss did not improve from 0.10530\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1261/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01261: val_loss improved from 0.10530 to 0.10530, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1262/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01262: val_loss improved from 0.10530 to 0.10528, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1263/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01263: val_loss improved from 0.10528 to 0.10528, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1264/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01264: val_loss did not improve from 0.10528\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1265/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01265: val_loss improved from 0.10528 to 0.10527, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1266/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.1056\n",
      "Epoch 01266: val_loss improved from 0.10527 to 0.10526, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1056 - val_loss: 0.1053\n",
      "Epoch 1267/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01267: val_loss improved from 0.10526 to 0.10525, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1268/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01268: val_loss did not improve from 0.10525\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1269/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01269: val_loss improved from 0.10525 to 0.10524, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1270/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01270: val_loss improved from 0.10524 to 0.10524, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1271/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01271: val_loss improved from 0.10524 to 0.10523, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1272/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01272: val_loss improved from 0.10523 to 0.10522, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1273/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01273: val_loss improved from 0.10522 to 0.10521, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1274/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01274: val_loss improved from 0.10521 to 0.10521, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1275/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01275: val_loss did not improve from 0.10521\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1276/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01276: val_loss did not improve from 0.10521\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1277/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01277: val_loss improved from 0.10521 to 0.10519, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1278/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01278: val_loss improved from 0.10519 to 0.10519, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1279/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01279: val_loss improved from 0.10519 to 0.10518, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1280/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01280: val_loss improved from 0.10518 to 0.10517, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1281/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01281: val_loss improved from 0.10517 to 0.10517, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1282/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01282: val_loss did not improve from 0.10517\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1283/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01283: val_loss improved from 0.10517 to 0.10515, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1055 - val_loss: 0.1052\n",
      "Epoch 1284/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1055\n",
      "Epoch 01284: val_loss improved from 0.10515 to 0.10515, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1055 - val_loss: 0.1051\n",
      "Epoch 1285/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01285: val_loss improved from 0.10515 to 0.10515, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1286/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01286: val_loss did not improve from 0.10515\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1287/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01287: val_loss improved from 0.10515 to 0.10513, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1288/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01288: val_loss improved from 0.10513 to 0.10513, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1289/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01289: val_loss improved from 0.10513 to 0.10512, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1290/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01290: val_loss did not improve from 0.10512\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1291/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01291: val_loss improved from 0.10512 to 0.10511, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1292/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01292: val_loss improved from 0.10511 to 0.10510, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1293/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01293: val_loss improved from 0.10510 to 0.10509, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1294/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01294: val_loss improved from 0.10509 to 0.10509, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1295/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01295: val_loss improved from 0.10509 to 0.10509, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1296/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01296: val_loss improved from 0.10509 to 0.10508, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1297/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01297: val_loss improved from 0.10508 to 0.10508, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1054 - val_loss: 0.1051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1298/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01298: val_loss improved from 0.10508 to 0.10507, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1299/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1054\n",
      "Epoch 01299: val_loss improved from 0.10507 to 0.10507, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1054 - val_loss: 0.1051\n",
      "Epoch 1300/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01300: val_loss improved from 0.10507 to 0.10505, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1053 - val_loss: 0.1051\n",
      "Epoch 1301/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01301: val_loss improved from 0.10505 to 0.10505, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1302/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01302: val_loss improved from 0.10505 to 0.10504, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1303/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01303: val_loss improved from 0.10504 to 0.10504, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1304/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01304: val_loss improved from 0.10504 to 0.10503, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1305/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01305: val_loss improved from 0.10503 to 0.10503, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1306/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01306: val_loss improved from 0.10503 to 0.10502, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1307/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01307: val_loss improved from 0.10502 to 0.10502, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1308/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01308: val_loss improved from 0.10502 to 0.10501, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1309/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01309: val_loss did not improve from 0.10501\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1310/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01310: val_loss improved from 0.10501 to 0.10500, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1311/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01311: val_loss did not improve from 0.10500\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1312/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01312: val_loss improved from 0.10500 to 0.10498, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1313/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01313: val_loss improved from 0.10498 to 0.10497, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1314/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01314: val_loss did not improve from 0.10497\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1315/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01315: val_loss improved from 0.10497 to 0.10496, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1316/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1053\n",
      "Epoch 01316: val_loss did not improve from 0.10496\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1053 - val_loss: 0.1050\n",
      "Epoch 1317/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01317: val_loss improved from 0.10496 to 0.10496, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1052 - val_loss: 0.1050\n",
      "Epoch 1318/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01318: val_loss improved from 0.10496 to 0.10495, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1319/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01319: val_loss improved from 0.10495 to 0.10494, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1320/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01320: val_loss improved from 0.10494 to 0.10494, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1321/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01321: val_loss improved from 0.10494 to 0.10493, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1322/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01322: val_loss improved from 0.10493 to 0.10493, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1323/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01323: val_loss improved from 0.10493 to 0.10493, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1324/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01324: val_loss improved from 0.10493 to 0.10492, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1325/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01325: val_loss improved from 0.10492 to 0.10491, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1326/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01326: val_loss improved from 0.10491 to 0.10490, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1327/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01327: val_loss did not improve from 0.10490\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1328/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01328: val_loss improved from 0.10490 to 0.10489, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1329/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01329: val_loss improved from 0.10489 to 0.10488, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1330/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01330: val_loss improved from 0.10488 to 0.10488, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1331/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01331: val_loss improved from 0.10488 to 0.10487, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1332/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01332: val_loss improved from 0.10487 to 0.10487, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1333/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1052\n",
      "Epoch 01333: val_loss did not improve from 0.10487\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1052 - val_loss: 0.1049\n",
      "Epoch 1334/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01334: val_loss improved from 0.10487 to 0.10485, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1051 - val_loss: 0.1049\n",
      "Epoch 1335/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01335: val_loss improved from 0.10485 to 0.10485, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1336/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01336: val_loss improved from 0.10485 to 0.10484, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1337/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01337: val_loss did not improve from 0.10484\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1338/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01338: val_loss improved from 0.10484 to 0.10483, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1339/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01339: val_loss did not improve from 0.10483\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1340/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01340: val_loss improved from 0.10483 to 0.10482, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1341/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01341: val_loss improved from 0.10482 to 0.10482, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1342/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01342: val_loss did not improve from 0.10482\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1343/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01343: val_loss improved from 0.10482 to 0.10480, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1344/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01344: val_loss improved from 0.10480 to 0.10480, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1345/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01345: val_loss did not improve from 0.10480\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1346/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01346: val_loss improved from 0.10480 to 0.10479, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1347/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01347: val_loss improved from 0.10479 to 0.10479, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1348/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01348: val_loss improved from 0.10479 to 0.10477, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1349/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01349: val_loss improved from 0.10477 to 0.10477, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1350/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1051\n",
      "Epoch 01350: val_loss improved from 0.10477 to 0.10476, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1051 - val_loss: 0.1048\n",
      "Epoch 1351/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01351: val_loss did not improve from 0.10476\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1050 - val_loss: 0.1048\n",
      "Epoch 1352/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01352: val_loss improved from 0.10476 to 0.10476, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1050 - val_loss: 0.1048\n",
      "Epoch 1353/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01353: val_loss improved from 0.10476 to 0.10475, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1050 - val_loss: 0.1047\n",
      "Epoch 1354/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01354: val_loss improved from 0.10475 to 0.10474, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1050 - val_loss: 0.1047\n",
      "Epoch 1355/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01355: val_loss did not improve from 0.10474\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1050 - val_loss: 0.1048\n",
      "Epoch 1356/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01356: val_loss improved from 0.10474 to 0.10473, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1050 - val_loss: 0.1047\n",
      "Epoch 1357/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01357: val_loss did not improve from 0.10473\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1050 - val_loss: 0.1047\n",
      "Epoch 1358/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01358: val_loss improved from 0.10473 to 0.10472, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1050 - val_loss: 0.1047\n",
      "Epoch 1359/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01359: val_loss improved from 0.10472 to 0.10471, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1050 - val_loss: 0.1047\n",
      "Epoch 1360/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01360: val_loss did not improve from 0.10471\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1050 - val_loss: 0.1047\n",
      "Epoch 1361/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01361: val_loss improved from 0.10471 to 0.10470, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1050 - val_loss: 0.1047\n",
      "Epoch 1362/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01362: val_loss did not improve from 0.10470\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1050 - val_loss: 0.1047\n",
      "Epoch 1363/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01363: val_loss did not improve from 0.10470\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1050 - val_loss: 0.1047\n",
      "Epoch 1364/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01364: val_loss improved from 0.10470 to 0.10469, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1050 - val_loss: 0.1047\n",
      "Epoch 1365/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01365: val_loss improved from 0.10469 to 0.10468, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1050 - val_loss: 0.1047\n",
      "Epoch 1366/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1050\n",
      "Epoch 01366: val_loss improved from 0.10468 to 0.10468, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1050 - val_loss: 0.1047\n",
      "Epoch 1367/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01367: val_loss improved from 0.10468 to 0.10467, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1049 - val_loss: 0.1047\n",
      "Epoch 1368/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01368: val_loss improved from 0.10467 to 0.10466, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1049 - val_loss: 0.1047\n",
      "Epoch 1369/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01369: val_loss improved from 0.10466 to 0.10466, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1049 - val_loss: 0.1047\n",
      "Epoch 1370/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01370: val_loss improved from 0.10466 to 0.10465, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1049 - val_loss: 0.1046\n",
      "Epoch 1371/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01371: val_loss improved from 0.10465 to 0.10464, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1049 - val_loss: 0.1046\n",
      "Epoch 1372/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01372: val_loss did not improve from 0.10464\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1049 - val_loss: 0.1047\n",
      "Epoch 1373/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01373: val_loss improved from 0.10464 to 0.10464, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1049 - val_loss: 0.1046\n",
      "Epoch 1374/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01374: val_loss improved from 0.10464 to 0.10463, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1049 - val_loss: 0.1046\n",
      "Epoch 1375/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01375: val_loss did not improve from 0.10463\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1049 - val_loss: 0.1046\n",
      "Epoch 1376/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01376: val_loss improved from 0.10463 to 0.10461, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1049 - val_loss: 0.1046\n",
      "Epoch 1377/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01377: val_loss improved from 0.10461 to 0.10461, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1049 - val_loss: 0.1046\n",
      "Epoch 1378/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01378: val_loss improved from 0.10461 to 0.10460, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1049 - val_loss: 0.1046\n",
      "Epoch 1379/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01379: val_loss did not improve from 0.10460\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1049 - val_loss: 0.1046\n",
      "Epoch 1380/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01380: val_loss did not improve from 0.10460\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1049 - val_loss: 0.1046\n",
      "Epoch 1381/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01381: val_loss improved from 0.10460 to 0.10458, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1049 - val_loss: 0.1046\n",
      "Epoch 1382/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01382: val_loss improved from 0.10458 to 0.10458, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1049 - val_loss: 0.1046\n",
      "Epoch 1383/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1049\n",
      "Epoch 01383: val_loss improved from 0.10458 to 0.10458, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1049 - val_loss: 0.1046\n",
      "Epoch 1384/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01384: val_loss did not improve from 0.10458\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1048 - val_loss: 0.1046\n",
      "Epoch 1385/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01385: val_loss did not improve from 0.10458\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1048 - val_loss: 0.1046\n",
      "Epoch 1386/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01386: val_loss improved from 0.10458 to 0.10456, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1048 - val_loss: 0.1046\n",
      "Epoch 1387/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01387: val_loss improved from 0.10456 to 0.10455, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1048 - val_loss: 0.1046\n",
      "Epoch 1388/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01388: val_loss improved from 0.10455 to 0.10455, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1048 - val_loss: 0.1045\n",
      "Epoch 1389/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01389: val_loss improved from 0.10455 to 0.10454, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1048 - val_loss: 0.1045\n",
      "Epoch 1390/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01390: val_loss improved from 0.10454 to 0.10454, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1048 - val_loss: 0.1045\n",
      "Epoch 1391/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01391: val_loss did not improve from 0.10454\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1048 - val_loss: 0.1046\n",
      "Epoch 1392/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01392: val_loss improved from 0.10454 to 0.10453, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1048 - val_loss: 0.1045\n",
      "Epoch 1393/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01393: val_loss improved from 0.10453 to 0.10453, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1048 - val_loss: 0.1045\n",
      "Epoch 1394/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01394: val_loss improved from 0.10453 to 0.10451, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1048 - val_loss: 0.1045\n",
      "Epoch 1395/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01395: val_loss improved from 0.10451 to 0.10451, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1048 - val_loss: 0.1045\n",
      "Epoch 1396/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01396: val_loss improved from 0.10451 to 0.10451, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1048 - val_loss: 0.1045\n",
      "Epoch 1397/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01397: val_loss did not improve from 0.10451\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1048 - val_loss: 0.1045\n",
      "Epoch 1398/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01398: val_loss did not improve from 0.10451\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1048 - val_loss: 0.1045\n",
      "Epoch 1399/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01399: val_loss improved from 0.10451 to 0.10449, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1048 - val_loss: 0.1045\n",
      "Epoch 1400/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1048\n",
      "Epoch 01400: val_loss improved from 0.10449 to 0.10448, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1048 - val_loss: 0.1045\n",
      "Epoch 1401/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01401: val_loss did not improve from 0.10448\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1047 - val_loss: 0.1045\n",
      "Epoch 1402/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01402: val_loss improved from 0.10448 to 0.10447, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1047 - val_loss: 0.1045\n",
      "Epoch 1403/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01403: val_loss improved from 0.10447 to 0.10447, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1047 - val_loss: 0.1045\n",
      "Epoch 1404/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01404: val_loss did not improve from 0.10447\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1047 - val_loss: 0.1045\n",
      "Epoch 1405/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01405: val_loss improved from 0.10447 to 0.10446, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1047 - val_loss: 0.1045\n",
      "Epoch 1406/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01406: val_loss improved from 0.10446 to 0.10446, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1047 - val_loss: 0.1045\n",
      "Epoch 1407/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01407: val_loss did not improve from 0.10446\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1047 - val_loss: 0.1045\n",
      "Epoch 1408/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01408: val_loss improved from 0.10446 to 0.10444, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1047 - val_loss: 0.1044\n",
      "Epoch 1409/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01409: val_loss improved from 0.10444 to 0.10443, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1047 - val_loss: 0.1044\n",
      "Epoch 1410/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01410: val_loss improved from 0.10443 to 0.10443, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1047 - val_loss: 0.1044\n",
      "Epoch 1411/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01411: val_loss improved from 0.10443 to 0.10443, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1047 - val_loss: 0.1044\n",
      "Epoch 1412/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01412: val_loss did not improve from 0.10443\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1047 - val_loss: 0.1044\n",
      "Epoch 1413/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01413: val_loss improved from 0.10443 to 0.10442, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1047 - val_loss: 0.1044\n",
      "Epoch 1414/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01414: val_loss improved from 0.10442 to 0.10441, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1047 - val_loss: 0.1044\n",
      "Epoch 1415/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01415: val_loss improved from 0.10441 to 0.10441, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1047 - val_loss: 0.1044\n",
      "Epoch 1416/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01416: val_loss improved from 0.10441 to 0.10440, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1047 - val_loss: 0.1044\n",
      "Epoch 1417/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01417: val_loss did not improve from 0.10440\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1047 - val_loss: 0.1044\n",
      "Epoch 1418/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1047\n",
      "Epoch 01418: val_loss improved from 0.10440 to 0.10440, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1047 - val_loss: 0.1044\n",
      "Epoch 1419/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01419: val_loss improved from 0.10440 to 0.10439, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1046 - val_loss: 0.1044\n",
      "Epoch 1420/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01420: val_loss improved from 0.10439 to 0.10438, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1046 - val_loss: 0.1044\n",
      "Epoch 1421/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01421: val_loss improved from 0.10438 to 0.10438, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1046 - val_loss: 0.1044\n",
      "Epoch 1422/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01422: val_loss improved from 0.10438 to 0.10438, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1046 - val_loss: 0.1044\n",
      "Epoch 1423/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01423: val_loss improved from 0.10438 to 0.10437, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1046 - val_loss: 0.1044\n",
      "Epoch 1424/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01424: val_loss did not improve from 0.10437\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1046 - val_loss: 0.1044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1425/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01425: val_loss did not improve from 0.10437\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1046 - val_loss: 0.1044\n",
      "Epoch 1426/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01426: val_loss improved from 0.10437 to 0.10435, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1046 - val_loss: 0.1043\n",
      "Epoch 1427/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01427: val_loss improved from 0.10435 to 0.10434, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1046 - val_loss: 0.1043\n",
      "Epoch 1428/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01428: val_loss improved from 0.10434 to 0.10434, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1046 - val_loss: 0.1043\n",
      "Epoch 1429/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01429: val_loss improved from 0.10434 to 0.10433, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1046 - val_loss: 0.1043\n",
      "Epoch 1430/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01430: val_loss did not improve from 0.10433\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1046 - val_loss: 0.1043\n",
      "Epoch 1431/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01431: val_loss improved from 0.10433 to 0.10433, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1046 - val_loss: 0.1043\n",
      "Epoch 1432/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01432: val_loss improved from 0.10433 to 0.10432, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1046 - val_loss: 0.1043\n",
      "Epoch 1433/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01433: val_loss improved from 0.10432 to 0.10431, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1046 - val_loss: 0.1043\n",
      "Epoch 1434/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01434: val_loss did not improve from 0.10431\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1046 - val_loss: 0.1043\n",
      "Epoch 1435/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01435: val_loss improved from 0.10431 to 0.10430, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1046 - val_loss: 0.1043\n",
      "Epoch 1436/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01436: val_loss improved from 0.10430 to 0.10430, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1046 - val_loss: 0.1043\n",
      "Epoch 1437/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1046\n",
      "Epoch 01437: val_loss improved from 0.10430 to 0.10430, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1046 - val_loss: 0.1043\n",
      "Epoch 1438/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01438: val_loss improved from 0.10430 to 0.10429, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1045 - val_loss: 0.1043\n",
      "Epoch 1439/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01439: val_loss improved from 0.10429 to 0.10428, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1045 - val_loss: 0.1043\n",
      "Epoch 1440/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01440: val_loss did not improve from 0.10428\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1045 - val_loss: 0.1043\n",
      "Epoch 1441/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01441: val_loss improved from 0.10428 to 0.10428, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1045 - val_loss: 0.1043\n",
      "Epoch 1442/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01442: val_loss improved from 0.10428 to 0.10427, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1045 - val_loss: 0.1043\n",
      "Epoch 1443/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01443: val_loss improved from 0.10427 to 0.10427, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1045 - val_loss: 0.1043\n",
      "Epoch 1444/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01444: val_loss did not improve from 0.10427\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1045 - val_loss: 0.1043\n",
      "Epoch 1445/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01445: val_loss improved from 0.10427 to 0.10426, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1045 - val_loss: 0.1043\n",
      "Epoch 1446/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01446: val_loss improved from 0.10426 to 0.10425, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1045 - val_loss: 0.1043\n",
      "Epoch 1447/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01447: val_loss improved from 0.10425 to 0.10425, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1045 - val_loss: 0.1043\n",
      "Epoch 1448/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01448: val_loss improved from 0.10425 to 0.10424, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1045 - val_loss: 0.1042\n",
      "Epoch 1449/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01449: val_loss improved from 0.10424 to 0.10423, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1045 - val_loss: 0.1042\n",
      "Epoch 1450/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01450: val_loss did not improve from 0.10423\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1045 - val_loss: 0.1042\n",
      "Epoch 1451/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01451: val_loss improved from 0.10423 to 0.10422, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1045 - val_loss: 0.1042\n",
      "Epoch 1452/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01452: val_loss improved from 0.10422 to 0.10422, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1045 - val_loss: 0.1042\n",
      "Epoch 1453/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01453: val_loss improved from 0.10422 to 0.10422, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1045 - val_loss: 0.1042\n",
      "Epoch 1454/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01454: val_loss did not improve from 0.10422\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1045 - val_loss: 0.1042\n",
      "Epoch 1455/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01455: val_loss improved from 0.10422 to 0.10421, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1045 - val_loss: 0.1042\n",
      "Epoch 1456/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1045\n",
      "Epoch 01456: val_loss improved from 0.10421 to 0.10421, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1045 - val_loss: 0.1042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1457/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01457: val_loss improved from 0.10421 to 0.10420, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1044 - val_loss: 0.1042\n",
      "Epoch 1458/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044- ETA: 0s - loss: 0.10\n",
      "Epoch 01458: val_loss improved from 0.10420 to 0.10419, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1044 - val_loss: 0.1042\n",
      "Epoch 1459/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01459: val_loss improved from 0.10419 to 0.10419, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1044 - val_loss: 0.1042\n",
      "Epoch 1460/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01460: val_loss improved from 0.10419 to 0.10418, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1044 - val_loss: 0.1042\n",
      "Epoch 1461/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01461: val_loss improved from 0.10418 to 0.10418, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1044 - val_loss: 0.1042\n",
      "Epoch 1462/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01462: val_loss did not improve from 0.10418\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1044 - val_loss: 0.1042\n",
      "Epoch 1463/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01463: val_loss improved from 0.10418 to 0.10417, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1044 - val_loss: 0.1042\n",
      "Epoch 1464/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01464: val_loss improved from 0.10417 to 0.10416, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1044 - val_loss: 0.1042\n",
      "Epoch 1465/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01465: val_loss improved from 0.10416 to 0.10416, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1044 - val_loss: 0.1042\n",
      "Epoch 1466/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01466: val_loss improved from 0.10416 to 0.10415, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1044 - val_loss: 0.1042\n",
      "Epoch 1467/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01467: val_loss improved from 0.10415 to 0.10414, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1044 - val_loss: 0.1041\n",
      "Epoch 1468/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01468: val_loss improved from 0.10414 to 0.10414, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1044 - val_loss: 0.1041\n",
      "Epoch 1469/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01469: val_loss improved from 0.10414 to 0.10414, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1044 - val_loss: 0.1041\n",
      "Epoch 1470/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01470: val_loss improved from 0.10414 to 0.10413, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1044 - val_loss: 0.1041\n",
      "Epoch 1471/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01471: val_loss improved from 0.10413 to 0.10413, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1044 - val_loss: 0.1041\n",
      "Epoch 1472/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01472: val_loss improved from 0.10413 to 0.10412, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1044 - val_loss: 0.1041\n",
      "Epoch 1473/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01473: val_loss improved from 0.10412 to 0.10412, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1044 - val_loss: 0.1041\n",
      "Epoch 1474/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1044\n",
      "Epoch 01474: val_loss improved from 0.10412 to 0.10411, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1044 - val_loss: 0.1041\n",
      "Epoch 1475/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01475: val_loss improved from 0.10411 to 0.10411, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1043 - val_loss: 0.1041\n",
      "Epoch 1476/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01476: val_loss did not improve from 0.10411\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1043 - val_loss: 0.1041\n",
      "Epoch 1477/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01477: val_loss did not improve from 0.10411\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1043 - val_loss: 0.1041\n",
      "Epoch 1478/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01478: val_loss improved from 0.10411 to 0.10409, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1043 - val_loss: 0.1041\n",
      "Epoch 1479/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01479: val_loss improved from 0.10409 to 0.10409, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1043 - val_loss: 0.1041\n",
      "Epoch 1480/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01480: val_loss improved from 0.10409 to 0.10408, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1043 - val_loss: 0.1041\n",
      "Epoch 1481/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01481: val_loss improved from 0.10408 to 0.10408, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1043 - val_loss: 0.1041\n",
      "Epoch 1482/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01482: val_loss improved from 0.10408 to 0.10407, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1043 - val_loss: 0.1041\n",
      "Epoch 1483/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01483: val_loss improved from 0.10407 to 0.10407, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1043 - val_loss: 0.1041\n",
      "Epoch 1484/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01484: val_loss did not improve from 0.10407\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1043 - val_loss: 0.1041\n",
      "Epoch 1485/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01485: val_loss did not improve from 0.10407\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1043 - val_loss: 0.1041\n",
      "Epoch 1486/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01486: val_loss improved from 0.10407 to 0.10405, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1043 - val_loss: 0.1041\n",
      "Epoch 1487/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01487: val_loss improved from 0.10405 to 0.10405, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1043 - val_loss: 0.1040\n",
      "Epoch 1488/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01488: val_loss did not improve from 0.10405\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1043 - val_loss: 0.1041\n",
      "Epoch 1489/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01489: val_loss improved from 0.10405 to 0.10404, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1043 - val_loss: 0.1040\n",
      "Epoch 1490/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01490: val_loss improved from 0.10404 to 0.10403, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1043 - val_loss: 0.1040\n",
      "Epoch 1491/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01491: val_loss improved from 0.10403 to 0.10403, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1043 - val_loss: 0.1040\n",
      "Epoch 1492/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01492: val_loss did not improve from 0.10403\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1043 - val_loss: 0.1040\n",
      "Epoch 1493/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1043\n",
      "Epoch 01493: val_loss improved from 0.10403 to 0.10402, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1043 - val_loss: 0.1040\n",
      "Epoch 1494/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01494: val_loss improved from 0.10402 to 0.10402, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1495/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01495: val_loss improved from 0.10402 to 0.10401, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1496/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01496: val_loss improved from 0.10401 to 0.10400, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1497/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01497: val_loss improved from 0.10400 to 0.10400, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1498/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01498: val_loss did not improve from 0.10400\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1499/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01499: val_loss improved from 0.10400 to 0.10398, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1500/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01500: val_loss improved from 0.10398 to 0.10398, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1501/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01501: val_loss improved from 0.10398 to 0.10398, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1502/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01502: val_loss did not improve from 0.10398\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1503/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01503: val_loss improved from 0.10398 to 0.10397, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1504/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01504: val_loss improved from 0.10397 to 0.10396, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1505/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01505: val_loss improved from 0.10396 to 0.10396, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1506/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01506: val_loss did not improve from 0.10396\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1507/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01507: val_loss improved from 0.10396 to 0.10395, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1042 - val_loss: 0.1039\n",
      "Epoch 1508/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01508: val_loss improved from 0.10395 to 0.10394, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1042 - val_loss: 0.1039\n",
      "Epoch 1509/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01509: val_loss did not improve from 0.10394\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1042 - val_loss: 0.1040\n",
      "Epoch 1510/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01510: val_loss improved from 0.10394 to 0.10394, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1042 - val_loss: 0.1039\n",
      "Epoch 1511/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01511: val_loss improved from 0.10394 to 0.10392, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1042 - val_loss: 0.1039\n",
      "Epoch 1512/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1042\n",
      "Epoch 01512: val_loss improved from 0.10392 to 0.10392, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1042 - val_loss: 0.1039\n",
      "Epoch 1513/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01513: val_loss did not improve from 0.10392\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1514/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041- ETA: 0s - loss: 0.1\n",
      "Epoch 01514: val_loss improved from 0.10392 to 0.10392, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1515/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01515: val_loss improved from 0.10392 to 0.10391, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1516/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01516: val_loss improved from 0.10391 to 0.10390, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1517/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01517: val_loss improved from 0.10390 to 0.10390, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1518/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01518: val_loss improved from 0.10390 to 0.10390, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1519/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01519: val_loss improved from 0.10390 to 0.10389, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1041 - val_loss: 0.1039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1520/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01520: val_loss improved from 0.10389 to 0.10389, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1521/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01521: val_loss improved from 0.10389 to 0.10388, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1522/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01522: val_loss improved from 0.10388 to 0.10388, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1523/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01523: val_loss did not improve from 0.10388\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1524/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01524: val_loss improved from 0.10388 to 0.10386, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1525/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01525: val_loss improved from 0.10386 to 0.10386, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1526/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01526: val_loss did not improve from 0.10386\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1527/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01527: val_loss did not improve from 0.10386\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1041 - val_loss: 0.1039\n",
      "Epoch 1528/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01528: val_loss improved from 0.10386 to 0.10384, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1041 - val_loss: 0.1038\n",
      "Epoch 1529/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01529: val_loss improved from 0.10384 to 0.10384, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1041 - val_loss: 0.1038\n",
      "Epoch 1530/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01530: val_loss improved from 0.10384 to 0.10383, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1041 - val_loss: 0.1038\n",
      "Epoch 1531/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1041\n",
      "Epoch 01531: val_loss improved from 0.10383 to 0.10383, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1041 - val_loss: 0.1038\n",
      "Epoch 1532/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01532: val_loss did not improve from 0.10383\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1533/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01533: val_loss improved from 0.10383 to 0.10382, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1534/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01534: val_loss improved from 0.10382 to 0.10382, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1535/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01535: val_loss improved from 0.10382 to 0.10381, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1536/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01536: val_loss improved from 0.10381 to 0.10381, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1537/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01537: val_loss improved from 0.10381 to 0.10380, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1538/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01538: val_loss improved from 0.10380 to 0.10380, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1539/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01539: val_loss improved from 0.10380 to 0.10379, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1540/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01540: val_loss improved from 0.10379 to 0.10379, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1541/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01541: val_loss improved from 0.10379 to 0.10378, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1542/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01542: val_loss improved from 0.10378 to 0.10378, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1543/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01543: val_loss improved from 0.10378 to 0.10377, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1544/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01544: val_loss improved from 0.10377 to 0.10377, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1545/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01545: val_loss improved from 0.10377 to 0.10377, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1546/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01546: val_loss improved from 0.10377 to 0.10376, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1547/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01547: val_loss improved from 0.10376 to 0.10375, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1040 - val_loss: 0.1038\n",
      "Epoch 1548/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01548: val_loss improved from 0.10375 to 0.10374, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1040 - val_loss: 0.1037\n",
      "Epoch 1549/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01549: val_loss did not improve from 0.10374\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1040 - val_loss: 0.1037\n",
      "Epoch 1550/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01550: val_loss did not improve from 0.10374\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1040 - val_loss: 0.1037\n",
      "Epoch 1551/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1040\n",
      "Epoch 01551: val_loss improved from 0.10374 to 0.10373, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1040 - val_loss: 0.1037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1552/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039- ETA: 0s - loss: 0\n",
      "Epoch 01552: val_loss did not improve from 0.10373\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1553/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01553: val_loss improved from 0.10373 to 0.10372, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1554/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01554: val_loss did not improve from 0.10372\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1555/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01555: val_loss improved from 0.10372 to 0.10371, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1556/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01556: val_loss improved from 0.10371 to 0.10370, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1557/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01557: val_loss improved from 0.10370 to 0.10370, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1558/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01558: val_loss improved from 0.10370 to 0.10370, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1559/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01559: val_loss did not improve from 0.10370\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1560/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01560: val_loss improved from 0.10370 to 0.10368, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1561/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01561: val_loss improved from 0.10368 to 0.10368, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1562/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01562: val_loss improved from 0.10368 to 0.10368, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1563/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01563: val_loss did not improve from 0.10368\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1564/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01564: val_loss improved from 0.10368 to 0.10367, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1565/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01565: val_loss did not improve from 0.10367\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1566/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01566: val_loss did not improve from 0.10367\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1567/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01567: val_loss improved from 0.10367 to 0.10366, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1039 - val_loss: 0.1037\n",
      "Epoch 1568/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01568: val_loss improved from 0.10366 to 0.10365, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1039 - val_loss: 0.1036\n",
      "Epoch 1569/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01569: val_loss improved from 0.10365 to 0.10364, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1039 - val_loss: 0.1036\n",
      "Epoch 1570/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01570: val_loss did not improve from 0.10364\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1039 - val_loss: 0.1036\n",
      "Epoch 1571/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1039\n",
      "Epoch 01571: val_loss improved from 0.10364 to 0.10364, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1039 - val_loss: 0.1036\n",
      "Epoch 1572/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01572: val_loss improved from 0.10364 to 0.10363, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1573/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01573: val_loss improved from 0.10363 to 0.10362, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1574/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01574: val_loss did not improve from 0.10362\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1575/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01575: val_loss improved from 0.10362 to 0.10361, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1576/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01576: val_loss did not improve from 0.10361\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1577/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01577: val_loss improved from 0.10361 to 0.10361, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1578/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01578: val_loss improved from 0.10361 to 0.10360, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1579/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01579: val_loss improved from 0.10360 to 0.10359, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1580/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01580: val_loss did not improve from 0.10359\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1581/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01581: val_loss improved from 0.10359 to 0.10358, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1582/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01582: val_loss did not improve from 0.10358\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1583/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01583: val_loss improved from 0.10358 to 0.10358, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1584/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01584: val_loss improved from 0.10358 to 0.10357, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1038 - val_loss: 0.1036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1585/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01585: val_loss did not improve from 0.10357\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1586/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01586: val_loss did not improve from 0.10357\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1587/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01587: val_loss improved from 0.10357 to 0.10356, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1038 - val_loss: 0.1036\n",
      "Epoch 1588/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01588: val_loss improved from 0.10356 to 0.10355, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1038 - val_loss: 0.1035\n",
      "Epoch 1589/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01589: val_loss improved from 0.10355 to 0.10354, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1038 - val_loss: 0.1035\n",
      "Epoch 1590/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01590: val_loss did not improve from 0.10354\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1038 - val_loss: 0.1035\n",
      "Epoch 1591/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1038\n",
      "Epoch 01591: val_loss improved from 0.10354 to 0.10353, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1038 - val_loss: 0.1035\n",
      "Epoch 1592/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01592: val_loss improved from 0.10353 to 0.10353, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1593/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01593: val_loss improved from 0.10353 to 0.10353, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1594/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01594: val_loss did not improve from 0.10353\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1595/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01595: val_loss improved from 0.10353 to 0.10351, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1596/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01596: val_loss improved from 0.10351 to 0.10351, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1597/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01597: val_loss did not improve from 0.10351\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1598/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01598: val_loss improved from 0.10351 to 0.10351, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1599/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01599: val_loss improved from 0.10351 to 0.10350, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1600/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01600: val_loss improved from 0.10350 to 0.10349, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1601/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01601: val_loss did not improve from 0.10349\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1602/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01602: val_loss improved from 0.10349 to 0.10349, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1603/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01603: val_loss improved from 0.10349 to 0.10348, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1604/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01604: val_loss improved from 0.10348 to 0.10348, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1605/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01605: val_loss improved from 0.10348 to 0.10347, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1606/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01606: val_loss improved from 0.10347 to 0.10346, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1607/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01607: val_loss improved from 0.10346 to 0.10346, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1608/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01608: val_loss improved from 0.10346 to 0.10345, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1609/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01609: val_loss improved from 0.10345 to 0.10345, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1037 - val_loss: 0.1034\n",
      "Epoch 1610/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037- ETA: 0s - loss: 0.10\n",
      "Epoch 01610: val_loss improved from 0.10345 to 0.10345, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1037 - val_loss: 0.1034\n",
      "Epoch 1611/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1037\n",
      "Epoch 01611: val_loss did not improve from 0.10345\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1037 - val_loss: 0.1035\n",
      "Epoch 1612/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01612: val_loss improved from 0.10345 to 0.10344, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1613/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01613: val_loss improved from 0.10344 to 0.10343, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1614/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01614: val_loss did not improve from 0.10343\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1615/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01615: val_loss improved from 0.10343 to 0.10343, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1616/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01616: val_loss improved from 0.10343 to 0.10341, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.1036 - val_loss: 0.1034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1617/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01617: val_loss did not improve from 0.10341\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1618/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01618: val_loss did not improve from 0.10341\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1619/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01619: val_loss improved from 0.10341 to 0.10340, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1620/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01620: val_loss did not improve from 0.10340\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1621/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01621: val_loss improved from 0.10340 to 0.10339, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1622/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01622: val_loss improved from 0.10339 to 0.10339, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1623/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01623: val_loss improved from 0.10339 to 0.10338, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1624/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01624: val_loss improved from 0.10338 to 0.10338, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1625/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01625: val_loss improved from 0.10338 to 0.10338, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1626/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01626: val_loss improved from 0.10338 to 0.10337, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1627/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01627: val_loss did not improve from 0.10337\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1628/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01628: val_loss improved from 0.10337 to 0.10336, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1629/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01629: val_loss improved from 0.10336 to 0.10335, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1630/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01630: val_loss did not improve from 0.10335\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1036 - val_loss: 0.1034\n",
      "Epoch 1631/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01631: val_loss improved from 0.10335 to 0.10335, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1036 - val_loss: 0.1033\n",
      "Epoch 1632/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1036\n",
      "Epoch 01632: val_loss improved from 0.10335 to 0.10334, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 109ms/step - loss: 0.1036 - val_loss: 0.1033\n",
      "Epoch 1633/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01633: val_loss did not improve from 0.10334\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 0.1035 - val_loss: 0.1034\n",
      "Epoch 1634/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01634: val_loss improved from 0.10334 to 0.10333, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1635/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01635: val_loss improved from 0.10333 to 0.10333, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1636/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01636: val_loss improved from 0.10333 to 0.10333, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1637/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01637: val_loss improved from 0.10333 to 0.10333, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1638/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01638: val_loss improved from 0.10333 to 0.10331, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1639/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01639: val_loss did not improve from 0.10331\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1640/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01640: val_loss did not improve from 0.10331\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1641/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01641: val_loss improved from 0.10331 to 0.10330, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1642/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01642: val_loss improved from 0.10330 to 0.10329, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1643/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01643: val_loss improved from 0.10329 to 0.10329, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1644/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01644: val_loss improved from 0.10329 to 0.10329, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1645/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01645: val_loss improved from 0.10329 to 0.10328, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1646/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01646: val_loss improved from 0.10328 to 0.10328, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1647/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01647: val_loss improved from 0.10328 to 0.10328, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1648/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01648: val_loss improved from 0.10328 to 0.10327, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1035 - val_loss: 0.1033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1649/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01649: val_loss improved from 0.10327 to 0.10327, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1650/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01650: val_loss improved from 0.10327 to 0.10326, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1651/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01651: val_loss improved from 0.10326 to 0.10325, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1652/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01652: val_loss did not improve from 0.10325\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1653/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1035\n",
      "Epoch 01653: val_loss did not improve from 0.10325\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1035 - val_loss: 0.1033\n",
      "Epoch 1654/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034- ETA: 0s - loss: 0.103\n",
      "Epoch 01654: val_loss improved from 0.10325 to 0.10324, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1655/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01655: val_loss improved from 0.10324 to 0.10323, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1656/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01656: val_loss did not improve from 0.10323\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1657/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01657: val_loss did not improve from 0.10323\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1658/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01658: val_loss improved from 0.10323 to 0.10322, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1659/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01659: val_loss improved from 0.10322 to 0.10322, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1660/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01660: val_loss did not improve from 0.10322\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1661/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01661: val_loss improved from 0.10322 to 0.10321, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1662/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01662: val_loss improved from 0.10321 to 0.10321, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1663/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01663: val_loss did not improve from 0.10321\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1664/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01664: val_loss improved from 0.10321 to 0.10320, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1665/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01665: val_loss improved from 0.10320 to 0.10319, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1666/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01666: val_loss improved from 0.10319 to 0.10319, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1667/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01667: val_loss improved from 0.10319 to 0.10318, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1668/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01668: val_loss improved from 0.10318 to 0.10318, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1669/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01669: val_loss improved from 0.10318 to 0.10317, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1670/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01670: val_loss improved from 0.10317 to 0.10317, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1671/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01671: val_loss improved from 0.10317 to 0.10317, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1672/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01672: val_loss improved from 0.10317 to 0.10316, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1673/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01673: val_loss did not improve from 0.10316\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1674/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1034\n",
      "Epoch 01674: val_loss did not improve from 0.10316\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1034 - val_loss: 0.1032\n",
      "Epoch 1675/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01675: val_loss improved from 0.10316 to 0.10314, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1676/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01676: val_loss improved from 0.10314 to 0.10314, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1677/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01677: val_loss did not improve from 0.10314\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1033 - val_loss: 0.1032\n",
      "Epoch 1678/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01678: val_loss improved from 0.10314 to 0.10313, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1679/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01679: val_loss improved from 0.10313 to 0.10313, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1680/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01680: val_loss improved from 0.10313 to 0.10312, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1033 - val_loss: 0.1031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1681/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01681: val_loss did not improve from 0.10312\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1682/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01682: val_loss improved from 0.10312 to 0.10312, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1683/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01683: val_loss improved from 0.10312 to 0.10311, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1684/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01684: val_loss improved from 0.10311 to 0.10310, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1685/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01685: val_loss improved from 0.10310 to 0.10310, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1686/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01686: val_loss improved from 0.10310 to 0.10309, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1687/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01687: val_loss did not improve from 0.10309\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1688/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01688: val_loss improved from 0.10309 to 0.10309, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1689/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01689: val_loss improved from 0.10309 to 0.10308, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1690/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01690: val_loss improved from 0.10308 to 0.10308, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1691/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01691: val_loss improved from 0.10308 to 0.10308, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1692/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01692: val_loss improved from 0.10308 to 0.10307, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1693/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01693: val_loss did not improve from 0.10307\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1694/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01694: val_loss did not improve from 0.10307\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1695/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01695: val_loss improved from 0.10307 to 0.10306, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1696/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01696: val_loss improved from 0.10306 to 0.10304, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1033 - val_loss: 0.1030\n",
      "Epoch 1697/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1033\n",
      "Epoch 01697: val_loss did not improve from 0.10304\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1033 - val_loss: 0.1031\n",
      "Epoch 1698/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01698: val_loss improved from 0.10304 to 0.10304, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1699/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01699: val_loss improved from 0.10304 to 0.10304, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1700/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01700: val_loss improved from 0.10304 to 0.10304, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1701/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01701: val_loss did not improve from 0.10304\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1702/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01702: val_loss improved from 0.10304 to 0.10303, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1703/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01703: val_loss improved from 0.10303 to 0.10302, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1704/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01704: val_loss did not improve from 0.10302\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1705/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01705: val_loss did not improve from 0.10302\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1706/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01706: val_loss improved from 0.10302 to 0.10301, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1707/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01707: val_loss improved from 0.10301 to 0.10300, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1708/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01708: val_loss did not improve from 0.10300\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1709/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01709: val_loss improved from 0.10300 to 0.10300, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1710/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01710: val_loss improved from 0.10300 to 0.10299, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 109ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1711/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01711: val_loss improved from 0.10299 to 0.10298, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 120ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1712/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01712: val_loss improved from 0.10298 to 0.10297, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.1032 - val_loss: 0.1030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1713/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01713: val_loss did not improve from 0.10297\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1714/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01714: val_loss did not improve from 0.10297\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1715/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01715: val_loss improved from 0.10297 to 0.10296, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1716/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01716: val_loss improved from 0.10296 to 0.10296, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1717/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01717: val_loss did not improve from 0.10296\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1718/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1032\n",
      "Epoch 01718: val_loss improved from 0.10296 to 0.10296, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 112ms/step - loss: 0.1032 - val_loss: 0.1030\n",
      "Epoch 1719/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01719: val_loss improved from 0.10296 to 0.10295, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 109ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1720/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01720: val_loss improved from 0.10295 to 0.10294, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 112ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1721/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01721: val_loss improved from 0.10294 to 0.10294, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 112ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1722/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01722: val_loss improved from 0.10294 to 0.10293, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1723/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01723: val_loss improved from 0.10293 to 0.10293, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 109ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1724/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01724: val_loss improved from 0.10293 to 0.10293, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1725/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01725: val_loss improved from 0.10293 to 0.10292, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1726/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01726: val_loss improved from 0.10292 to 0.10292, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1727/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01727: val_loss did not improve from 0.10292\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1728/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01728: val_loss improved from 0.10292 to 0.10291, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1729/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01729: val_loss improved from 0.10291 to 0.10290, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1730/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01730: val_loss did not improve from 0.10290\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1731/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01731: val_loss did not improve from 0.10290\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1732/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01732: val_loss improved from 0.10290 to 0.10290, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1733/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01733: val_loss improved from 0.10290 to 0.10289, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1734/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01734: val_loss improved from 0.10289 to 0.10288, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1735/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01735: val_loss improved from 0.10288 to 0.10288, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1736/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01736: val_loss improved from 0.10288 to 0.10287, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1737/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01737: val_loss improved from 0.10287 to 0.10287, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1738/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01738: val_loss improved from 0.10287 to 0.10286, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1739/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01739: val_loss improved from 0.10286 to 0.10286, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1740/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1031\n",
      "Epoch 01740: val_loss improved from 0.10286 to 0.10285, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1031 - val_loss: 0.1029\n",
      "Epoch 1741/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01741: val_loss improved from 0.10285 to 0.10285, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 0.1030 - val_loss: 0.1029\n",
      "Epoch 1742/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01742: val_loss improved from 0.10285 to 0.10284, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1743/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01743: val_loss improved from 0.10284 to 0.10284, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1744/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01744: val_loss improved from 0.10284 to 0.10283, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1030 - val_loss: 0.1028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1745/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01745: val_loss improved from 0.10283 to 0.10283, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1746/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01746: val_loss did not improve from 0.10283\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1747/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01747: val_loss did not improve from 0.10283\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1748/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01748: val_loss improved from 0.10283 to 0.10282, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1749/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01749: val_loss did not improve from 0.10282\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1750/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01750: val_loss did not improve from 0.10282\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1751/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01751: val_loss improved from 0.10282 to 0.10281, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1752/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01752: val_loss improved from 0.10281 to 0.10280, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1753/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01753: val_loss improved from 0.10280 to 0.10280, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1754/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01754: val_loss improved from 0.10280 to 0.10279, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1755/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01755: val_loss improved from 0.10279 to 0.10279, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1756/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01756: val_loss improved from 0.10279 to 0.10279, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1757/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01757: val_loss improved from 0.10279 to 0.10278, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1758/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01758: val_loss improved from 0.10278 to 0.10277, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1759/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01759: val_loss improved from 0.10277 to 0.10277, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1760/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01760: val_loss did not improve from 0.10277\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1761/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01761: val_loss improved from 0.10277 to 0.10276, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1762/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01762: val_loss improved from 0.10276 to 0.10275, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1763/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1030\n",
      "Epoch 01763: val_loss did not improve from 0.10275\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1030 - val_loss: 0.1028\n",
      "Epoch 1764/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01764: val_loss improved from 0.10275 to 0.10275, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1765/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01765: val_loss did not improve from 0.10275\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1766/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01766: val_loss did not improve from 0.10275\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1767/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01767: val_loss improved from 0.10275 to 0.10273, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1768/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029- ETA: 0s - loss: 0.1\n",
      "Epoch 01768: val_loss improved from 0.10273 to 0.10273, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1769/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01769: val_loss improved from 0.10273 to 0.10272, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1770/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01770: val_loss improved from 0.10272 to 0.10272, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1771/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01771: val_loss improved from 0.10272 to 0.10272, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1772/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01772: val_loss improved from 0.10272 to 0.10271, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1773/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01773: val_loss improved from 0.10271 to 0.10271, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1774/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01774: val_loss did not improve from 0.10271\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1775/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01775: val_loss improved from 0.10271 to 0.10270, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1776/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01776: val_loss improved from 0.10270 to 0.10269, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1029 - val_loss: 0.1027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1777/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01777: val_loss improved from 0.10269 to 0.10269, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1778/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01778: val_loss improved from 0.10269 to 0.10269, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1779/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01779: val_loss improved from 0.10269 to 0.10268, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1780/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01780: val_loss did not improve from 0.10268\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1781/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01781: val_loss improved from 0.10268 to 0.10268, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1782/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01782: val_loss did not improve from 0.10268\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1783/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01783: val_loss improved from 0.10268 to 0.10267, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1784/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01784: val_loss improved from 0.10267 to 0.10267, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1785/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01785: val_loss improved from 0.10267 to 0.10265, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1786/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1029\n",
      "Epoch 01786: val_loss did not improve from 0.10265\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1029 - val_loss: 0.1027\n",
      "Epoch 1787/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01787: val_loss improved from 0.10265 to 0.10265, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1788/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01788: val_loss improved from 0.10265 to 0.10264, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1789/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01789: val_loss did not improve from 0.10264\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1028 - val_loss: 0.1027\n",
      "Epoch 1790/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01790: val_loss improved from 0.10264 to 0.10264, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1791/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01791: val_loss improved from 0.10264 to 0.10263, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1792/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01792: val_loss did not improve from 0.10263\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1793/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01793: val_loss did not improve from 0.10263\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1794/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01794: val_loss improved from 0.10263 to 0.10261, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1795/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01795: val_loss did not improve from 0.10261\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1796/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01796: val_loss did not improve from 0.10261\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1797/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01797: val_loss improved from 0.10261 to 0.10260, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1798/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01798: val_loss improved from 0.10260 to 0.10260, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1799/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01799: val_loss did not improve from 0.10260\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1800/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01800: val_loss improved from 0.10260 to 0.10259, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1801/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01801: val_loss improved from 0.10259 to 0.10259, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1802/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028- ETA: 0s - loss: 0.\n",
      "Epoch 01802: val_loss did not improve from 0.10259\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1803/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01803: val_loss improved from 0.10259 to 0.10258, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1804/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01804: val_loss improved from 0.10258 to 0.10257, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1805/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01805: val_loss did not improve from 0.10257\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1806/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01806: val_loss did not improve from 0.10257\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1807/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01807: val_loss improved from 0.10257 to 0.10256, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1808/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01808: val_loss improved from 0.10256 to 0.10256, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1809/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01809: val_loss did not improve from 0.10256\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1028 - val_loss: 0.1026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1810/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1028\n",
      "Epoch 01810: val_loss did not improve from 0.10256\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1028 - val_loss: 0.1026\n",
      "Epoch 1811/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01811: val_loss improved from 0.10256 to 0.10254, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1812/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01812: val_loss did not improve from 0.10254\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1813/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01813: val_loss improved from 0.10254 to 0.10253, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1814/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01814: val_loss improved from 0.10253 to 0.10253, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1815/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01815: val_loss did not improve from 0.10253\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1816/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01816: val_loss improved from 0.10253 to 0.10253, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1817/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01817: val_loss improved from 0.10253 to 0.10252, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1818/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01818: val_loss did not improve from 0.10252\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1819/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01819: val_loss improved from 0.10252 to 0.10251, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1820/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01820: val_loss improved from 0.10251 to 0.10250, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1821/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01821: val_loss did not improve from 0.10250\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1822/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01822: val_loss improved from 0.10250 to 0.10250, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1823/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01823: val_loss improved from 0.10250 to 0.10249, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1824/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01824: val_loss did not improve from 0.10249\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1825/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01825: val_loss did not improve from 0.10249\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1826/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01826: val_loss improved from 0.10249 to 0.10248, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1827/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01827: val_loss improved from 0.10248 to 0.10247, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1828/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01828: val_loss did not improve from 0.10247\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1829/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01829: val_loss improved from 0.10247 to 0.10247, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1830/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01830: val_loss improved from 0.10247 to 0.10246, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 110ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1831/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1027\n",
      "Epoch 01831: val_loss improved from 0.10246 to 0.10246, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1027 - val_loss: 0.1025\n",
      "Epoch 1832/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01832: val_loss did not improve from 0.10246\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1026 - val_loss: 0.1025\n",
      "Epoch 1833/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01833: val_loss improved from 0.10246 to 0.10245, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1834/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01834: val_loss improved from 0.10245 to 0.10244, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1835/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01835: val_loss did not improve from 0.10244\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1026 - val_loss: 0.1025\n",
      "Epoch 1836/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01836: val_loss improved from 0.10244 to 0.10244, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1837/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01837: val_loss did not improve from 0.10244\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1838/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01838: val_loss improved from 0.10244 to 0.10244, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1839/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01839: val_loss did not improve from 0.10244\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1840/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01840: val_loss improved from 0.10244 to 0.10241, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1841/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01841: val_loss improved from 0.10241 to 0.10241, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1842/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01842: val_loss did not improve from 0.10241\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1026 - val_loss: 0.1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1843/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01843: val_loss did not improve from 0.10241\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1844/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01844: val_loss improved from 0.10241 to 0.10241, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1845/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01845: val_loss improved from 0.10241 to 0.10240, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 111ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1846/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01846: val_loss improved from 0.10240 to 0.10239, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1847/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01847: val_loss improved from 0.10239 to 0.10239, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1848/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01848: val_loss did not improve from 0.10239\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1849/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01849: val_loss improved from 0.10239 to 0.10238, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1850/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01850: val_loss improved from 0.10238 to 0.10238, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1851/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01851: val_loss improved from 0.10238 to 0.10237, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1852/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01852: val_loss improved from 0.10237 to 0.10237, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1853/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01853: val_loss improved from 0.10237 to 0.10236, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1854/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01854: val_loss did not improve from 0.10236\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1855/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1026\n",
      "Epoch 01855: val_loss improved from 0.10236 to 0.10235, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1026 - val_loss: 0.1024\n",
      "Epoch 1856/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01856: val_loss improved from 0.10235 to 0.10235, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1025 - val_loss: 0.1024\n",
      "Epoch 1857/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01857: val_loss did not improve from 0.10235\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1025 - val_loss: 0.1024\n",
      "Epoch 1858/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01858: val_loss did not improve from 0.10235\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1025 - val_loss: 0.1024\n",
      "Epoch 1859/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01859: val_loss improved from 0.10235 to 0.10234, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1860/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01860: val_loss improved from 0.10234 to 0.10233, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1861/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01861: val_loss did not improve from 0.10233\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1025 - val_loss: 0.1024\n",
      "Epoch 1862/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01862: val_loss improved from 0.10233 to 0.10232, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1863/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01863: val_loss did not improve from 0.10232\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1864/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01864: val_loss did not improve from 0.10232\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1865/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01865: val_loss improved from 0.10232 to 0.10231, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1866/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01866: val_loss improved from 0.10231 to 0.10231, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1867/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01867: val_loss improved from 0.10231 to 0.10231, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1868/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01868: val_loss improved from 0.10231 to 0.10230, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 110ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1869/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01869: val_loss did not improve from 0.10230\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1870/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01870: val_loss did not improve from 0.10230\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1871/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01871: val_loss improved from 0.10230 to 0.10229, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1872/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01872: val_loss improved from 0.10229 to 0.10228, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1873/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01873: val_loss did not improve from 0.10228\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1874/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01874: val_loss improved from 0.10228 to 0.10228, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1875/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01875: val_loss did not improve from 0.10228\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1876/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01876: val_loss improved from 0.10228 to 0.10227, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1877/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01877: val_loss improved from 0.10227 to 0.10227, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1878/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01878: val_loss improved from 0.10227 to 0.10226, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1879/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1025\n",
      "Epoch 01879: val_loss improved from 0.10226 to 0.10226, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1025 - val_loss: 0.1023\n",
      "Epoch 1880/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01880: val_loss improved from 0.10226 to 0.10225, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1024 - val_loss: 0.1023\n",
      "Epoch 1881/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01881: val_loss improved from 0.10225 to 0.10225, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1882/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01882: val_loss improved from 0.10225 to 0.10225, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1883/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01883: val_loss improved from 0.10225 to 0.10224, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1884/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01884: val_loss improved from 0.10224 to 0.10224, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1885/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01885: val_loss improved from 0.10224 to 0.10223, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1886/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01886: val_loss did not improve from 0.10223\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1887/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01887: val_loss improved from 0.10223 to 0.10222, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1888/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01888: val_loss improved from 0.10222 to 0.10222, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1889/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01889: val_loss improved from 0.10222 to 0.10222, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1890/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01890: val_loss improved from 0.10222 to 0.10222, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1891/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01891: val_loss improved from 0.10222 to 0.10221, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1892/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01892: val_loss improved from 0.10221 to 0.10220, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1893/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01893: val_loss improved from 0.10220 to 0.10220, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 114ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1894/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01894: val_loss did not improve from 0.10220\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1895/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01895: val_loss improved from 0.10220 to 0.10219, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 116ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1896/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01896: val_loss did not improve from 0.10219\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1897/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01897: val_loss improved from 0.10219 to 0.10218, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1898/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01898: val_loss improved from 0.10218 to 0.10218, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1899/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01899: val_loss improved from 0.10218 to 0.10217, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1900/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01900: val_loss improved from 0.10217 to 0.10217, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1901/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01901: val_loss improved from 0.10217 to 0.10217, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1902/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01902: val_loss did not improve from 0.10217\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1903/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01903: val_loss did not improve from 0.10217\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1904/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1024\n",
      "Epoch 01904: val_loss improved from 0.10217 to 0.10215, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1024 - val_loss: 0.1022\n",
      "Epoch 1905/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01905: val_loss improved from 0.10215 to 0.10215, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1023 - val_loss: 0.1022\n",
      "Epoch 1906/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01906: val_loss improved from 0.10215 to 0.10215, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1907/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01907: val_loss improved from 0.10215 to 0.10214, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1908/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01908: val_loss improved from 0.10214 to 0.10214, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1909/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01909: val_loss improved from 0.10214 to 0.10213, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1910/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01910: val_loss did not improve from 0.10213\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1911/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01911: val_loss improved from 0.10213 to 0.10213, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1912/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01912: val_loss improved from 0.10213 to 0.10212, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1913/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01913: val_loss did not improve from 0.10212\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1914/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01914: val_loss improved from 0.10212 to 0.10212, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1915/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01915: val_loss improved from 0.10212 to 0.10212, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1916/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01916: val_loss improved from 0.10212 to 0.10211, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1917/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01917: val_loss improved from 0.10211 to 0.10210, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1918/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01918: val_loss improved from 0.10210 to 0.10210, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1919/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01919: val_loss improved from 0.10210 to 0.10210, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1920/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01920: val_loss improved from 0.10210 to 0.10209, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1921/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01921: val_loss improved from 0.10209 to 0.10209, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1922/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01922: val_loss improved from 0.10209 to 0.10208, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1923/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01923: val_loss improved from 0.10208 to 0.10208, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1924/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01924: val_loss improved from 0.10208 to 0.10208, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1925/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01925: val_loss improved from 0.10208 to 0.10207, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1926/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01926: val_loss did not improve from 0.10207\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1927/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1023\n",
      "Epoch 01927: val_loss improved from 0.10207 to 0.10206, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1023 - val_loss: 0.1021\n",
      "Epoch 1928/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01928: val_loss improved from 0.10206 to 0.10206, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1022 - val_loss: 0.1021\n",
      "Epoch 1929/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01929: val_loss did not improve from 0.10206\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1022 - val_loss: 0.1021\n",
      "Epoch 1930/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01930: val_loss improved from 0.10206 to 0.10205, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1022 - val_loss: 0.1021\n",
      "Epoch 1931/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01931: val_loss improved from 0.10205 to 0.10204, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1932/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01932: val_loss improved from 0.10204 to 0.10204, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1933/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01933: val_loss improved from 0.10204 to 0.10203, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1934/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01934: val_loss did not improve from 0.10203\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1935/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01935: val_loss improved from 0.10203 to 0.10203, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1936/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01936: val_loss improved from 0.10203 to 0.10202, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1937/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01937: val_loss did not improve from 0.10202\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1938/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01938: val_loss improved from 0.10202 to 0.10201, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1939/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01939: val_loss did not improve from 0.10201\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1940/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01940: val_loss improved from 0.10201 to 0.10200, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1941/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01941: val_loss did not improve from 0.10200\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1942/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01942: val_loss improved from 0.10200 to 0.10200, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1943/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01943: val_loss improved from 0.10200 to 0.10199, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1944/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01944: val_loss did not improve from 0.10199\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1945/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01945: val_loss improved from 0.10199 to 0.10198, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1946/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01946: val_loss improved from 0.10198 to 0.10198, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1947/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01947: val_loss did not improve from 0.10198\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1948/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01948: val_loss improved from 0.10198 to 0.10197, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1949/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01949: val_loss improved from 0.10197 to 0.10196, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1950/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1022\n",
      "Epoch 01950: val_loss did not improve from 0.10196\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1022 - val_loss: 0.1020\n",
      "Epoch 1951/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01951: val_loss improved from 0.10196 to 0.10195, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1021 - val_loss: 0.1020\n",
      "Epoch 1952/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01952: val_loss did not improve from 0.10195\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1021 - val_loss: 0.1020\n",
      "Epoch 1953/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01953: val_loss did not improve from 0.10195\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1021 - val_loss: 0.1020\n",
      "Epoch 1954/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01954: val_loss did not improve from 0.10195\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1021 - val_loss: 0.1020\n",
      "Epoch 1955/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01955: val_loss improved from 0.10195 to 0.10194, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1956/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01956: val_loss improved from 0.10194 to 0.10194, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1957/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01957: val_loss improved from 0.10194 to 0.10193, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1958/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01958: val_loss did not improve from 0.10193\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1959/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01959: val_loss did not improve from 0.10193\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1960/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01960: val_loss improved from 0.10193 to 0.10192, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1961/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01961: val_loss improved from 0.10192 to 0.10192, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1962/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01962: val_loss did not improve from 0.10192\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1963/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01963: val_loss improved from 0.10192 to 0.10191, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1964/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01964: val_loss improved from 0.10191 to 0.10190, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1965/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01965: val_loss did not improve from 0.10190\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1966/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01966: val_loss improved from 0.10190 to 0.10189, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1967/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01967: val_loss did not improve from 0.10189\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1968/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01968: val_loss improved from 0.10189 to 0.10189, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1969/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01969: val_loss improved from 0.10189 to 0.10188, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1970/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01970: val_loss did not improve from 0.10188\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1971/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01971: val_loss improved from 0.10188 to 0.10187, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1972/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01972: val_loss did not improve from 0.10187\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1973/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021- ETA: 0s - loss: 0.102\n",
      "Epoch 01973: val_loss improved from 0.10187 to 0.10187, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1974/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1021\n",
      "Epoch 01974: val_loss improved from 0.10187 to 0.10186, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1021 - val_loss: 0.1019\n",
      "Epoch 1975/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01975: val_loss did not improve from 0.10186\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1020 - val_loss: 0.1019\n",
      "Epoch 1976/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01976: val_loss improved from 0.10186 to 0.10185, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1020 - val_loss: 0.1019\n",
      "Epoch 1977/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01977: val_loss improved from 0.10185 to 0.10185, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1978/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01978: val_loss improved from 0.10185 to 0.10184, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1979/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01979: val_loss did not improve from 0.10184\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1980/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01980: val_loss did not improve from 0.10184\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1981/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01981: val_loss improved from 0.10184 to 0.10183, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1982/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01982: val_loss did not improve from 0.10183\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1983/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01983: val_loss did not improve from 0.10183\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1984/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01984: val_loss improved from 0.10183 to 0.10183, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1985/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01985: val_loss improved from 0.10183 to 0.10182, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1986/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01986: val_loss improved from 0.10182 to 0.10181, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1987/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01987: val_loss improved from 0.10181 to 0.10181, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1988/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01988: val_loss did not improve from 0.10181\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1989/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01989: val_loss improved from 0.10181 to 0.10180, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1990/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01990: val_loss improved from 0.10180 to 0.10180, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1991/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01991: val_loss improved from 0.10180 to 0.10179, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1992/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01992: val_loss improved from 0.10179 to 0.10178, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1993/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01993: val_loss improved from 0.10178 to 0.10178, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1994/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01994: val_loss improved from 0.10178 to 0.10178, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1995/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01995: val_loss improved from 0.10178 to 0.10177, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1996/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01996: val_loss did not improve from 0.10177\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1997/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1020\n",
      "Epoch 01997: val_loss did not improve from 0.10177\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1020 - val_loss: 0.1018\n",
      "Epoch 1998/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 01998: val_loss improved from 0.10177 to 0.10176, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1019 - val_loss: 0.1018\n",
      "Epoch 1999/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 01999: val_loss did not improve from 0.10176\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1019 - val_loss: 0.1018\n",
      "Epoch 2000/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02000: val_loss improved from 0.10176 to 0.10176, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1019 - val_loss: 0.1018\n",
      "Epoch 2001/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02001: val_loss improved from 0.10176 to 0.10175, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1019 - val_loss: 0.1018\n",
      "Epoch 2002/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02002: val_loss improved from 0.10175 to 0.10174, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2003/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02003: val_loss did not improve from 0.10174\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2004/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02004: val_loss improved from 0.10174 to 0.10174, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2005/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02005: val_loss improved from 0.10174 to 0.10174, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2006/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02006: val_loss improved from 0.10174 to 0.10173, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2007/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02007: val_loss improved from 0.10173 to 0.10172, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2008/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02008: val_loss did not improve from 0.10172\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2009/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02009: val_loss did not improve from 0.10172\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2010/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02010: val_loss improved from 0.10172 to 0.10171, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2011/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02011: val_loss improved from 0.10171 to 0.10170, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2012/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02012: val_loss did not improve from 0.10170\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2013/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02013: val_loss did not improve from 0.10170\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2014/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02014: val_loss improved from 0.10170 to 0.10169, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2015/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02015: val_loss improved from 0.10169 to 0.10168, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2016/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02016: val_loss improved from 0.10168 to 0.10168, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2017/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02017: val_loss improved from 0.10168 to 0.10168, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2018/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02018: val_loss improved from 0.10168 to 0.10167, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2019/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02019: val_loss did not improve from 0.10167\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2020/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1019\n",
      "Epoch 02020: val_loss improved from 0.10167 to 0.10166, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1019 - val_loss: 0.1017\n",
      "Epoch 2021/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02021: val_loss did not improve from 0.10166\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1018 - val_loss: 0.1017\n",
      "Epoch 2022/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02022: val_loss improved from 0.10166 to 0.10166, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1018 - val_loss: 0.1017\n",
      "Epoch 2023/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02023: val_loss improved from 0.10166 to 0.10165, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2024/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02024: val_loss improved from 0.10165 to 0.10165, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2025/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02025: val_loss improved from 0.10165 to 0.10164, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2026/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02026: val_loss improved from 0.10164 to 0.10164, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2027/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02027: val_loss improved from 0.10164 to 0.10163, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2028/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02028: val_loss did not improve from 0.10163\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2029/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02029: val_loss improved from 0.10163 to 0.10162, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2030/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02030: val_loss improved from 0.10162 to 0.10162, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2031/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02031: val_loss improved from 0.10162 to 0.10162, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2032/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02032: val_loss did not improve from 0.10162\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2033/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02033: val_loss improved from 0.10162 to 0.10161, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2034/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02034: val_loss did not improve from 0.10161\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2035/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02035: val_loss improved from 0.10161 to 0.10160, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2036/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02036: val_loss improved from 0.10160 to 0.10159, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2037/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02037: val_loss did not improve from 0.10159\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2038/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02038: val_loss improved from 0.10159 to 0.10159, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2039/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02039: val_loss improved from 0.10159 to 0.10158, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2040/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02040: val_loss did not improve from 0.10158\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2041/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02041: val_loss improved from 0.10158 to 0.10158, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2042/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02042: val_loss improved from 0.10158 to 0.10157, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2043/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02043: val_loss improved from 0.10157 to 0.10156, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2044/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1018\n",
      "Epoch 02044: val_loss did not improve from 0.10156\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1018 - val_loss: 0.1016\n",
      "Epoch 2045/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02045: val_loss improved from 0.10156 to 0.10156, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1017 - val_loss: 0.1016\n",
      "Epoch 2046/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02046: val_loss improved from 0.10156 to 0.10155, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1017 - val_loss: 0.1016\n",
      "Epoch 2047/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02047: val_loss improved from 0.10155 to 0.10155, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1017 - val_loss: 0.1016\n",
      "Epoch 2048/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02048: val_loss did not improve from 0.10155\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1017 - val_loss: 0.1016\n",
      "Epoch 2049/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02049: val_loss improved from 0.10155 to 0.10154, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2050/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02050: val_loss improved from 0.10154 to 0.10154, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2051/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02051: val_loss improved from 0.10154 to 0.10153, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2052/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02052: val_loss improved from 0.10153 to 0.10153, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2053/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017- ETA: 0s - loss: 0.10\n",
      "Epoch 02053: val_loss improved from 0.10153 to 0.10153, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2054/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02054: val_loss improved from 0.10153 to 0.10152, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2055/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02055: val_loss improved from 0.10152 to 0.10152, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2056/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02056: val_loss did not improve from 0.10152\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2057/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02057: val_loss improved from 0.10152 to 0.10152, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2058/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02058: val_loss improved from 0.10152 to 0.10150, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2059/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02059: val_loss did not improve from 0.10150\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2060/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02060: val_loss improved from 0.10150 to 0.10149, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2061/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02061: val_loss improved from 0.10149 to 0.10149, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2062/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02062: val_loss improved from 0.10149 to 0.10148, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2063/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02063: val_loss improved from 0.10148 to 0.10148, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2064/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02064: val_loss did not improve from 0.10148\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2065/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02065: val_loss improved from 0.10148 to 0.10147, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2066/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02066: val_loss did not improve from 0.10147\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1017 - val_loss: 0.1015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2067/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02067: val_loss improved from 0.10147 to 0.10146, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2068/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1017\n",
      "Epoch 02068: val_loss improved from 0.10146 to 0.10146, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1017 - val_loss: 0.1015\n",
      "Epoch 2069/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02069: val_loss improved from 0.10146 to 0.10145, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1016 - val_loss: 0.1015\n",
      "Epoch 2070/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02070: val_loss improved from 0.10145 to 0.10145, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1016 - val_loss: 0.1015\n",
      "Epoch 2071/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02071: val_loss did not improve from 0.10145\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1016 - val_loss: 0.1015\n",
      "Epoch 2072/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02072: val_loss improved from 0.10145 to 0.10144, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2073/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02073: val_loss improved from 0.10144 to 0.10144, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2074/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02074: val_loss improved from 0.10144 to 0.10143, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2075/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02075: val_loss did not improve from 0.10143\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2076/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02076: val_loss improved from 0.10143 to 0.10143, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2077/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02077: val_loss improved from 0.10143 to 0.10142, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2078/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02078: val_loss improved from 0.10142 to 0.10142, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2079/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02079: val_loss did not improve from 0.10142\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2080/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02080: val_loss improved from 0.10142 to 0.10141, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2081/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02081: val_loss improved from 0.10141 to 0.10141, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2082/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02082: val_loss improved from 0.10141 to 0.10140, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2083/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02083: val_loss improved from 0.10140 to 0.10140, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2084/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02084: val_loss did not improve from 0.10140\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2085/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02085: val_loss improved from 0.10140 to 0.10139, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2086/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02086: val_loss improved from 0.10139 to 0.10139, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2087/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02087: val_loss did not improve from 0.10139\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2088/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02088: val_loss did not improve from 0.10139\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2089/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02089: val_loss improved from 0.10139 to 0.10138, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2090/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02090: val_loss improved from 0.10138 to 0.10136, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2091/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02091: val_loss did not improve from 0.10136\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2092/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016\n",
      "Epoch 02092: val_loss improved from 0.10136 to 0.10136, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2093/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1016- ETA: 0s - loss: 0.10\n",
      "Epoch 02093: val_loss improved from 0.10136 to 0.10136, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1016 - val_loss: 0.1014\n",
      "Epoch 2094/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02094: val_loss improved from 0.10136 to 0.10135, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2095/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02095: val_loss did not improve from 0.10135\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2096/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02096: val_loss did not improve from 0.10135\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1015 - val_loss: 0.1014\n",
      "Epoch 2097/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02097: val_loss improved from 0.10135 to 0.10134, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2098/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02098: val_loss improved from 0.10134 to 0.10133, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1015 - val_loss: 0.1013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2099/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02099: val_loss improved from 0.10133 to 0.10133, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2100/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02100: val_loss improved from 0.10133 to 0.10132, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2101/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02101: val_loss improved from 0.10132 to 0.10132, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2102/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02102: val_loss did not improve from 0.10132\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2103/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02103: val_loss improved from 0.10132 to 0.10131, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2104/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02104: val_loss improved from 0.10131 to 0.10131, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2105/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02105: val_loss improved from 0.10131 to 0.10130, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2106/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02106: val_loss improved from 0.10130 to 0.10130, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2107/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02107: val_loss did not improve from 0.10130\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2108/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02108: val_loss improved from 0.10130 to 0.10130, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2109/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02109: val_loss improved from 0.10130 to 0.10128, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2110/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02110: val_loss did not improve from 0.10128\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2111/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02111: val_loss did not improve from 0.10128\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2112/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02112: val_loss improved from 0.10128 to 0.10128, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2113/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02113: val_loss improved from 0.10128 to 0.10127, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2114/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02114: val_loss improved from 0.10127 to 0.10127, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2115/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02115: val_loss improved from 0.10127 to 0.10127, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2116/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02116: val_loss improved from 0.10127 to 0.10126, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2117/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1015\n",
      "Epoch 02117: val_loss improved from 0.10126 to 0.10126, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1015 - val_loss: 0.1013\n",
      "Epoch 2118/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02118: val_loss improved from 0.10126 to 0.10125, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2119/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02119: val_loss did not improve from 0.10125\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1014 - val_loss: 0.1013\n",
      "Epoch 2120/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02120: val_loss improved from 0.10125 to 0.10124, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2121/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02121: val_loss improved from 0.10124 to 0.10124, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2122/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02122: val_loss improved from 0.10124 to 0.10124, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2123/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02123: val_loss improved from 0.10124 to 0.10123, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2124/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02124: val_loss improved from 0.10123 to 0.10123, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2125/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02125: val_loss improved from 0.10123 to 0.10122, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2126/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02126: val_loss did not improve from 0.10122\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2127/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02127: val_loss improved from 0.10122 to 0.10121, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2128/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02128: val_loss improved from 0.10121 to 0.10121, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2129/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02129: val_loss did not improve from 0.10121\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2130/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02130: val_loss improved from 0.10121 to 0.10120, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1014 - val_loss: 0.1012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2131/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02131: val_loss improved from 0.10120 to 0.10120, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2132/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02132: val_loss improved from 0.10120 to 0.10120, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2133/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02133: val_loss did not improve from 0.10120\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2134/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02134: val_loss improved from 0.10120 to 0.10119, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2135/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02135: val_loss did not improve from 0.10119\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2136/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02136: val_loss improved from 0.10119 to 0.10118, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2137/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02137: val_loss did not improve from 0.10118\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2138/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02138: val_loss improved from 0.10118 to 0.10117, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2139/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02139: val_loss improved from 0.10117 to 0.10117, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2140/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02140: val_loss did not improve from 0.10117\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2141/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02141: val_loss improved from 0.10117 to 0.10116, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2142/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02142: val_loss improved from 0.10116 to 0.10116, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2143/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02143: val_loss did not improve from 0.10116\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1014 - val_loss: 0.1012\n",
      "Epoch 2144/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1014\n",
      "Epoch 02144: val_loss improved from 0.10116 to 0.10115, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1014 - val_loss: 0.1011\n",
      "Epoch 2145/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02145: val_loss improved from 0.10115 to 0.10115, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2146/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02146: val_loss did not improve from 0.10115\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2147/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013- ETA: 0s - loss: 0.09\n",
      "Epoch 02147: val_loss improved from 0.10115 to 0.10114, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2148/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02148: val_loss improved from 0.10114 to 0.10113, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2149/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02149: val_loss improved from 0.10113 to 0.10113, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2150/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02150: val_loss improved from 0.10113 to 0.10113, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2151/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02151: val_loss improved from 0.10113 to 0.10112, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2152/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02152: val_loss improved from 0.10112 to 0.10112, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2153/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02153: val_loss improved from 0.10112 to 0.10111, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2154/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02154: val_loss did not improve from 0.10111\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2155/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02155: val_loss improved from 0.10111 to 0.10111, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2156/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02156: val_loss improved from 0.10111 to 0.10110, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2157/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02157: val_loss improved from 0.10110 to 0.10110, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2158/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02158: val_loss improved from 0.10110 to 0.10109, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2159/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02159: val_loss did not improve from 0.10109\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2160/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02160: val_loss improved from 0.10109 to 0.10109, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2161/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02161: val_loss improved from 0.10109 to 0.10108, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2162/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02162: val_loss did not improve from 0.10108\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1013 - val_loss: 0.1011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2163/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02163: val_loss improved from 0.10108 to 0.10107, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2164/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02164: val_loss did not improve from 0.10107\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2165/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02165: val_loss improved from 0.10107 to 0.10107, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2166/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02166: val_loss did not improve from 0.10107\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2167/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02167: val_loss did not improve from 0.10107\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2168/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02168: val_loss improved from 0.10107 to 0.10105, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2169/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02169: val_loss improved from 0.10105 to 0.10105, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1013 - val_loss: 0.1011\n",
      "Epoch 2170/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1013\n",
      "Epoch 02170: val_loss improved from 0.10105 to 0.10105, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1013 - val_loss: 0.1010\n",
      "Epoch 2171/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02171: val_loss improved from 0.10105 to 0.10104, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2172/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02172: val_loss did not improve from 0.10104\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2173/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02173: val_loss improved from 0.10104 to 0.10104, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2174/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02174: val_loss improved from 0.10104 to 0.10103, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2175/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02175: val_loss improved from 0.10103 to 0.10103, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2176/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02176: val_loss improved from 0.10103 to 0.10102, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2177/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02177: val_loss did not improve from 0.10102\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2178/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02178: val_loss improved from 0.10102 to 0.10102, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2179/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02179: val_loss improved from 0.10102 to 0.10101, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2180/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02180: val_loss improved from 0.10101 to 0.10101, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2181/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02181: val_loss improved from 0.10101 to 0.10101, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2182/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02182: val_loss improved from 0.10101 to 0.10100, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2183/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02183: val_loss improved from 0.10100 to 0.10100, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2184/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02184: val_loss did not improve from 0.10100\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2185/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02185: val_loss improved from 0.10100 to 0.10100, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2186/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02186: val_loss improved from 0.10100 to 0.10099, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2187/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02187: val_loss improved from 0.10099 to 0.10098, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2188/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02188: val_loss did not improve from 0.10098\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2189/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02189: val_loss improved from 0.10098 to 0.10098, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2190/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02190: val_loss improved from 0.10098 to 0.10098, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2191/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02191: val_loss improved from 0.10098 to 0.10097, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2192/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02192: val_loss improved from 0.10097 to 0.10096, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2193/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02193: val_loss did not improve from 0.10096\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2194/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02194: val_loss did not improve from 0.10096\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2195/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02195: val_loss improved from 0.10096 to 0.10096, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2196/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1012\n",
      "Epoch 02196: val_loss improved from 0.10096 to 0.10095, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1012 - val_loss: 0.1010\n",
      "Epoch 2197/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02197: val_loss improved from 0.10095 to 0.10095, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2198/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02198: val_loss improved from 0.10095 to 0.10094, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2199/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02199: val_loss improved from 0.10094 to 0.10094, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2200/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02200: val_loss did not improve from 0.10094\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2201/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02201: val_loss improved from 0.10094 to 0.10093, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2202/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02202: val_loss improved from 0.10093 to 0.10093, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2203/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02203: val_loss did not improve from 0.10093\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2204/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02204: val_loss did not improve from 0.10093\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2205/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02205: val_loss improved from 0.10093 to 0.10092, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2206/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02206: val_loss improved from 0.10092 to 0.10091, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2207/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02207: val_loss did not improve from 0.10091\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2208/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02208: val_loss improved from 0.10091 to 0.10091, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2209/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02209: val_loss improved from 0.10091 to 0.10090, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2210/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02210: val_loss improved from 0.10090 to 0.10089, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2211/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02211: val_loss did not improve from 0.10089\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2212/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02212: val_loss improved from 0.10089 to 0.10088, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2213/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02213: val_loss did not improve from 0.10088\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2214/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02214: val_loss did not improve from 0.10088\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2215/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02215: val_loss improved from 0.10088 to 0.10088, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2216/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02216: val_loss improved from 0.10088 to 0.10087, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2217/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02217: val_loss improved from 0.10087 to 0.10087, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2218/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02218: val_loss improved from 0.10087 to 0.10087, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2219/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02219: val_loss improved from 0.10087 to 0.10086, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2220/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02220: val_loss did not improve from 0.10086\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2221/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02221: val_loss improved from 0.10086 to 0.10085, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1011 - val_loss: 0.1008\n",
      "Epoch 2222/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02222: val_loss did not improve from 0.10085\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1011 - val_loss: 0.1009\n",
      "Epoch 2223/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02223: val_loss did not improve from 0.10085\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1011 - val_loss: 0.1008\n",
      "Epoch 2224/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02224: val_loss improved from 0.10085 to 0.10085, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1011 - val_loss: 0.1008\n",
      "Epoch 2225/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1011\n",
      "Epoch 02225: val_loss improved from 0.10085 to 0.10084, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1011 - val_loss: 0.1008\n",
      "Epoch 2226/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02226: val_loss did not improve from 0.10084\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2227/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010- ETA: 0s - loss: 0.\n",
      "Epoch 02227: val_loss improved from 0.10084 to 0.10084, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1010 - val_loss: 0.1008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2228/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02228: val_loss improved from 0.10084 to 0.10083, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2229/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02229: val_loss did not improve from 0.10083\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2230/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02230: val_loss did not improve from 0.10083\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2231/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02231: val_loss improved from 0.10083 to 0.10083, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2232/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02232: val_loss improved from 0.10083 to 0.10081, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2233/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02233: val_loss did not improve from 0.10081\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2234/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02234: val_loss improved from 0.10081 to 0.10081, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2235/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010- ETA: 0s - loss: 0.\n",
      "Epoch 02235: val_loss improved from 0.10081 to 0.10080, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2236/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02236: val_loss did not improve from 0.10080\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2237/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02237: val_loss improved from 0.10080 to 0.10080, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2238/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02238: val_loss improved from 0.10080 to 0.10080, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2239/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02239: val_loss improved from 0.10080 to 0.10079, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2240/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02240: val_loss improved from 0.10079 to 0.10079, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2241/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02241: val_loss did not improve from 0.10079\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2242/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02242: val_loss improved from 0.10079 to 0.10078, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2243/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010- ETA: 0s - loss: 0.100\n",
      "Epoch 02243: val_loss improved from 0.10078 to 0.10077, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2244/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02244: val_loss did not improve from 0.10077\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2245/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02245: val_loss improved from 0.10077 to 0.10077, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2246/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02246: val_loss improved from 0.10077 to 0.10077, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2247/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02247: val_loss improved from 0.10077 to 0.10076, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2248/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02248: val_loss did not improve from 0.10076\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2249/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02249: val_loss improved from 0.10076 to 0.10076, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2250/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010- ETA: 0s - loss: 0.\n",
      "Epoch 02250: val_loss improved from 0.10076 to 0.10075, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2251/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02251: val_loss improved from 0.10075 to 0.10075, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2252/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1010\n",
      "Epoch 02252: val_loss did not improve from 0.10075\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1010 - val_loss: 0.1008\n",
      "Epoch 2253/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02253: val_loss improved from 0.10075 to 0.10074, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2254/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02254: val_loss improved from 0.10074 to 0.10074, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2255/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02255: val_loss improved from 0.10074 to 0.10073, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2256/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02256: val_loss improved from 0.10073 to 0.10073, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2257/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02257: val_loss did not improve from 0.10073\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2258/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02258: val_loss improved from 0.10073 to 0.10073, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2259/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02259: val_loss improved from 0.10073 to 0.10072, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1009 - val_loss: 0.1007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2260/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02260: val_loss did not improve from 0.10072\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2261/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02261: val_loss improved from 0.10072 to 0.10071, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2262/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02262: val_loss did not improve from 0.10071\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2263/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02263: val_loss improved from 0.10071 to 0.10070, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2264/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02264: val_loss did not improve from 0.10070\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2265/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02265: val_loss improved from 0.10070 to 0.10070, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2266/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02266: val_loss improved from 0.10070 to 0.10069, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2267/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02267: val_loss did not improve from 0.10069\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2268/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02268: val_loss improved from 0.10069 to 0.10068, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2269/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02269: val_loss did not improve from 0.10068\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2270/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02270: val_loss did not improve from 0.10068\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2271/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02271: val_loss did not improve from 0.10068\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2272/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02272: val_loss improved from 0.10068 to 0.10068, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2273/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02273: val_loss improved from 0.10068 to 0.10067, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2274/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02274: val_loss did not improve from 0.10067\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2275/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02275: val_loss did not improve from 0.10067\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2276/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02276: val_loss did not improve from 0.10067\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2277/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02277: val_loss improved from 0.10067 to 0.10066, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2278/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02278: val_loss improved from 0.10066 to 0.10065, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1009 - val_loss: 0.1006\n",
      "Epoch 2279/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009- ETA: 0s - loss: 0.0\n",
      "Epoch 02279: val_loss did not improve from 0.10065\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1009 - val_loss: 0.1007\n",
      "Epoch 2280/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02280: val_loss improved from 0.10065 to 0.10064, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1009 - val_loss: 0.1006\n",
      "Epoch 2281/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1009\n",
      "Epoch 02281: val_loss did not improve from 0.10064\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1009 - val_loss: 0.1006\n",
      "Epoch 2282/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02282: val_loss improved from 0.10064 to 0.10064, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2283/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02283: val_loss improved from 0.10064 to 0.10064, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2284/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02284: val_loss did not improve from 0.10064\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2285/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02285: val_loss improved from 0.10064 to 0.10063, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2286/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02286: val_loss improved from 0.10063 to 0.10063, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2287/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02287: val_loss improved from 0.10063 to 0.10062, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2288/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02288: val_loss did not improve from 0.10062\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2289/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02289: val_loss improved from 0.10062 to 0.10061, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2290/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02290: val_loss did not improve from 0.10061\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2291/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02291: val_loss improved from 0.10061 to 0.10061, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2292/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02292: val_loss improved from 0.10061 to 0.10060, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2293/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02293: val_loss did not improve from 0.10060\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2294/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02294: val_loss improved from 0.10060 to 0.10060, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2295/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02295: val_loss improved from 0.10060 to 0.10059, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2296/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02296: val_loss did not improve from 0.10059\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2297/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02297: val_loss did not improve from 0.10059\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2298/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02298: val_loss improved from 0.10059 to 0.10058, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2299/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02299: val_loss did not improve from 0.10058\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2300/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02300: val_loss improved from 0.10058 to 0.10058, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2301/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02301: val_loss improved from 0.10058 to 0.10057, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2302/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02302: val_loss did not improve from 0.10057\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2303/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02303: val_loss improved from 0.10057 to 0.10057, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2304/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02304: val_loss improved from 0.10057 to 0.10056, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2305/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02305: val_loss improved from 0.10056 to 0.10056, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2306/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02306: val_loss did not improve from 0.10056\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2307/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02307: val_loss improved from 0.10056 to 0.10055, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2308/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02308: val_loss improved from 0.10055 to 0.10055, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1008 - val_loss: 0.1005\n",
      "Epoch 2309/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02309: val_loss did not improve from 0.10055\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1008 - val_loss: 0.1006\n",
      "Epoch 2310/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1008\n",
      "Epoch 02310: val_loss improved from 0.10055 to 0.10054, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1008 - val_loss: 0.1005\n",
      "Epoch 2311/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02311: val_loss improved from 0.10054 to 0.10054, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2312/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02312: val_loss improved from 0.10054 to 0.10053, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2313/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02313: val_loss improved from 0.10053 to 0.10053, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2314/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02314: val_loss improved from 0.10053 to 0.10053, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2315/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02315: val_loss did not improve from 0.10053\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2316/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02316: val_loss improved from 0.10053 to 0.10052, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2317/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02317: val_loss improved from 0.10052 to 0.10052, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2318/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02318: val_loss did not improve from 0.10052\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2319/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02319: val_loss did not improve from 0.10052\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2320/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02320: val_loss improved from 0.10052 to 0.10051, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2321/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02321: val_loss did not improve from 0.10051\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2322/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02322: val_loss improved from 0.10051 to 0.10050, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2323/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02323: val_loss did not improve from 0.10050\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2324/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02324: val_loss improved from 0.10050 to 0.10049, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2325/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02325: val_loss did not improve from 0.10049\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1007 - val_loss: 0.1005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2326/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02326: val_loss improved from 0.10049 to 0.10049, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2327/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02327: val_loss improved from 0.10049 to 0.10049, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2328/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02328: val_loss improved from 0.10049 to 0.10048, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2329/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02329: val_loss did not improve from 0.10048\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2330/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02330: val_loss improved from 0.10048 to 0.10048, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2331/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02331: val_loss improved from 0.10048 to 0.10047, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2332/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02332: val_loss improved from 0.10047 to 0.10047, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2333/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02333: val_loss improved from 0.10047 to 0.10046, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2334/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02334: val_loss did not improve from 0.10046\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2335/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02335: val_loss did not improve from 0.10046\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2336/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02336: val_loss improved from 0.10046 to 0.10045, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2337/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02337: val_loss did not improve from 0.10045\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1007 - val_loss: 0.1005\n",
      "Epoch 2338/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02338: val_loss improved from 0.10045 to 0.10045, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1007 - val_loss: 0.1004\n",
      "Epoch 2339/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02339: val_loss improved from 0.10045 to 0.10044, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1007 - val_loss: 0.1004\n",
      "Epoch 2340/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02340: val_loss did not improve from 0.10044\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1007 - val_loss: 0.1004\n",
      "Epoch 2341/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02341: val_loss improved from 0.10044 to 0.10044, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1007 - val_loss: 0.1004\n",
      "Epoch 2342/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1007\n",
      "Epoch 02342: val_loss improved from 0.10044 to 0.10044, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1007 - val_loss: 0.1004\n",
      "Epoch 2343/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02343: val_loss improved from 0.10044 to 0.10043, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2344/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02344: val_loss improved from 0.10043 to 0.10043, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2345/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02345: val_loss improved from 0.10043 to 0.10043, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2346/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02346: val_loss did not improve from 0.10043\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2347/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02347: val_loss did not improve from 0.10043\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2348/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02348: val_loss improved from 0.10043 to 0.10042, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2349/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006- ETA: 0s - loss: 0.102\n",
      "Epoch 02349: val_loss did not improve from 0.10042\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2350/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02350: val_loss improved from 0.10042 to 0.10042, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2351/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02351: val_loss improved from 0.10042 to 0.10041, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2352/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02352: val_loss did not improve from 0.10041\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2353/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02353: val_loss improved from 0.10041 to 0.10040, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2354/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02354: val_loss improved from 0.10040 to 0.10040, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2355/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02355: val_loss improved from 0.10040 to 0.10040, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2356/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02356: val_loss did not improve from 0.10040\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2357/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02357: val_loss improved from 0.10040 to 0.10039, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2358/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02358: val_loss did not improve from 0.10039\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2359/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02359: val_loss did not improve from 0.10039\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2360/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02360: val_loss did not improve from 0.10039\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2361/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02361: val_loss improved from 0.10039 to 0.10038, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2362/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02362: val_loss improved from 0.10038 to 0.10037, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2363/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02363: val_loss did not improve from 0.10037\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2364/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02364: val_loss improved from 0.10037 to 0.10037, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2365/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02365: val_loss improved from 0.10037 to 0.10036, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2366/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02366: val_loss improved from 0.10036 to 0.10036, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2367/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02367: val_loss did not improve from 0.10036\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2368/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02368: val_loss improved from 0.10036 to 0.10035, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2369/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02369: val_loss improved from 0.10035 to 0.10035, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1006 - val_loss: 0.1003\n",
      "Epoch 2370/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02370: val_loss did not improve from 0.10035\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1006 - val_loss: 0.1004\n",
      "Epoch 2371/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02371: val_loss improved from 0.10035 to 0.10034, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1006 - val_loss: 0.1003\n",
      "Epoch 2372/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02372: val_loss improved from 0.10034 to 0.10034, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1006 - val_loss: 0.1003\n",
      "Epoch 2373/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1006\n",
      "Epoch 02373: val_loss improved from 0.10034 to 0.10034, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1006 - val_loss: 0.1003\n",
      "Epoch 2374/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02374: val_loss did not improve from 0.10034\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2375/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02375: val_loss improved from 0.10034 to 0.10033, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2376/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02376: val_loss did not improve from 0.10033\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2377/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02377: val_loss improved from 0.10033 to 0.10032, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2378/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02378: val_loss did not improve from 0.10032\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2379/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02379: val_loss did not improve from 0.10032\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2380/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02380: val_loss improved from 0.10032 to 0.10031, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2381/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02381: val_loss did not improve from 0.10031\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2382/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02382: val_loss did not improve from 0.10031\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2383/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02383: val_loss improved from 0.10031 to 0.10030, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2384/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02384: val_loss improved from 0.10030 to 0.10030, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2385/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02385: val_loss improved from 0.10030 to 0.10030, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2386/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02386: val_loss did not improve from 0.10030\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2387/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02387: val_loss did not improve from 0.10030\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2388/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02388: val_loss improved from 0.10030 to 0.10029, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2389/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02389: val_loss improved from 0.10029 to 0.10028, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2390/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02390: val_loss did not improve from 0.10028\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2391/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02391: val_loss improved from 0.10028 to 0.10028, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1005 - val_loss: 0.1003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2392/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02392: val_loss did not improve from 0.10028\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2393/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02393: val_loss did not improve from 0.10028\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2394/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02394: val_loss improved from 0.10028 to 0.10026, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2395/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02395: val_loss did not improve from 0.10026\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2396/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02396: val_loss did not improve from 0.10026\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2397/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02397: val_loss improved from 0.10026 to 0.10026, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2398/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02398: val_loss improved from 0.10026 to 0.10026, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2399/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02399: val_loss improved from 0.10026 to 0.10025, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2400/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02400: val_loss improved from 0.10025 to 0.10025, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1005 - val_loss: 0.1003\n",
      "Epoch 2401/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02401: val_loss improved from 0.10025 to 0.10024, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1005 - val_loss: 0.1002\n",
      "Epoch 2402/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02402: val_loss did not improve from 0.10024\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1005 - val_loss: 0.1002\n",
      "Epoch 2403/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02403: val_loss improved from 0.10024 to 0.10024, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1005 - val_loss: 0.1002\n",
      "Epoch 2404/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02404: val_loss did not improve from 0.10024\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1005 - val_loss: 0.1002\n",
      "Epoch 2405/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02405: val_loss improved from 0.10024 to 0.10023, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1005 - val_loss: 0.1002\n",
      "Epoch 2406/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02406: val_loss improved from 0.10023 to 0.10023, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1005 - val_loss: 0.1002\n",
      "Epoch 2407/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1005\n",
      "Epoch 02407: val_loss improved from 0.10023 to 0.10023, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1005 - val_loss: 0.1002\n",
      "Epoch 2408/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02408: val_loss did not improve from 0.10023\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2409/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02409: val_loss improved from 0.10023 to 0.10022, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2410/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02410: val_loss did not improve from 0.10022\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2411/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02411: val_loss did not improve from 0.10022\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2412/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02412: val_loss improved from 0.10022 to 0.10021, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2413/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004- ETA: 0s - loss: 0.09\n",
      "Epoch 02413: val_loss did not improve from 0.10021\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2414/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02414: val_loss improved from 0.10021 to 0.10020, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2415/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02415: val_loss improved from 0.10020 to 0.10020, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2416/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02416: val_loss did not improve from 0.10020\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2417/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02417: val_loss improved from 0.10020 to 0.10019, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2418/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02418: val_loss did not improve from 0.10019\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2419/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02419: val_loss improved from 0.10019 to 0.10019, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2420/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02420: val_loss improved from 0.10019 to 0.10019, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2421/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02421: val_loss improved from 0.10019 to 0.10018, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2422/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02422: val_loss did not improve from 0.10018\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2423/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02423: val_loss improved from 0.10018 to 0.10017, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2424/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02424: val_loss did not improve from 0.10017\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1004 - val_loss: 0.1002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2425/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02425: val_loss improved from 0.10017 to 0.10017, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2426/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02426: val_loss improved from 0.10017 to 0.10017, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2427/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02427: val_loss improved from 0.10017 to 0.10016, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2428/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02428: val_loss improved from 0.10016 to 0.10016, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2429/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02429: val_loss improved from 0.10016 to 0.10016, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2430/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02430: val_loss improved from 0.10016 to 0.10015, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2431/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02431: val_loss improved from 0.10015 to 0.10015, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1004 - val_loss: 0.1001\n",
      "Epoch 2432/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02432: val_loss did not improve from 0.10015\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1004 - val_loss: 0.1002\n",
      "Epoch 2433/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02433: val_loss improved from 0.10015 to 0.10014, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1004 - val_loss: 0.1001\n",
      "Epoch 2434/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02434: val_loss improved from 0.10014 to 0.10014, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1004 - val_loss: 0.1001\n",
      "Epoch 2435/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02435: val_loss improved from 0.10014 to 0.10014, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1004 - val_loss: 0.1001\n",
      "Epoch 2436/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02436: val_loss did not improve from 0.10014\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1004 - val_loss: 0.1001\n",
      "Epoch 2437/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02437: val_loss improved from 0.10014 to 0.10013, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1004 - val_loss: 0.1001\n",
      "Epoch 2438/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02438: val_loss improved from 0.10013 to 0.10013, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1004 - val_loss: 0.1001\n",
      "Epoch 2439/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1004\n",
      "Epoch 02439: val_loss did not improve from 0.10013\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1004 - val_loss: 0.1001\n",
      "Epoch 2440/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02440: val_loss improved from 0.10013 to 0.10012, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2441/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02441: val_loss did not improve from 0.10012\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2442/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02442: val_loss improved from 0.10012 to 0.10011, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2443/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02443: val_loss improved from 0.10011 to 0.10011, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2444/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02444: val_loss improved from 0.10011 to 0.10011, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2445/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02445: val_loss improved from 0.10011 to 0.10010, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2446/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02446: val_loss improved from 0.10010 to 0.10010, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2447/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02447: val_loss did not improve from 0.10010\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2448/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003- ETA: 0s - loss: 0\n",
      "Epoch 02448: val_loss improved from 0.10010 to 0.10009, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2449/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02449: val_loss improved from 0.10009 to 0.10009, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2450/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02450: val_loss did not improve from 0.10009\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2451/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02451: val_loss improved from 0.10009 to 0.10009, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2452/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02452: val_loss improved from 0.10009 to 0.10008, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2453/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02453: val_loss improved from 0.10008 to 0.10008, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2454/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02454: val_loss did not improve from 0.10008\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2455/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02455: val_loss improved from 0.10008 to 0.10008, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2456/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02456: val_loss improved from 0.10008 to 0.10007, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2457/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02457: val_loss did not improve from 0.10007\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2458/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02458: val_loss improved from 0.10007 to 0.10006, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2459/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02459: val_loss improved from 0.10006 to 0.10006, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2460/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02460: val_loss did not improve from 0.10006\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2461/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02461: val_loss improved from 0.10006 to 0.10006, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2462/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02462: val_loss improved from 0.10006 to 0.10005, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1003 - val_loss: 0.1000\n",
      "Epoch 2463/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02463: val_loss did not improve from 0.10005\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1003 - val_loss: 0.1001\n",
      "Epoch 2464/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02464: val_loss improved from 0.10005 to 0.10004, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1003 - val_loss: 0.1000\n",
      "Epoch 2465/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02465: val_loss improved from 0.10004 to 0.10004, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1003 - val_loss: 0.1000\n",
      "Epoch 2466/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02466: val_loss did not improve from 0.10004\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1003 - val_loss: 0.1000\n",
      "Epoch 2467/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02467: val_loss improved from 0.10004 to 0.10004, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1003 - val_loss: 0.1000\n",
      "Epoch 2468/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02468: val_loss improved from 0.10004 to 0.10003, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1003 - val_loss: 0.1000\n",
      "Epoch 2469/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02469: val_loss did not improve from 0.10003\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1003 - val_loss: 0.1000\n",
      "Epoch 2470/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02470: val_loss did not improve from 0.10003\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1003 - val_loss: 0.1000\n",
      "Epoch 2471/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02471: val_loss improved from 0.10003 to 0.10002, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1003 - val_loss: 0.1000\n",
      "Epoch 2472/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1003\n",
      "Epoch 02472: val_loss improved from 0.10002 to 0.10002, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1003 - val_loss: 0.1000\n",
      "Epoch 2473/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02473: val_loss did not improve from 0.10002\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2474/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02474: val_loss did not improve from 0.10002\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2475/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02475: val_loss improved from 0.10002 to 0.10001, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2476/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02476: val_loss did not improve from 0.10001\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2477/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02477: val_loss improved from 0.10001 to 0.10000, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2478/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02478: val_loss did not improve from 0.10000\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2479/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02479: val_loss improved from 0.10000 to 0.10000, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2480/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02480: val_loss improved from 0.10000 to 0.10000, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2481/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02481: val_loss improved from 0.10000 to 0.09999, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2482/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02482: val_loss did not improve from 0.09999\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2483/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02483: val_loss did not improve from 0.09999\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2484/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02484: val_loss improved from 0.09999 to 0.09999, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2485/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02485: val_loss improved from 0.09999 to 0.09999, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2486/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02486: val_loss improved from 0.09999 to 0.09998, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2487/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02487: val_loss improved from 0.09998 to 0.09997, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2488/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02488: val_loss improved from 0.09997 to 0.09997, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2489/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02489: val_loss improved from 0.09997 to 0.09997, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1002 - val_loss: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2490/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02490: val_loss did not improve from 0.09997\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2491/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02491: val_loss improved from 0.09997 to 0.09996, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2492/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02492: val_loss improved from 0.09996 to 0.09996, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2493/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02493: val_loss did not improve from 0.09996\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2494/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02494: val_loss improved from 0.09996 to 0.09996, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2495/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02495: val_loss improved from 0.09996 to 0.09995, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1002 - val_loss: 0.1000\n",
      "Epoch 2496/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02496: val_loss improved from 0.09995 to 0.09995, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1002 - val_loss: 0.0999\n",
      "Epoch 2497/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02497: val_loss improved from 0.09995 to 0.09994, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1002 - val_loss: 0.0999\n",
      "Epoch 2498/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02498: val_loss improved from 0.09994 to 0.09994, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1002 - val_loss: 0.0999\n",
      "Epoch 2499/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02499: val_loss did not improve from 0.09994\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1002 - val_loss: 0.0999\n",
      "Epoch 2500/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02500: val_loss improved from 0.09994 to 0.09994, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1002 - val_loss: 0.0999\n",
      "Epoch 2501/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02501: val_loss improved from 0.09994 to 0.09993, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1002 - val_loss: 0.0999\n",
      "Epoch 2502/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02502: val_loss improved from 0.09993 to 0.09993, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.1002 - val_loss: 0.0999\n",
      "Epoch 2503/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02503: val_loss improved from 0.09993 to 0.09993, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1002 - val_loss: 0.0999\n",
      "Epoch 2504/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02504: val_loss improved from 0.09993 to 0.09992, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1002 - val_loss: 0.0999\n",
      "Epoch 2505/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02505: val_loss improved from 0.09992 to 0.09992, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1002 - val_loss: 0.0999\n",
      "Epoch 2506/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02506: val_loss did not improve from 0.09992\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1002 - val_loss: 0.0999\n",
      "Epoch 2507/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1002\n",
      "Epoch 02507: val_loss did not improve from 0.09992\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1002 - val_loss: 0.0999\n",
      "Epoch 2508/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02508: val_loss improved from 0.09992 to 0.09991, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2509/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02509: val_loss improved from 0.09991 to 0.09991, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2510/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02510: val_loss did not improve from 0.09991\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2511/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02511: val_loss improved from 0.09991 to 0.09990, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2512/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02512: val_loss improved from 0.09990 to 0.09989, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2513/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02513: val_loss did not improve from 0.09989\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2514/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02514: val_loss improved from 0.09989 to 0.09989, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2515/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02515: val_loss improved from 0.09989 to 0.09989, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2516/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02516: val_loss improved from 0.09989 to 0.09988, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2517/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02517: val_loss improved from 0.09988 to 0.09988, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2518/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02518: val_loss did not improve from 0.09988\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2519/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02519: val_loss improved from 0.09988 to 0.09987, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2520/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02520: val_loss did not improve from 0.09987\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2521/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02521: val_loss did not improve from 0.09987\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2522/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02522: val_loss improved from 0.09987 to 0.09987, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2523/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02523: val_loss improved from 0.09987 to 0.09987, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2524/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02524: val_loss improved from 0.09987 to 0.09986, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2525/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02525: val_loss did not improve from 0.09986\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2526/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02526: val_loss improved from 0.09986 to 0.09986, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2527/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02527: val_loss improved from 0.09986 to 0.09985, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2528/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02528: val_loss did not improve from 0.09985\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1001 - val_loss: 0.0999\n",
      "Epoch 2529/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02529: val_loss improved from 0.09985 to 0.09985, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2530/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02530: val_loss improved from 0.09985 to 0.09985, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2531/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02531: val_loss improved from 0.09985 to 0.09984, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2532/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02532: val_loss improved from 0.09984 to 0.09984, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2533/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02533: val_loss improved from 0.09984 to 0.09984, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2534/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02534: val_loss improved from 0.09984 to 0.09984, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2535/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001- ETA: 0s - loss: 0.0\n",
      "Epoch 02535: val_loss improved from 0.09984 to 0.09983, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2536/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02536: val_loss improved from 0.09983 to 0.09983, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2537/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02537: val_loss improved from 0.09983 to 0.09982, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2538/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02538: val_loss did not improve from 0.09982\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2539/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02539: val_loss did not improve from 0.09982\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2540/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02540: val_loss improved from 0.09982 to 0.09981, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2541/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02541: val_loss improved from 0.09981 to 0.09981, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2542/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1001\n",
      "Epoch 02542: val_loss improved from 0.09981 to 0.09981, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1001 - val_loss: 0.0998\n",
      "Epoch 2543/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02543: val_loss improved from 0.09981 to 0.09981, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2544/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02544: val_loss did not improve from 0.09981\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2545/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02545: val_loss improved from 0.09981 to 0.09980, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2546/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02546: val_loss improved from 0.09980 to 0.09980, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2547/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02547: val_loss did not improve from 0.09980\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2548/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02548: val_loss improved from 0.09980 to 0.09979, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2549/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02549: val_loss improved from 0.09979 to 0.09979, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2550/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02550: val_loss improved from 0.09979 to 0.09978, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2551/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02551: val_loss did not improve from 0.09978\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2552/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02552: val_loss did not improve from 0.09978\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2553/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02553: val_loss improved from 0.09978 to 0.09977, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1000 - val_loss: 0.0998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2554/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02554: val_loss did not improve from 0.09977\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2555/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02555: val_loss improved from 0.09977 to 0.09977, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2556/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02556: val_loss improved from 0.09977 to 0.09977, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2557/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02557: val_loss improved from 0.09977 to 0.09977, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2558/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000- ETA: 0s - loss: 0.098\n",
      "Epoch 02558: val_loss improved from 0.09977 to 0.09976, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2559/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02559: val_loss did not improve from 0.09976\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2560/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02560: val_loss did not improve from 0.09976\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2561/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02561: val_loss improved from 0.09976 to 0.09975, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2562/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02562: val_loss did not improve from 0.09975\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2563/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02563: val_loss did not improve from 0.09975\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1000 - val_loss: 0.0998\n",
      "Epoch 2564/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02564: val_loss improved from 0.09975 to 0.09974, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2565/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02565: val_loss did not improve from 0.09974\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2566/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02566: val_loss improved from 0.09974 to 0.09974, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2567/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02567: val_loss improved from 0.09974 to 0.09974, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2568/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02568: val_loss improved from 0.09974 to 0.09974, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2569/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02569: val_loss improved from 0.09974 to 0.09973, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2570/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02570: val_loss improved from 0.09973 to 0.09973, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2571/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02571: val_loss did not improve from 0.09973\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2572/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02572: val_loss improved from 0.09973 to 0.09973, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2573/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02573: val_loss improved from 0.09973 to 0.09972, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2574/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02574: val_loss improved from 0.09972 to 0.09972, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2575/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02575: val_loss did not improve from 0.09972\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2576/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02576: val_loss improved from 0.09972 to 0.09971, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2577/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02577: val_loss improved from 0.09971 to 0.09971, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2578/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02578: val_loss did not improve from 0.09971\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2579/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02579: val_loss did not improve from 0.09971\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2580/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.1000\n",
      "Epoch 02580: val_loss improved from 0.09971 to 0.09970, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.1000 - val_loss: 0.0997\n",
      "Epoch 2581/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02581: val_loss improved from 0.09970 to 0.09970, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2582/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02582: val_loss improved from 0.09970 to 0.09969, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2583/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02583: val_loss improved from 0.09969 to 0.09969, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2584/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02584: val_loss did not improve from 0.09969\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2585/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02585: val_loss improved from 0.09969 to 0.09968, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2586/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02586: val_loss did not improve from 0.09968\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0999 - val_loss: 0.0997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2587/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02587: val_loss improved from 0.09968 to 0.09968, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2588/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02588: val_loss improved from 0.09968 to 0.09968, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2589/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02589: val_loss improved from 0.09968 to 0.09968, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2590/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02590: val_loss did not improve from 0.09968\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2591/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02591: val_loss improved from 0.09968 to 0.09966, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2592/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02592: val_loss did not improve from 0.09966\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2593/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02593: val_loss did not improve from 0.09966\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2594/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02594: val_loss improved from 0.09966 to 0.09966, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2595/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02595: val_loss improved from 0.09966 to 0.09965, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2596/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02596: val_loss improved from 0.09965 to 0.09965, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2597/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02597: val_loss improved from 0.09965 to 0.09965, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2598/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02598: val_loss did not improve from 0.09965\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0999 - val_loss: 0.0997\n",
      "Epoch 2599/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02599: val_loss improved from 0.09965 to 0.09965, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2600/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02600: val_loss improved from 0.09965 to 0.09964, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2601/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02601: val_loss did not improve from 0.09964\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2602/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02602: val_loss did not improve from 0.09964\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2603/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02603: val_loss improved from 0.09964 to 0.09964, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2604/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02604: val_loss improved from 0.09964 to 0.09964, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2605/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02605: val_loss improved from 0.09964 to 0.09964, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2606/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02606: val_loss did not improve from 0.09964\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2607/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02607: val_loss improved from 0.09964 to 0.09962, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2608/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02608: val_loss improved from 0.09962 to 0.09962, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2609/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02609: val_loss improved from 0.09962 to 0.09962, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2610/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02610: val_loss did not improve from 0.09962\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2611/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02611: val_loss improved from 0.09962 to 0.09962, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2612/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02612: val_loss improved from 0.09962 to 0.09961, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2613/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02613: val_loss improved from 0.09961 to 0.09961, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2614/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02614: val_loss did not improve from 0.09961\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2615/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02615: val_loss improved from 0.09961 to 0.09960, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2616/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02616: val_loss did not improve from 0.09960\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2617/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0999\n",
      "Epoch 02617: val_loss improved from 0.09960 to 0.09960, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0999 - val_loss: 0.0996\n",
      "Epoch 2618/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02618: val_loss did not improve from 0.09960\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2619/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02619: val_loss improved from 0.09960 to 0.09959, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0998 - val_loss: 0.0996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2620/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02620: val_loss improved from 0.09959 to 0.09959, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2621/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02621: val_loss did not improve from 0.09959\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2622/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02622: val_loss improved from 0.09959 to 0.09958, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2623/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02623: val_loss did not improve from 0.09958\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2624/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02624: val_loss improved from 0.09958 to 0.09958, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2625/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02625: val_loss did not improve from 0.09958\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2626/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02626: val_loss improved from 0.09958 to 0.09958, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2627/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02627: val_loss did not improve from 0.09958\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2628/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02628: val_loss improved from 0.09958 to 0.09957, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2629/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02629: val_loss improved from 0.09957 to 0.09957, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2630/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02630: val_loss did not improve from 0.09957\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2631/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02631: val_loss improved from 0.09957 to 0.09956, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2632/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02632: val_loss improved from 0.09956 to 0.09956, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2633/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02633: val_loss improved from 0.09956 to 0.09955, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2634/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02634: val_loss did not improve from 0.09955\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2635/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02635: val_loss improved from 0.09955 to 0.09955, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2636/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02636: val_loss improved from 0.09955 to 0.09954, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2637/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02637: val_loss did not improve from 0.09954\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0998 - val_loss: 0.0996\n",
      "Epoch 2638/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02638: val_loss improved from 0.09954 to 0.09954, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2639/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02639: val_loss did not improve from 0.09954\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2640/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02640: val_loss improved from 0.09954 to 0.09953, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2641/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02641: val_loss did not improve from 0.09953\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2642/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02642: val_loss did not improve from 0.09953\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2643/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02643: val_loss did not improve from 0.09953\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2644/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02644: val_loss improved from 0.09953 to 0.09952, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2645/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02645: val_loss did not improve from 0.09952\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2646/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02646: val_loss improved from 0.09952 to 0.09952, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2647/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02647: val_loss improved from 0.09952 to 0.09952, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2648/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02648: val_loss improved from 0.09952 to 0.09952, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2649/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02649: val_loss improved from 0.09952 to 0.09951, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2650/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02650: val_loss did not improve from 0.09951\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2651/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02651: val_loss improved from 0.09951 to 0.09950, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2652/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02652: val_loss did not improve from 0.09950\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2653/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02653: val_loss improved from 0.09950 to 0.09950, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2654/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02654: val_loss did not improve from 0.09950\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2655/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02655: val_loss improved from 0.09950 to 0.09950, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2656/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02656: val_loss improved from 0.09950 to 0.09949, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2657/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0998\n",
      "Epoch 02657: val_loss improved from 0.09949 to 0.09949, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0998 - val_loss: 0.0995\n",
      "Epoch 2658/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02658: val_loss improved from 0.09949 to 0.09949, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2659/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02659: val_loss improved from 0.09949 to 0.09949, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2660/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02660: val_loss did not improve from 0.09949\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2661/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02661: val_loss did not improve from 0.09949\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2662/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02662: val_loss improved from 0.09949 to 0.09948, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2663/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02663: val_loss improved from 0.09948 to 0.09947, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2664/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02664: val_loss did not improve from 0.09947\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2665/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02665: val_loss did not improve from 0.09947\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2666/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02666: val_loss improved from 0.09947 to 0.09946, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2667/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02667: val_loss did not improve from 0.09946\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2668/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02668: val_loss improved from 0.09946 to 0.09946, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2669/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02669: val_loss did not improve from 0.09946\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2670/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02670: val_loss did not improve from 0.09946\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2671/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02671: val_loss improved from 0.09946 to 0.09945, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2672/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02672: val_loss improved from 0.09945 to 0.09945, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2673/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02673: val_loss improved from 0.09945 to 0.09945, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2674/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02674: val_loss did not improve from 0.09945\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0997 - val_loss: 0.0995\n",
      "Epoch 2675/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02675: val_loss improved from 0.09945 to 0.09944, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2676/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02676: val_loss did not improve from 0.09944\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2677/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02677: val_loss improved from 0.09944 to 0.09944, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2678/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02678: val_loss did not improve from 0.09944\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2679/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02679: val_loss improved from 0.09944 to 0.09944, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2680/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02680: val_loss did not improve from 0.09944\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2681/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02681: val_loss improved from 0.09944 to 0.09943, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2682/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02682: val_loss improved from 0.09943 to 0.09943, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2683/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02683: val_loss improved from 0.09943 to 0.09943, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2684/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02684: val_loss improved from 0.09943 to 0.09942, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2685/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02685: val_loss improved from 0.09942 to 0.09942, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0997 - val_loss: 0.0994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2686/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02686: val_loss did not improve from 0.09942\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2687/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02687: val_loss improved from 0.09942 to 0.09941, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2688/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02688: val_loss improved from 0.09941 to 0.09941, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2689/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02689: val_loss did not improve from 0.09941\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2690/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02690: val_loss improved from 0.09941 to 0.09940, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2691/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02691: val_loss did not improve from 0.09940\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2692/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02692: val_loss improved from 0.09940 to 0.09940, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2693/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02693: val_loss improved from 0.09940 to 0.09940, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2694/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02694: val_loss improved from 0.09940 to 0.09940, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2695/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02695: val_loss did not improve from 0.09940\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2696/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02696: val_loss improved from 0.09940 to 0.09939, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2697/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02697: val_loss improved from 0.09939 to 0.09938, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2698/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02698: val_loss did not improve from 0.09938\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.0996 - val_loss: 0.0994\n",
      "Epoch 2699/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02699: val_loss did not improve from 0.09938\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0996 - val_loss: 0.0994\n",
      "Epoch 2700/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02700: val_loss improved from 0.09938 to 0.09938, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2701/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02701: val_loss did not improve from 0.09938\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0996 - val_loss: 0.0994\n",
      "Epoch 2702/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0997\n",
      "Epoch 02702: val_loss improved from 0.09938 to 0.09938, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0997 - val_loss: 0.0994\n",
      "Epoch 2703/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02703: val_loss improved from 0.09938 to 0.09937, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0996 - val_loss: 0.0994\n",
      "Epoch 2704/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02704: val_loss improved from 0.09937 to 0.09937, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0996 - val_loss: 0.0994\n",
      "Epoch 2705/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02705: val_loss did not improve from 0.09937\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.0996 - val_loss: 0.0994\n",
      "Epoch 2706/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02706: val_loss improved from 0.09937 to 0.09936, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0996 - val_loss: 0.0994\n",
      "Epoch 2707/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02707: val_loss improved from 0.09936 to 0.09936, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0996 - val_loss: 0.0994\n",
      "Epoch 2708/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02708: val_loss did not improve from 0.09936\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0996 - val_loss: 0.0994\n",
      "Epoch 2709/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02709: val_loss improved from 0.09936 to 0.09936, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0996 - val_loss: 0.0994\n",
      "Epoch 2710/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02710: val_loss did not improve from 0.09936\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0996 - val_loss: 0.0994\n",
      "Epoch 2711/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02711: val_loss improved from 0.09936 to 0.09935, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2712/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02712: val_loss did not improve from 0.09935\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0996 - val_loss: 0.0994\n",
      "Epoch 2713/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02713: val_loss improved from 0.09935 to 0.09934, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2714/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02714: val_loss did not improve from 0.09934\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2715/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02715: val_loss improved from 0.09934 to 0.09934, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2716/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02716: val_loss improved from 0.09934 to 0.09933, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2717/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02717: val_loss did not improve from 0.09933\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2718/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02718: val_loss did not improve from 0.09933\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2719/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02719: val_loss improved from 0.09933 to 0.09933, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2720/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02720: val_loss improved from 0.09933 to 0.09933, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2721/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02721: val_loss improved from 0.09933 to 0.09933, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2722/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02722: val_loss did not improve from 0.09933\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2723/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02723: val_loss improved from 0.09933 to 0.09932, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2724/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02724: val_loss improved from 0.09932 to 0.09931, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2725/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02725: val_loss did not improve from 0.09931\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2726/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02726: val_loss improved from 0.09931 to 0.09931, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2727/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02727: val_loss improved from 0.09931 to 0.09931, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2728/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02728: val_loss improved from 0.09931 to 0.09930, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2729/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02729: val_loss did not improve from 0.09930\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2730/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02730: val_loss improved from 0.09930 to 0.09930, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2731/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02731: val_loss improved from 0.09930 to 0.09929, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2732/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02732: val_loss did not improve from 0.09929\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2733/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02733: val_loss did not improve from 0.09929\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2734/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02734: val_loss improved from 0.09929 to 0.09929, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2735/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02735: val_loss did not improve from 0.09929\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2736/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02736: val_loss did not improve from 0.09929\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2737/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02737: val_loss improved from 0.09929 to 0.09928, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2738/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02738: val_loss improved from 0.09928 to 0.09928, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2739/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02739: val_loss did not improve from 0.09928\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2740/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02740: val_loss improved from 0.09928 to 0.09927, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2741/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0996\n",
      "Epoch 02741: val_loss improved from 0.09927 to 0.09927, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0996 - val_loss: 0.0993\n",
      "Epoch 2742/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02742: val_loss did not improve from 0.09927\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0995 - val_loss: 0.0993\n",
      "Epoch 2743/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02743: val_loss improved from 0.09927 to 0.09927, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0995 - val_loss: 0.0993\n",
      "Epoch 2744/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02744: val_loss improved from 0.09927 to 0.09926, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0995 - val_loss: 0.0993\n",
      "Epoch 2745/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02745: val_loss did not improve from 0.09926\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0995 - val_loss: 0.0993\n",
      "Epoch 2746/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02746: val_loss did not improve from 0.09926\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0995 - val_loss: 0.0993\n",
      "Epoch 2747/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02747: val_loss improved from 0.09926 to 0.09925, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0995 - val_loss: 0.0993\n",
      "Epoch 2748/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02748: val_loss did not improve from 0.09925\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0995 - val_loss: 0.0993\n",
      "Epoch 2749/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02749: val_loss did not improve from 0.09925\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0995 - val_loss: 0.0993\n",
      "Epoch 2750/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02750: val_loss improved from 0.09925 to 0.09925, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0995 - val_loss: 0.0993\n",
      "Epoch 2751/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02751: val_loss improved from 0.09925 to 0.09925, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0995 - val_loss: 0.0992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2752/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02752: val_loss improved from 0.09925 to 0.09924, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2753/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02753: val_loss did not improve from 0.09924\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2754/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02754: val_loss did not improve from 0.09924\n",
      "7/7 [==============================] - 1s 81ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2755/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02755: val_loss improved from 0.09924 to 0.09923, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2756/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02756: val_loss did not improve from 0.09923\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2757/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02757: val_loss improved from 0.09923 to 0.09923, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2758/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02758: val_loss did not improve from 0.09923\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2759/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02759: val_loss did not improve from 0.09923\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2760/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02760: val_loss improved from 0.09923 to 0.09922, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2761/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02761: val_loss did not improve from 0.09922\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2762/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02762: val_loss improved from 0.09922 to 0.09922, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2763/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02763: val_loss did not improve from 0.09922\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2764/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02764: val_loss improved from 0.09922 to 0.09922, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2765/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02765: val_loss did not improve from 0.09922\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2766/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02766: val_loss improved from 0.09922 to 0.09921, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2767/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02767: val_loss did not improve from 0.09921\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2768/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02768: val_loss did not improve from 0.09921\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2769/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02769: val_loss did not improve from 0.09921\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2770/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02770: val_loss improved from 0.09921 to 0.09920, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2771/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02771: val_loss improved from 0.09920 to 0.09920, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2772/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02772: val_loss did not improve from 0.09920\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2773/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02773: val_loss improved from 0.09920 to 0.09919, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2774/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02774: val_loss improved from 0.09919 to 0.09919, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2775/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02775: val_loss improved from 0.09919 to 0.09918, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2776/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02776: val_loss did not improve from 0.09918\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2777/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02777: val_loss improved from 0.09918 to 0.09918, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2778/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02778: val_loss improved from 0.09918 to 0.09918, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2779/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02779: val_loss did not improve from 0.09918\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2780/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02780: val_loss improved from 0.09918 to 0.09918, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2781/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02781: val_loss improved from 0.09918 to 0.09917, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2782/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02782: val_loss did not improve from 0.09917\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2783/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02783: val_loss improved from 0.09917 to 0.09917, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2784/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0995\n",
      "Epoch 02784: val_loss improved from 0.09917 to 0.09916, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0995 - val_loss: 0.0992\n",
      "Epoch 2785/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02785: val_loss did not improve from 0.09916\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0994 - val_loss: 0.0992\n",
      "Epoch 2786/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02786: val_loss did not improve from 0.09916\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0994 - val_loss: 0.0992\n",
      "Epoch 2787/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02787: val_loss improved from 0.09916 to 0.09916, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0994 - val_loss: 0.0992\n",
      "Epoch 2788/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02788: val_loss improved from 0.09916 to 0.09915, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0994 - val_loss: 0.0992\n",
      "Epoch 2789/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02789: val_loss did not improve from 0.09915\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0994 - val_loss: 0.0992\n",
      "Epoch 2790/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02790: val_loss improved from 0.09915 to 0.09915, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0994 - val_loss: 0.0992\n",
      "Epoch 2791/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02791: val_loss improved from 0.09915 to 0.09915, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2792/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02792: val_loss improved from 0.09915 to 0.09914, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2793/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02793: val_loss improved from 0.09914 to 0.09914, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2794/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02794: val_loss did not improve from 0.09914\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2795/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02795: val_loss did not improve from 0.09914\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2796/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02796: val_loss improved from 0.09914 to 0.09914, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2797/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02797: val_loss improved from 0.09914 to 0.09913, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2798/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02798: val_loss did not improve from 0.09913\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2799/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994- ETA: 0s - loss: 0.09\n",
      "Epoch 02799: val_loss did not improve from 0.09913\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2800/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02800: val_loss improved from 0.09913 to 0.09912, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2801/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02801: val_loss did not improve from 0.09912\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2802/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02802: val_loss did not improve from 0.09912\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2803/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02803: val_loss improved from 0.09912 to 0.09912, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2804/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02804: val_loss improved from 0.09912 to 0.09911, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2805/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02805: val_loss did not improve from 0.09911\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2806/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02806: val_loss improved from 0.09911 to 0.09911, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2807/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02807: val_loss did not improve from 0.09911\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2808/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02808: val_loss did not improve from 0.09911\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2809/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02809: val_loss improved from 0.09911 to 0.09911, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2810/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02810: val_loss improved from 0.09911 to 0.09910, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2811/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02811: val_loss did not improve from 0.09910\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2812/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02812: val_loss did not improve from 0.09910\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2813/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02813: val_loss improved from 0.09910 to 0.09909, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2814/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02814: val_loss improved from 0.09909 to 0.09909, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2815/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02815: val_loss improved from 0.09909 to 0.09909, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2816/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02816: val_loss did not improve from 0.09909\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2817/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02817: val_loss improved from 0.09909 to 0.09908, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2818/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02818: val_loss did not improve from 0.09908\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2819/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02819: val_loss improved from 0.09908 to 0.09908, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2820/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02820: val_loss improved from 0.09908 to 0.09908, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2821/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02821: val_loss did not improve from 0.09908\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2822/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02822: val_loss improved from 0.09908 to 0.09907, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2823/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02823: val_loss did not improve from 0.09907\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2824/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02824: val_loss improved from 0.09907 to 0.09907, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2825/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02825: val_loss did not improve from 0.09907\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2826/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02826: val_loss did not improve from 0.09907\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2827/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02827: val_loss improved from 0.09907 to 0.09906, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2828/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02828: val_loss did not improve from 0.09906\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2829/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02829: val_loss did not improve from 0.09906\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2830/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0994\n",
      "Epoch 02830: val_loss improved from 0.09906 to 0.09905, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0994 - val_loss: 0.0991\n",
      "Epoch 2831/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02831: val_loss did not improve from 0.09905\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0993 - val_loss: 0.0991\n",
      "Epoch 2832/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02832: val_loss did not improve from 0.09905\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0993 - val_loss: 0.0991\n",
      "Epoch 2833/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02833: val_loss improved from 0.09905 to 0.09905, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2834/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02834: val_loss improved from 0.09905 to 0.09904, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2835/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02835: val_loss did not improve from 0.09904\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2836/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02836: val_loss improved from 0.09904 to 0.09904, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2837/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02837: val_loss did not improve from 0.09904\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2838/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02838: val_loss improved from 0.09904 to 0.09903, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2839/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02839: val_loss did not improve from 0.09903\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2840/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02840: val_loss improved from 0.09903 to 0.09903, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2841/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02841: val_loss improved from 0.09903 to 0.09903, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2842/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02842: val_loss improved from 0.09903 to 0.09902, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2843/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02843: val_loss did not improve from 0.09902\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2844/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02844: val_loss improved from 0.09902 to 0.09902, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2845/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02845: val_loss did not improve from 0.09902\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2846/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02846: val_loss improved from 0.09902 to 0.09902, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2847/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02847: val_loss improved from 0.09902 to 0.09901, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2848/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02848: val_loss improved from 0.09901 to 0.09901, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2849/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02849: val_loss improved from 0.09901 to 0.09901, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2850/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02850: val_loss did not improve from 0.09901\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2851/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02851: val_loss improved from 0.09901 to 0.09901, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0993 - val_loss: 0.0990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2852/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02852: val_loss improved from 0.09901 to 0.09901, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2853/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02853: val_loss improved from 0.09901 to 0.09900, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2854/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02854: val_loss improved from 0.09900 to 0.09899, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2855/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02855: val_loss did not improve from 0.09899\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2856/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02856: val_loss did not improve from 0.09899\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2857/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02857: val_loss improved from 0.09899 to 0.09899, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2858/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02858: val_loss did not improve from 0.09899\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2859/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02859: val_loss did not improve from 0.09899\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2860/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02860: val_loss improved from 0.09899 to 0.09899, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2861/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02861: val_loss improved from 0.09899 to 0.09898, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2862/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02862: val_loss improved from 0.09898 to 0.09898, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2863/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02863: val_loss did not improve from 0.09898\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2864/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993- ETA: 0s - loss: 0.\n",
      "Epoch 02864: val_loss did not improve from 0.09898\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2865/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02865: val_loss improved from 0.09898 to 0.09897, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2866/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02866: val_loss improved from 0.09897 to 0.09897, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2867/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02867: val_loss did not improve from 0.09897\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2868/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02868: val_loss improved from 0.09897 to 0.09896, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2869/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02869: val_loss did not improve from 0.09896\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2870/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02870: val_loss improved from 0.09896 to 0.09896, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2871/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02871: val_loss did not improve from 0.09896\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2872/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02872: val_loss improved from 0.09896 to 0.09895, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2873/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02873: val_loss improved from 0.09895 to 0.09895, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0993 - val_loss: 0.0989\n",
      "Epoch 2874/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0993\n",
      "Epoch 02874: val_loss did not improve from 0.09895\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0993 - val_loss: 0.0990\n",
      "Epoch 2875/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02875: val_loss did not improve from 0.09895\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0992 - val_loss: 0.0990\n",
      "Epoch 2876/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02876: val_loss improved from 0.09895 to 0.09894, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2877/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02877: val_loss did not improve from 0.09894\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2878/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02878: val_loss did not improve from 0.09894\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2879/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02879: val_loss improved from 0.09894 to 0.09894, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2880/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02880: val_loss improved from 0.09894 to 0.09893, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2881/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02881: val_loss did not improve from 0.09893\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2882/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02882: val_loss improved from 0.09893 to 0.09893, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2883/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02883: val_loss improved from 0.09893 to 0.09893, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2884/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02884: val_loss improved from 0.09893 to 0.09893, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0992 - val_loss: 0.0989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2885/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02885: val_loss improved from 0.09893 to 0.09892, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2886/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02886: val_loss did not improve from 0.09892\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2887/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02887: val_loss did not improve from 0.09892\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2888/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02888: val_loss improved from 0.09892 to 0.09892, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2889/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02889: val_loss improved from 0.09892 to 0.09891, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2890/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02890: val_loss did not improve from 0.09891\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2891/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02891: val_loss did not improve from 0.09891\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2892/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02892: val_loss improved from 0.09891 to 0.09891, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2893/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02893: val_loss did not improve from 0.09891\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2894/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02894: val_loss improved from 0.09891 to 0.09890, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2895/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02895: val_loss improved from 0.09890 to 0.09890, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2896/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02896: val_loss improved from 0.09890 to 0.09890, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2897/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02897: val_loss did not improve from 0.09890\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2898/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02898: val_loss did not improve from 0.09890\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2899/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02899: val_loss improved from 0.09890 to 0.09889, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2900/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02900: val_loss improved from 0.09889 to 0.09889, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2901/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02901: val_loss improved from 0.09889 to 0.09888, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2902/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02902: val_loss did not improve from 0.09888\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2903/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02903: val_loss improved from 0.09888 to 0.09888, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2904/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02904: val_loss improved from 0.09888 to 0.09888, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2905/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02905: val_loss improved from 0.09888 to 0.09887, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2906/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02906: val_loss did not improve from 0.09887\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2907/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02907: val_loss improved from 0.09887 to 0.09887, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2908/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992- ETA: 0s - loss: 0.099\n",
      "Epoch 02908: val_loss did not improve from 0.09887\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2909/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02909: val_loss did not improve from 0.09887\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2910/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02910: val_loss improved from 0.09887 to 0.09886, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2911/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02911: val_loss improved from 0.09886 to 0.09886, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2912/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02912: val_loss did not improve from 0.09886\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2913/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02913: val_loss improved from 0.09886 to 0.09886, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2914/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02914: val_loss improved from 0.09886 to 0.09885, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2915/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02915: val_loss improved from 0.09885 to 0.09885, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2916/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02916: val_loss improved from 0.09885 to 0.09885, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2917/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02917: val_loss improved from 0.09885 to 0.09885, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0992 - val_loss: 0.0988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2918/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02918: val_loss did not improve from 0.09885\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 2919/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02919: val_loss improved from 0.09885 to 0.09884, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2920/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0992\n",
      "Epoch 02920: val_loss did not improve from 0.09884\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0992 - val_loss: 0.0988\n",
      "Epoch 2921/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02921: val_loss improved from 0.09884 to 0.09884, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2922/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02922: val_loss improved from 0.09884 to 0.09884, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2923/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02923: val_loss improved from 0.09884 to 0.09883, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2924/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02924: val_loss did not improve from 0.09883\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2925/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02925: val_loss improved from 0.09883 to 0.09883, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2926/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02926: val_loss did not improve from 0.09883\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2927/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02927: val_loss did not improve from 0.09883\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2928/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02928: val_loss did not improve from 0.09883\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2929/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02929: val_loss improved from 0.09883 to 0.09882, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2930/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02930: val_loss improved from 0.09882 to 0.09882, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2931/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02931: val_loss did not improve from 0.09882\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2932/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02932: val_loss improved from 0.09882 to 0.09881, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2933/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02933: val_loss improved from 0.09881 to 0.09881, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2934/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02934: val_loss did not improve from 0.09881\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2935/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02935: val_loss improved from 0.09881 to 0.09881, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2936/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02936: val_loss did not improve from 0.09881\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2937/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02937: val_loss did not improve from 0.09881\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2938/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02938: val_loss improved from 0.09881 to 0.09880, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2939/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02939: val_loss did not improve from 0.09880\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2940/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02940: val_loss improved from 0.09880 to 0.09880, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2941/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02941: val_loss improved from 0.09880 to 0.09879, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2942/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02942: val_loss did not improve from 0.09879\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2943/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02943: val_loss improved from 0.09879 to 0.09879, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2944/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02944: val_loss improved from 0.09879 to 0.09879, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2945/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02945: val_loss improved from 0.09879 to 0.09879, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2946/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02946: val_loss did not improve from 0.09879\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2947/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02947: val_loss improved from 0.09879 to 0.09878, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2948/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02948: val_loss did not improve from 0.09878\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2949/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02949: val_loss improved from 0.09878 to 0.09878, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2950/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02950: val_loss improved from 0.09878 to 0.09877, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2951/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02951: val_loss did not improve from 0.09877\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2952/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02952: val_loss did not improve from 0.09877\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2953/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02953: val_loss improved from 0.09877 to 0.09877, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2954/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02954: val_loss improved from 0.09877 to 0.09876, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2955/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02955: val_loss improved from 0.09876 to 0.09876, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2956/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02956: val_loss improved from 0.09876 to 0.09876, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2957/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02957: val_loss did not improve from 0.09876\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2958/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02958: val_loss did not improve from 0.09876\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2959/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02959: val_loss improved from 0.09876 to 0.09875, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2960/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02960: val_loss improved from 0.09875 to 0.09875, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2961/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02961: val_loss did not improve from 0.09875\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2962/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02962: val_loss improved from 0.09875 to 0.09875, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0991 - val_loss: 0.0987\n",
      "Epoch 2963/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02963: val_loss did not improve from 0.09875\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0991 - val_loss: 0.0987\n",
      "Epoch 2964/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02964: val_loss did not improve from 0.09875\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0991 - val_loss: 0.0988\n",
      "Epoch 2965/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02965: val_loss did not improve from 0.09875\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0991 - val_loss: 0.0987\n",
      "Epoch 2966/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02966: val_loss improved from 0.09875 to 0.09874, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0991 - val_loss: 0.0987\n",
      "Epoch 2967/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02967: val_loss improved from 0.09874 to 0.09873, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0991 - val_loss: 0.0987\n",
      "Epoch 2968/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991\n",
      "Epoch 02968: val_loss did not improve from 0.09873\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0991 - val_loss: 0.0987\n",
      "Epoch 2969/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02969: val_loss improved from 0.09873 to 0.09873, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2970/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02970: val_loss did not improve from 0.09873\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2971/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02971: val_loss improved from 0.09873 to 0.09873, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2972/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02972: val_loss did not improve from 0.09873\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2973/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02973: val_loss improved from 0.09873 to 0.09873, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2974/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02974: val_loss improved from 0.09873 to 0.09872, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2975/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02975: val_loss did not improve from 0.09872\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2976/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02976: val_loss did not improve from 0.09872\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2977/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02977: val_loss improved from 0.09872 to 0.09871, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2978/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02978: val_loss improved from 0.09871 to 0.09871, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2979/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02979: val_loss did not improve from 0.09871\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2980/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02980: val_loss improved from 0.09871 to 0.09871, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2981/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02981: val_loss improved from 0.09871 to 0.09871, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2982/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02982: val_loss did not improve from 0.09871\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2983/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02983: val_loss improved from 0.09871 to 0.09870, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2984/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02984: val_loss improved from 0.09870 to 0.09870, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0990 - val_loss: 0.0987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2985/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02985: val_loss did not improve from 0.09870\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2986/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02986: val_loss improved from 0.09870 to 0.09869, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2987/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02987: val_loss did not improve from 0.09869\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2988/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02988: val_loss did not improve from 0.09869\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2989/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02989: val_loss improved from 0.09869 to 0.09869, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2990/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02990: val_loss did not improve from 0.09869\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2991/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02991: val_loss improved from 0.09869 to 0.09868, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2992/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02992: val_loss did not improve from 0.09868\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2993/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02993: val_loss improved from 0.09868 to 0.09868, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2994/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02994: val_loss improved from 0.09868 to 0.09868, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2995/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02995: val_loss improved from 0.09868 to 0.09867, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2996/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02996: val_loss did not improve from 0.09867\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2997/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02997: val_loss improved from 0.09867 to 0.09867, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2998/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02998: val_loss improved from 0.09867 to 0.09867, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 2999/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 02999: val_loss did not improve from 0.09867\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 3000/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03000: val_loss improved from 0.09867 to 0.09866, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 3001/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03001: val_loss improved from 0.09866 to 0.09866, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 3002/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03002: val_loss improved from 0.09866 to 0.09866, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 3003/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03003: val_loss did not improve from 0.09866\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 3004/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990- ETA: 0s - loss: 0.0\n",
      "Epoch 03004: val_loss improved from 0.09866 to 0.09866, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 3005/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03005: val_loss did not improve from 0.09866\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 3006/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03006: val_loss improved from 0.09866 to 0.09865, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 3007/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03007: val_loss improved from 0.09865 to 0.09865, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0990 - val_loss: 0.0986\n",
      "Epoch 3008/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03008: val_loss did not improve from 0.09865\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0990 - val_loss: 0.0987\n",
      "Epoch 3009/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03009: val_loss improved from 0.09865 to 0.09864, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0990 - val_loss: 0.0986\n",
      "Epoch 3010/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03010: val_loss did not improve from 0.09864\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0990 - val_loss: 0.0986\n",
      "Epoch 3011/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03011: val_loss improved from 0.09864 to 0.09864, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0990 - val_loss: 0.0986\n",
      "Epoch 3012/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03012: val_loss did not improve from 0.09864\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0990 - val_loss: 0.0986\n",
      "Epoch 3013/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03013: val_loss did not improve from 0.09864\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0990 - val_loss: 0.0986\n",
      "Epoch 3014/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03014: val_loss improved from 0.09864 to 0.09863, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0990 - val_loss: 0.0986\n",
      "Epoch 3015/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03015: val_loss did not improve from 0.09863\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0990 - val_loss: 0.0986\n",
      "Epoch 3016/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0990\n",
      "Epoch 03016: val_loss did not improve from 0.09863\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0990 - val_loss: 0.0986\n",
      "Epoch 3017/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03017: val_loss improved from 0.09863 to 0.09862, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3018/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03018: val_loss did not improve from 0.09862\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3019/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03019: val_loss improved from 0.09862 to 0.09862, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3020/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03020: val_loss improved from 0.09862 to 0.09862, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3021/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03021: val_loss improved from 0.09862 to 0.09862, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3022/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03022: val_loss did not improve from 0.09862\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3023/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03023: val_loss improved from 0.09862 to 0.09861, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3024/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03024: val_loss improved from 0.09861 to 0.09861, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3025/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03025: val_loss improved from 0.09861 to 0.09861, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3026/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03026: val_loss improved from 0.09861 to 0.09861, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3027/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03027: val_loss improved from 0.09861 to 0.09860, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3028/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03028: val_loss improved from 0.09860 to 0.09860, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3029/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03029: val_loss improved from 0.09860 to 0.09860, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3030/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03030: val_loss improved from 0.09860 to 0.09860, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3031/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03031: val_loss improved from 0.09860 to 0.09859, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3032/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03032: val_loss did not improve from 0.09859\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3033/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03033: val_loss did not improve from 0.09859\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3034/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03034: val_loss improved from 0.09859 to 0.09859, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3035/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03035: val_loss improved from 0.09859 to 0.09859, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3036/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03036: val_loss improved from 0.09859 to 0.09858, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3037/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03037: val_loss did not improve from 0.09858\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3038/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03038: val_loss improved from 0.09858 to 0.09858, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3039/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03039: val_loss improved from 0.09858 to 0.09858, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3040/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03040: val_loss improved from 0.09858 to 0.09857, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3041/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03041: val_loss did not improve from 0.09857\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3042/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03042: val_loss improved from 0.09857 to 0.09857, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3043/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03043: val_loss improved from 0.09857 to 0.09857, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3044/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03044: val_loss did not improve from 0.09857\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3045/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03045: val_loss improved from 0.09857 to 0.09856, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3046/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03046: val_loss improved from 0.09856 to 0.09856, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3047/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03047: val_loss improved from 0.09856 to 0.09856, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3048/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03048: val_loss did not improve from 0.09856\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3049/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03049: val_loss improved from 0.09856 to 0.09856, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0989 - val_loss: 0.0986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3050/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03050: val_loss improved from 0.09856 to 0.09855, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3051/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03051: val_loss improved from 0.09855 to 0.09855, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3052/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03052: val_loss did not improve from 0.09855\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0989 - val_loss: 0.0986\n",
      "Epoch 3053/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03053: val_loss improved from 0.09855 to 0.09855, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3054/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03054: val_loss did not improve from 0.09855\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3055/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03055: val_loss improved from 0.09855 to 0.09854, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3056/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03056: val_loss improved from 0.09854 to 0.09854, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3057/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03057: val_loss improved from 0.09854 to 0.09854, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3058/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03058: val_loss improved from 0.09854 to 0.09854, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3059/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03059: val_loss did not improve from 0.09854\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3060/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03060: val_loss improved from 0.09854 to 0.09853, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3061/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03061: val_loss did not improve from 0.09853\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3062/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03062: val_loss did not improve from 0.09853\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3063/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03063: val_loss improved from 0.09853 to 0.09852, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3064/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03064: val_loss did not improve from 0.09852\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3065/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03065: val_loss did not improve from 0.09852\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3066/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03066: val_loss improved from 0.09852 to 0.09852, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3067/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03067: val_loss did not improve from 0.09852\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3068/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0989\n",
      "Epoch 03068: val_loss improved from 0.09852 to 0.09851, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0989 - val_loss: 0.0985\n",
      "Epoch 3069/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03069: val_loss improved from 0.09851 to 0.09851, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3070/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03070: val_loss improved from 0.09851 to 0.09851, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3071/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03071: val_loss improved from 0.09851 to 0.09851, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3072/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03072: val_loss did not improve from 0.09851\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3073/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03073: val_loss improved from 0.09851 to 0.09850, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3074/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03074: val_loss did not improve from 0.09850\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3075/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03075: val_loss did not improve from 0.09850\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3076/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03076: val_loss improved from 0.09850 to 0.09850, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3077/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03077: val_loss improved from 0.09850 to 0.09849, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3078/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03078: val_loss did not improve from 0.09849\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3079/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03079: val_loss did not improve from 0.09849\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3080/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03080: val_loss improved from 0.09849 to 0.09849, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3081/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03081: val_loss did not improve from 0.09849\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3082/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03082: val_loss did not improve from 0.09849\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3083/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03083: val_loss improved from 0.09849 to 0.09848, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3084/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03084: val_loss did not improve from 0.09848\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3085/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03085: val_loss did not improve from 0.09848\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3086/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03086: val_loss did not improve from 0.09848\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3087/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03087: val_loss improved from 0.09848 to 0.09847, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3088/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03088: val_loss improved from 0.09847 to 0.09847, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3089/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03089: val_loss did not improve from 0.09847\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3090/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03090: val_loss did not improve from 0.09847\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3091/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03091: val_loss did not improve from 0.09847\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3092/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03092: val_loss improved from 0.09847 to 0.09847, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3093/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03093: val_loss did not improve from 0.09847\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3094/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03094: val_loss improved from 0.09847 to 0.09846, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3095/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03095: val_loss improved from 0.09846 to 0.09846, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3096/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03096: val_loss improved from 0.09846 to 0.09846, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3097/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03097: val_loss improved from 0.09846 to 0.09845, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3098/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03098: val_loss improved from 0.09845 to 0.09845, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3099/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03099: val_loss did not improve from 0.09845\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3100/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03100: val_loss improved from 0.09845 to 0.09845, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3101/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03101: val_loss improved from 0.09845 to 0.09845, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3102/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03102: val_loss improved from 0.09845 to 0.09844, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3103/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03103: val_loss did not improve from 0.09844\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0988 - val_loss: 0.0985\n",
      "Epoch 3104/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03104: val_loss improved from 0.09844 to 0.09844, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3105/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03105: val_loss improved from 0.09844 to 0.09844, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3106/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03106: val_loss improved from 0.09844 to 0.09843, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3107/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03107: val_loss did not improve from 0.09843\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3108/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03108: val_loss improved from 0.09843 to 0.09843, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3109/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03109: val_loss improved from 0.09843 to 0.09843, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3110/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03110: val_loss did not improve from 0.09843\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3111/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03111: val_loss did not improve from 0.09843\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3112/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03112: val_loss improved from 0.09843 to 0.09842, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3113/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03113: val_loss improved from 0.09842 to 0.09842, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3114/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03114: val_loss did not improve from 0.09842\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3115/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03115: val_loss did not improve from 0.09842\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3116/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03116: val_loss improved from 0.09842 to 0.09841, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3117/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03117: val_loss improved from 0.09841 to 0.09841, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3118/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03118: val_loss improved from 0.09841 to 0.09841, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3119/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03119: val_loss improved from 0.09841 to 0.09840, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3120/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03120: val_loss did not improve from 0.09840\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3121/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03121: val_loss did not improve from 0.09840\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3122/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03122: val_loss improved from 0.09840 to 0.09840, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3123/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0988\n",
      "Epoch 03123: val_loss improved from 0.09840 to 0.09840, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0988 - val_loss: 0.0984\n",
      "Epoch 3124/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03124: val_loss did not improve from 0.09840\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3125/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03125: val_loss improved from 0.09840 to 0.09839, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3126/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03126: val_loss improved from 0.09839 to 0.09839, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3127/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03127: val_loss did not improve from 0.09839\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3128/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03128: val_loss did not improve from 0.09839\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3129/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03129: val_loss improved from 0.09839 to 0.09839, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3130/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03130: val_loss improved from 0.09839 to 0.09839, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3131/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03131: val_loss improved from 0.09839 to 0.09838, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3132/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03132: val_loss improved from 0.09838 to 0.09838, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3133/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03133: val_loss did not improve from 0.09838\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3134/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03134: val_loss did not improve from 0.09838\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3135/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03135: val_loss improved from 0.09838 to 0.09837, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3136/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03136: val_loss improved from 0.09837 to 0.09837, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3137/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03137: val_loss did not improve from 0.09837\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3138/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03138: val_loss improved from 0.09837 to 0.09837, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3139/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987- ETA: 0s - loss: 0.0\n",
      "Epoch 03139: val_loss improved from 0.09837 to 0.09836, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3140/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03140: val_loss improved from 0.09836 to 0.09836, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3141/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03141: val_loss did not improve from 0.09836\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3142/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03142: val_loss improved from 0.09836 to 0.09836, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3143/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03143: val_loss improved from 0.09836 to 0.09836, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3144/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03144: val_loss improved from 0.09836 to 0.09835, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3145/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03145: val_loss improved from 0.09835 to 0.09835, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3146/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03146: val_loss did not improve from 0.09835\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0987 - val_loss: 0.0984\n",
      "Epoch 3147/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03147: val_loss improved from 0.09835 to 0.09835, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3148/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03148: val_loss did not improve from 0.09835\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3149/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03149: val_loss improved from 0.09835 to 0.09834, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3150/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03150: val_loss improved from 0.09834 to 0.09834, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3151/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03151: val_loss did not improve from 0.09834\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3152/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03152: val_loss improved from 0.09834 to 0.09834, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3153/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03153: val_loss improved from 0.09834 to 0.09834, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3154/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03154: val_loss improved from 0.09834 to 0.09833, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3155/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03155: val_loss did not improve from 0.09833\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3156/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03156: val_loss improved from 0.09833 to 0.09833, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3157/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03157: val_loss improved from 0.09833 to 0.09832, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3158/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03158: val_loss did not improve from 0.09832\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3159/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03159: val_loss improved from 0.09832 to 0.09832, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3160/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03160: val_loss improved from 0.09832 to 0.09832, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3161/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03161: val_loss improved from 0.09832 to 0.09832, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3162/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03162: val_loss improved from 0.09832 to 0.09832, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3163/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03163: val_loss improved from 0.09832 to 0.09831, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3164/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03164: val_loss improved from 0.09831 to 0.09831, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3165/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03165: val_loss did not improve from 0.09831\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3166/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03166: val_loss did not improve from 0.09831\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3167/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03167: val_loss improved from 0.09831 to 0.09831, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3168/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03168: val_loss did not improve from 0.09831\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3169/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03169: val_loss did not improve from 0.09831\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3170/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987\n",
      "Epoch 03170: val_loss improved from 0.09831 to 0.09830, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0987 - val_loss: 0.0983\n",
      "Epoch 3171/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03171: val_loss did not improve from 0.09830\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3172/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03172: val_loss improved from 0.09830 to 0.09829, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3173/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03173: val_loss did not improve from 0.09829\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3174/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03174: val_loss did not improve from 0.09829\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3175/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03175: val_loss improved from 0.09829 to 0.09829, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3176/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03176: val_loss improved from 0.09829 to 0.09829, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3177/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03177: val_loss did not improve from 0.09829\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3178/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03178: val_loss improved from 0.09829 to 0.09828, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3179/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03179: val_loss improved from 0.09828 to 0.09828, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3180/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03180: val_loss improved from 0.09828 to 0.09828, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0986 - val_loss: 0.0983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3181/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03181: val_loss did not improve from 0.09828\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3182/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03182: val_loss improved from 0.09828 to 0.09827, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3183/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03183: val_loss improved from 0.09827 to 0.09827, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3184/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03184: val_loss improved from 0.09827 to 0.09827, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3185/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03185: val_loss improved from 0.09827 to 0.09827, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3186/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03186: val_loss did not improve from 0.09827\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3187/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03187: val_loss improved from 0.09827 to 0.09826, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3188/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03188: val_loss did not improve from 0.09826\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3189/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03189: val_loss improved from 0.09826 to 0.09826, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3190/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03190: val_loss did not improve from 0.09826\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3191/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03191: val_loss improved from 0.09826 to 0.09825, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3192/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03192: val_loss improved from 0.09825 to 0.09825, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3193/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03193: val_loss improved from 0.09825 to 0.09825, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3194/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03194: val_loss did not improve from 0.09825\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 3195/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03195: val_loss improved from 0.09825 to 0.09825, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3196/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03196: val_loss improved from 0.09825 to 0.09824, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3197/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03197: val_loss did not improve from 0.09824\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3198/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03198: val_loss did not improve from 0.09824\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3199/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03199: val_loss improved from 0.09824 to 0.09824, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3200/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03200: val_loss improved from 0.09824 to 0.09823, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3201/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03201: val_loss did not improve from 0.09823\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3202/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03202: val_loss did not improve from 0.09823\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3203/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03203: val_loss did not improve from 0.09823\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3204/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03204: val_loss improved from 0.09823 to 0.09822, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3205/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03205: val_loss did not improve from 0.09822\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3206/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03206: val_loss improved from 0.09822 to 0.09822, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3207/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03207: val_loss improved from 0.09822 to 0.09822, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3208/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03208: val_loss did not improve from 0.09822\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3209/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03209: val_loss improved from 0.09822 to 0.09822, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3210/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03210: val_loss improved from 0.09822 to 0.09821, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3211/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03211: val_loss did not improve from 0.09821\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3212/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03212: val_loss did not improve from 0.09821\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3213/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03213: val_loss improved from 0.09821 to 0.09821, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3214/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03214: val_loss improved from 0.09821 to 0.09820, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3215/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03215: val_loss did not improve from 0.09820\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3216/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03216: val_loss improved from 0.09820 to 0.09820, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3217/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03217: val_loss did not improve from 0.09820\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3218/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0986\n",
      "Epoch 03218: val_loss improved from 0.09820 to 0.09820, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0986 - val_loss: 0.0982\n",
      "Epoch 3219/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03219: val_loss improved from 0.09820 to 0.09819, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3220/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03220: val_loss did not improve from 0.09819\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3221/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03221: val_loss improved from 0.09819 to 0.09819, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3222/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03222: val_loss improved from 0.09819 to 0.09819, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3223/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03223: val_loss did not improve from 0.09819\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3224/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03224: val_loss improved from 0.09819 to 0.09818, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3225/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03225: val_loss improved from 0.09818 to 0.09818, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3226/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03226: val_loss did not improve from 0.09818\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3227/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03227: val_loss improved from 0.09818 to 0.09818, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3228/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03228: val_loss improved from 0.09818 to 0.09818, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3229/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03229: val_loss improved from 0.09818 to 0.09817, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3230/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03230: val_loss improved from 0.09817 to 0.09817, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3231/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03231: val_loss did not improve from 0.09817\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3232/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03232: val_loss improved from 0.09817 to 0.09817, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3233/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03233: val_loss improved from 0.09817 to 0.09816, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3234/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03234: val_loss improved from 0.09816 to 0.09816, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3235/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03235: val_loss improved from 0.09816 to 0.09816, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3236/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03236: val_loss did not improve from 0.09816\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3237/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03237: val_loss did not improve from 0.09816\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3238/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03238: val_loss did not improve from 0.09816\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3239/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03239: val_loss improved from 0.09816 to 0.09816, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3240/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03240: val_loss did not improve from 0.09816\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0985 - val_loss: 0.0982\n",
      "Epoch 3241/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03241: val_loss improved from 0.09816 to 0.09815, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3242/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03242: val_loss improved from 0.09815 to 0.09815, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3243/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03243: val_loss did not improve from 0.09815\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3244/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03244: val_loss improved from 0.09815 to 0.09815, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3245/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03245: val_loss improved from 0.09815 to 0.09814, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3246/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03246: val_loss improved from 0.09814 to 0.09814, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0985 - val_loss: 0.0981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3247/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03247: val_loss improved from 0.09814 to 0.09814, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3248/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03248: val_loss did not improve from 0.09814\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3249/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03249: val_loss improved from 0.09814 to 0.09813, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3250/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03250: val_loss improved from 0.09813 to 0.09813, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3251/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03251: val_loss improved from 0.09813 to 0.09813, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3252/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03252: val_loss improved from 0.09813 to 0.09813, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3253/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03253: val_loss improved from 0.09813 to 0.09812, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3254/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03254: val_loss improved from 0.09812 to 0.09812, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3255/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03255: val_loss did not improve from 0.09812\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3256/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03256: val_loss improved from 0.09812 to 0.09812, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3257/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03257: val_loss improved from 0.09812 to 0.09811, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3258/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03258: val_loss did not improve from 0.09811\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3259/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03259: val_loss improved from 0.09811 to 0.09811, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3260/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985- ETA: 0s - loss: 0.098\n",
      "Epoch 03260: val_loss improved from 0.09811 to 0.09811, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3261/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03261: val_loss improved from 0.09811 to 0.09811, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3262/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03262: val_loss did not improve from 0.09811\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3263/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03263: val_loss improved from 0.09811 to 0.09810, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3264/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03264: val_loss improved from 0.09810 to 0.09810, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3265/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0985\n",
      "Epoch 03265: val_loss improved from 0.09810 to 0.09810, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0985 - val_loss: 0.0981\n",
      "Epoch 3266/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03266: val_loss improved from 0.09810 to 0.09809, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3267/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03267: val_loss improved from 0.09809 to 0.09809, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3268/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03268: val_loss did not improve from 0.09809\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3269/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03269: val_loss improved from 0.09809 to 0.09809, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3270/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03270: val_loss improved from 0.09809 to 0.09808, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3271/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03271: val_loss improved from 0.09808 to 0.09808, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3272/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03272: val_loss improved from 0.09808 to 0.09808, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3273/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03273: val_loss did not improve from 0.09808\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3274/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03274: val_loss improved from 0.09808 to 0.09807, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3275/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03275: val_loss did not improve from 0.09807\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3276/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03276: val_loss did not improve from 0.09807\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3277/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03277: val_loss improved from 0.09807 to 0.09807, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3278/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03278: val_loss improved from 0.09807 to 0.09807, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0984 - val_loss: 0.0981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3279/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03279: val_loss did not improve from 0.09807\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3280/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03280: val_loss improved from 0.09807 to 0.09806, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3281/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03281: val_loss improved from 0.09806 to 0.09806, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3282/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03282: val_loss improved from 0.09806 to 0.09806, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3283/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03283: val_loss improved from 0.09806 to 0.09806, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3284/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03284: val_loss improved from 0.09806 to 0.09806, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3285/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03285: val_loss did not improve from 0.09806\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3286/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03286: val_loss improved from 0.09806 to 0.09805, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3287/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03287: val_loss improved from 0.09805 to 0.09805, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3288/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03288: val_loss improved from 0.09805 to 0.09805, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3289/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03289: val_loss did not improve from 0.09805\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0984 - val_loss: 0.0981\n",
      "Epoch 3290/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03290: val_loss improved from 0.09805 to 0.09804, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3291/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03291: val_loss improved from 0.09804 to 0.09804, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3292/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03292: val_loss did not improve from 0.09804\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3293/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03293: val_loss improved from 0.09804 to 0.09804, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3294/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03294: val_loss improved from 0.09804 to 0.09804, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3295/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03295: val_loss improved from 0.09804 to 0.09803, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3296/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03296: val_loss improved from 0.09803 to 0.09803, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3297/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03297: val_loss improved from 0.09803 to 0.09803, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3298/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03298: val_loss did not improve from 0.09803\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3299/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03299: val_loss improved from 0.09803 to 0.09802, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3300/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03300: val_loss improved from 0.09802 to 0.09802, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3301/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03301: val_loss improved from 0.09802 to 0.09802, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3302/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03302: val_loss did not improve from 0.09802\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3303/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03303: val_loss improved from 0.09802 to 0.09802, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3304/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03304: val_loss did not improve from 0.09802\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3305/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03305: val_loss improved from 0.09802 to 0.09801, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3306/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03306: val_loss did not improve from 0.09801\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3307/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03307: val_loss improved from 0.09801 to 0.09801, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3308/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03308: val_loss improved from 0.09801 to 0.09801, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3309/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03309: val_loss did not improve from 0.09801\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3310/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03310: val_loss improved from 0.09801 to 0.09801, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3311/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03311: val_loss improved from 0.09801 to 0.09800, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3312/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03312: val_loss improved from 0.09800 to 0.09800, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3313/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03313: val_loss improved from 0.09800 to 0.09799, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3314/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0984\n",
      "Epoch 03314: val_loss improved from 0.09799 to 0.09799, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0984 - val_loss: 0.0980\n",
      "Epoch 3315/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03315: val_loss did not improve from 0.09799\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3316/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03316: val_loss did not improve from 0.09799\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3317/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03317: val_loss improved from 0.09799 to 0.09798, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3318/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03318: val_loss did not improve from 0.09798\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3319/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03319: val_loss improved from 0.09798 to 0.09798, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3320/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03320: val_loss improved from 0.09798 to 0.09798, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3321/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03321: val_loss did not improve from 0.09798\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3322/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03322: val_loss did not improve from 0.09798\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3323/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03323: val_loss improved from 0.09798 to 0.09797, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3324/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03324: val_loss improved from 0.09797 to 0.09797, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3325/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03325: val_loss improved from 0.09797 to 0.09797, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3326/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03326: val_loss improved from 0.09797 to 0.09797, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3327/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03327: val_loss improved from 0.09797 to 0.09797, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3328/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03328: val_loss improved from 0.09797 to 0.09796, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3329/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03329: val_loss improved from 0.09796 to 0.09796, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3330/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03330: val_loss did not improve from 0.09796\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3331/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03331: val_loss did not improve from 0.09796\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3332/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03332: val_loss did not improve from 0.09796\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3333/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03333: val_loss improved from 0.09796 to 0.09795, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3334/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03334: val_loss improved from 0.09795 to 0.09795, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3335/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03335: val_loss did not improve from 0.09795\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3336/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03336: val_loss did not improve from 0.09795\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0983 - val_loss: 0.0980\n",
      "Epoch 3337/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03337: val_loss improved from 0.09795 to 0.09794, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3338/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03338: val_loss did not improve from 0.09794\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3339/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03339: val_loss did not improve from 0.09794\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3340/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03340: val_loss improved from 0.09794 to 0.09794, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3341/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03341: val_loss improved from 0.09794 to 0.09794, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3342/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03342: val_loss did not improve from 0.09794\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3343/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03343: val_loss improved from 0.09794 to 0.09794, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0983 - val_loss: 0.0979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3344/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03344: val_loss did not improve from 0.09794\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3345/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03345: val_loss improved from 0.09794 to 0.09793, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3346/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03346: val_loss improved from 0.09793 to 0.09793, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3347/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03347: val_loss improved from 0.09793 to 0.09793, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3348/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03348: val_loss improved from 0.09793 to 0.09793, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3349/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03349: val_loss improved from 0.09793 to 0.09792, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3350/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03350: val_loss did not improve from 0.09792\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3351/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03351: val_loss improved from 0.09792 to 0.09791, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3352/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03352: val_loss did not improve from 0.09791\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3353/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03353: val_loss improved from 0.09791 to 0.09791, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3354/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03354: val_loss improved from 0.09791 to 0.09791, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3355/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03355: val_loss did not improve from 0.09791\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3356/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03356: val_loss improved from 0.09791 to 0.09790, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3357/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03357: val_loss did not improve from 0.09790\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3358/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03358: val_loss improved from 0.09790 to 0.09790, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3359/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03359: val_loss did not improve from 0.09790\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3360/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03360: val_loss did not improve from 0.09790\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3361/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03361: val_loss improved from 0.09790 to 0.09790, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3362/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03362: val_loss did not improve from 0.09790\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3363/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03363: val_loss did not improve from 0.09790\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3364/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0983\n",
      "Epoch 03364: val_loss improved from 0.09790 to 0.09789, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0983 - val_loss: 0.0979\n",
      "Epoch 3365/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03365: val_loss improved from 0.09789 to 0.09789, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3366/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03366: val_loss improved from 0.09789 to 0.09789, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3367/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03367: val_loss did not improve from 0.09789\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3368/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03368: val_loss improved from 0.09789 to 0.09789, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3369/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03369: val_loss improved from 0.09789 to 0.09788, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3370/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03370: val_loss did not improve from 0.09788\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3371/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03371: val_loss did not improve from 0.09788\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3372/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03372: val_loss did not improve from 0.09788\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3373/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03373: val_loss improved from 0.09788 to 0.09788, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3374/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03374: val_loss did not improve from 0.09788\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3375/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03375: val_loss improved from 0.09788 to 0.09787, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3376/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03376: val_loss improved from 0.09787 to 0.09787, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3377/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03377: val_loss improved from 0.09787 to 0.09787, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3378/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03378: val_loss did not improve from 0.09787\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3379/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03379: val_loss improved from 0.09787 to 0.09786, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3380/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03380: val_loss improved from 0.09786 to 0.09786, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3381/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03381: val_loss did not improve from 0.09786\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3382/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03382: val_loss improved from 0.09786 to 0.09786, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3383/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982- ETA: 0s - loss: 0.\n",
      "Epoch 03383: val_loss improved from 0.09786 to 0.09785, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3384/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03384: val_loss did not improve from 0.09785\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3385/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03385: val_loss did not improve from 0.09785\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0982 - val_loss: 0.0979\n",
      "Epoch 3386/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03386: val_loss improved from 0.09785 to 0.09785, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3387/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03387: val_loss improved from 0.09785 to 0.09784, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3388/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03388: val_loss did not improve from 0.09784\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3389/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03389: val_loss did not improve from 0.09784\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3390/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03390: val_loss improved from 0.09784 to 0.09784, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3391/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03391: val_loss improved from 0.09784 to 0.09784, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3392/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03392: val_loss improved from 0.09784 to 0.09783, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3393/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03393: val_loss improved from 0.09783 to 0.09783, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3394/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03394: val_loss did not improve from 0.09783\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3395/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03395: val_loss did not improve from 0.09783\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3396/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03396: val_loss did not improve from 0.09783\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3397/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03397: val_loss improved from 0.09783 to 0.09783, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3398/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03398: val_loss did not improve from 0.09783\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3399/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03399: val_loss improved from 0.09783 to 0.09782, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3400/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03400: val_loss improved from 0.09782 to 0.09782, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3401/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03401: val_loss improved from 0.09782 to 0.09782, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3402/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03402: val_loss improved from 0.09782 to 0.09782, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3403/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03403: val_loss improved from 0.09782 to 0.09782, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3404/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03404: val_loss did not improve from 0.09782\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3405/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03405: val_loss did not improve from 0.09782\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3406/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03406: val_loss improved from 0.09782 to 0.09781, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3407/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03407: val_loss improved from 0.09781 to 0.09781, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3408/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03408: val_loss improved from 0.09781 to 0.09781, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3409/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03409: val_loss improved from 0.09781 to 0.09780, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0982 - val_loss: 0.0978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3410/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03410: val_loss improved from 0.09780 to 0.09780, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3411/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03411: val_loss improved from 0.09780 to 0.09780, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3412/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03412: val_loss did not improve from 0.09780\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3413/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03413: val_loss did not improve from 0.09780\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3414/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03414: val_loss improved from 0.09780 to 0.09779, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3415/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03415: val_loss did not improve from 0.09779\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3416/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03416: val_loss did not improve from 0.09779\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3417/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0982\n",
      "Epoch 03417: val_loss improved from 0.09779 to 0.09779, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0982 - val_loss: 0.0978\n",
      "Epoch 3418/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03418: val_loss improved from 0.09779 to 0.09779, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3419/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03419: val_loss improved from 0.09779 to 0.09779, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3420/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03420: val_loss improved from 0.09779 to 0.09778, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3421/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03421: val_loss improved from 0.09778 to 0.09778, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3422/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03422: val_loss improved from 0.09778 to 0.09778, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3423/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03423: val_loss did not improve from 0.09778\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3424/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03424: val_loss improved from 0.09778 to 0.09778, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3425/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03425: val_loss improved from 0.09778 to 0.09778, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3426/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03426: val_loss improved from 0.09778 to 0.09777, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3427/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03427: val_loss improved from 0.09777 to 0.09777, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3428/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03428: val_loss did not improve from 0.09777\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3429/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03429: val_loss improved from 0.09777 to 0.09777, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3430/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03430: val_loss improved from 0.09777 to 0.09777, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3431/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03431: val_loss did not improve from 0.09777\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3432/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03432: val_loss did not improve from 0.09777\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3433/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03433: val_loss improved from 0.09777 to 0.09776, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3434/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03434: val_loss did not improve from 0.09776\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3435/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03435: val_loss improved from 0.09776 to 0.09776, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3436/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03436: val_loss improved from 0.09776 to 0.09775, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3437/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03437: val_loss improved from 0.09775 to 0.09775, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3438/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03438: val_loss did not improve from 0.09775\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0981 - val_loss: 0.0978\n",
      "Epoch 3439/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03439: val_loss improved from 0.09775 to 0.09775, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3440/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03440: val_loss improved from 0.09775 to 0.09775, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3441/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03441: val_loss improved from 0.09775 to 0.09774, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3442/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03442: val_loss improved from 0.09774 to 0.09774, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3443/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03443: val_loss improved from 0.09774 to 0.09774, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3444/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03444: val_loss improved from 0.09774 to 0.09774, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3445/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03445: val_loss did not improve from 0.09774\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3446/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03446: val_loss did not improve from 0.09774\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3447/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03447: val_loss improved from 0.09774 to 0.09774, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3448/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03448: val_loss improved from 0.09774 to 0.09773, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3449/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03449: val_loss did not improve from 0.09773\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3450/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03450: val_loss did not improve from 0.09773\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3451/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03451: val_loss did not improve from 0.09773\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3452/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03452: val_loss improved from 0.09773 to 0.09773, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3453/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03453: val_loss did not improve from 0.09773\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3454/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03454: val_loss improved from 0.09773 to 0.09773, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3455/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03455: val_loss did not improve from 0.09773\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3456/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03456: val_loss improved from 0.09773 to 0.09772, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3457/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03457: val_loss did not improve from 0.09772\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3458/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03458: val_loss improved from 0.09772 to 0.09771, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3459/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03459: val_loss improved from 0.09771 to 0.09771, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3460/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03460: val_loss did not improve from 0.09771\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3461/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03461: val_loss did not improve from 0.09771\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3462/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03462: val_loss improved from 0.09771 to 0.09770, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3463/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03463: val_loss improved from 0.09770 to 0.09770, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3464/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03464: val_loss did not improve from 0.09770\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3465/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03465: val_loss improved from 0.09770 to 0.09770, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3466/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03466: val_loss improved from 0.09770 to 0.09770, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3467/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03467: val_loss did not improve from 0.09770\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3468/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03468: val_loss did not improve from 0.09770\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3469/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03469: val_loss improved from 0.09770 to 0.09769, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3470/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03470: val_loss improved from 0.09769 to 0.09769, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3471/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03471: val_loss improved from 0.09769 to 0.09769, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3472/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03472: val_loss improved from 0.09769 to 0.09769, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3473/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03473: val_loss did not improve from 0.09769\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3474/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0981\n",
      "Epoch 03474: val_loss did not improve from 0.09769\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0981 - val_loss: 0.0977\n",
      "Epoch 3475/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03475: val_loss improved from 0.09769 to 0.09769, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0980 - val_loss: 0.0977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3476/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03476: val_loss improved from 0.09769 to 0.09768, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3477/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03477: val_loss did not improve from 0.09768\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3478/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03478: val_loss improved from 0.09768 to 0.09768, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3479/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03479: val_loss improved from 0.09768 to 0.09768, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3480/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03480: val_loss improved from 0.09768 to 0.09767, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3481/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03481: val_loss did not improve from 0.09767\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3482/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03482: val_loss improved from 0.09767 to 0.09767, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3483/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03483: val_loss improved from 0.09767 to 0.09767, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3484/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03484: val_loss improved from 0.09767 to 0.09767, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3485/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03485: val_loss did not improve from 0.09767\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3486/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03486: val_loss improved from 0.09767 to 0.09766, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3487/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03487: val_loss did not improve from 0.09766\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3488/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03488: val_loss did not improve from 0.09766\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3489/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03489: val_loss did not improve from 0.09766\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3490/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03490: val_loss improved from 0.09766 to 0.09766, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3491/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03491: val_loss improved from 0.09766 to 0.09766, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3492/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03492: val_loss did not improve from 0.09766\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3493/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03493: val_loss improved from 0.09766 to 0.09765, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3494/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03494: val_loss improved from 0.09765 to 0.09765, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3495/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03495: val_loss did not improve from 0.09765\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3496/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03496: val_loss did not improve from 0.09765\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3497/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03497: val_loss improved from 0.09765 to 0.09764, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3498/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03498: val_loss did not improve from 0.09764\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0980 - val_loss: 0.0977\n",
      "Epoch 3499/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03499: val_loss improved from 0.09764 to 0.09764, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3500/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03500: val_loss improved from 0.09764 to 0.09764, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3501/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03501: val_loss improved from 0.09764 to 0.09764, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3502/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03502: val_loss improved from 0.09764 to 0.09764, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3503/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03503: val_loss did not improve from 0.09764\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3504/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03504: val_loss improved from 0.09764 to 0.09763, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3505/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03505: val_loss improved from 0.09763 to 0.09763, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3506/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03506: val_loss did not improve from 0.09763\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3507/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03507: val_loss did not improve from 0.09763\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3508/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03508: val_loss improved from 0.09763 to 0.09763, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0980 - val_loss: 0.0976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3509/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03509: val_loss improved from 0.09763 to 0.09763, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3510/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03510: val_loss did not improve from 0.09763\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3511/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03511: val_loss did not improve from 0.09763\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3512/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03512: val_loss improved from 0.09763 to 0.09762, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3513/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03513: val_loss did not improve from 0.09762\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3514/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03514: val_loss did not improve from 0.09762\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3515/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03515: val_loss improved from 0.09762 to 0.09762, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3516/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03516: val_loss improved from 0.09762 to 0.09761, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3517/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03517: val_loss improved from 0.09761 to 0.09761, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3518/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03518: val_loss did not improve from 0.09761\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3519/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03519: val_loss improved from 0.09761 to 0.09761, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3520/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03520: val_loss improved from 0.09761 to 0.09761, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3521/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03521: val_loss improved from 0.09761 to 0.09760, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3522/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03522: val_loss improved from 0.09760 to 0.09760, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3523/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03523: val_loss improved from 0.09760 to 0.09760, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3524/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03524: val_loss improved from 0.09760 to 0.09760, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3525/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03525: val_loss did not improve from 0.09760\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3526/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980- ETA: 0s - loss: 0.09\n",
      "Epoch 03526: val_loss improved from 0.09760 to 0.09760, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3527/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03527: val_loss improved from 0.09760 to 0.09759, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3528/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03528: val_loss improved from 0.09759 to 0.09759, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3529/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03529: val_loss did not improve from 0.09759\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3530/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03530: val_loss improved from 0.09759 to 0.09759, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3531/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03531: val_loss did not improve from 0.09759\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3532/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03532: val_loss did not improve from 0.09759\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3533/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03533: val_loss did not improve from 0.09759\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3534/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03534: val_loss improved from 0.09759 to 0.09758, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3535/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0980\n",
      "Epoch 03535: val_loss improved from 0.09758 to 0.09758, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0980 - val_loss: 0.0976\n",
      "Epoch 3536/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03536: val_loss did not improve from 0.09758\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3537/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03537: val_loss improved from 0.09758 to 0.09758, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3538/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03538: val_loss did not improve from 0.09758\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3539/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03539: val_loss did not improve from 0.09758\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3540/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03540: val_loss improved from 0.09758 to 0.09757, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3541/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03541: val_loss did not improve from 0.09757\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0979 - val_loss: 0.0976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3542/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03542: val_loss improved from 0.09757 to 0.09757, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3543/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03543: val_loss improved from 0.09757 to 0.09757, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3544/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03544: val_loss did not improve from 0.09757\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3545/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03545: val_loss improved from 0.09757 to 0.09756, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3546/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03546: val_loss did not improve from 0.09756\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3547/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03547: val_loss did not improve from 0.09756\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3548/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03548: val_loss did not improve from 0.09756\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3549/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03549: val_loss improved from 0.09756 to 0.09755, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3550/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03550: val_loss did not improve from 0.09755\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3551/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03551: val_loss improved from 0.09755 to 0.09755, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3552/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03552: val_loss improved from 0.09755 to 0.09755, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3553/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03553: val_loss did not improve from 0.09755\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3554/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03554: val_loss improved from 0.09755 to 0.09755, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3555/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03555: val_loss did not improve from 0.09755\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3556/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03556: val_loss did not improve from 0.09755\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3557/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03557: val_loss improved from 0.09755 to 0.09754, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3558/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03558: val_loss improved from 0.09754 to 0.09754, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3559/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979- ETA: 0s - loss: 0.0\n",
      "Epoch 03559: val_loss did not improve from 0.09754\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3560/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03560: val_loss did not improve from 0.09754\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3561/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03561: val_loss improved from 0.09754 to 0.09754, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3562/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03562: val_loss did not improve from 0.09754\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0979 - val_loss: 0.0976\n",
      "Epoch 3563/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03563: val_loss improved from 0.09754 to 0.09753, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3564/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03564: val_loss did not improve from 0.09753\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3565/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03565: val_loss improved from 0.09753 to 0.09753, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3566/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03566: val_loss did not improve from 0.09753\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3567/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03567: val_loss improved from 0.09753 to 0.09753, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3568/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03568: val_loss did not improve from 0.09753\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3569/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03569: val_loss improved from 0.09753 to 0.09753, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3570/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03570: val_loss did not improve from 0.09753\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3571/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03571: val_loss did not improve from 0.09753\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3572/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03572: val_loss improved from 0.09753 to 0.09752, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3573/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03573: val_loss improved from 0.09752 to 0.09752, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3574/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03574: val_loss improved from 0.09752 to 0.09752, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3575/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03575: val_loss improved from 0.09752 to 0.09752, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0979 - val_loss: 0.0975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3576/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03576: val_loss improved from 0.09752 to 0.09751, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3577/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03577: val_loss improved from 0.09751 to 0.09751, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3578/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03578: val_loss improved from 0.09751 to 0.09751, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3579/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03579: val_loss improved from 0.09751 to 0.09751, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3580/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03580: val_loss did not improve from 0.09751\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3581/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03581: val_loss did not improve from 0.09751\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3582/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03582: val_loss improved from 0.09751 to 0.09750, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3583/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03583: val_loss improved from 0.09750 to 0.09750, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3584/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03584: val_loss did not improve from 0.09750\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3585/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03585: val_loss did not improve from 0.09750\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3586/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03586: val_loss improved from 0.09750 to 0.09750, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3587/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03587: val_loss improved from 0.09750 to 0.09749, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3588/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03588: val_loss did not improve from 0.09749\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3589/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03589: val_loss improved from 0.09749 to 0.09749, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3590/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03590: val_loss did not improve from 0.09749\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3591/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03591: val_loss improved from 0.09749 to 0.09749, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3592/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0979\n",
      "Epoch 03592: val_loss did not improve from 0.09749\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0979 - val_loss: 0.0975\n",
      "Epoch 3593/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03593: val_loss improved from 0.09749 to 0.09749, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3594/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03594: val_loss improved from 0.09749 to 0.09748, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3595/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03595: val_loss improved from 0.09748 to 0.09748, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3596/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03596: val_loss improved from 0.09748 to 0.09748, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3597/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03597: val_loss improved from 0.09748 to 0.09748, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3598/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03598: val_loss did not improve from 0.09748\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3599/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03599: val_loss improved from 0.09748 to 0.09747, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3600/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03600: val_loss improved from 0.09747 to 0.09747, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3601/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03601: val_loss did not improve from 0.09747\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3602/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03602: val_loss improved from 0.09747 to 0.09747, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3603/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03603: val_loss did not improve from 0.09747\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3604/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03604: val_loss improved from 0.09747 to 0.09747, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3605/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03605: val_loss did not improve from 0.09747\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3606/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03606: val_loss improved from 0.09747 to 0.09746, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3607/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03607: val_loss did not improve from 0.09746\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3608/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03608: val_loss did not improve from 0.09746\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0978 - val_loss: 0.0975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3609/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03609: val_loss improved from 0.09746 to 0.09746, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3610/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03610: val_loss improved from 0.09746 to 0.09746, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3611/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03611: val_loss did not improve from 0.09746\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3612/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03612: val_loss did not improve from 0.09746\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3613/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03613: val_loss improved from 0.09746 to 0.09745, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3614/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03614: val_loss did not improve from 0.09745\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0978 - val_loss: 0.0975\n",
      "Epoch 3615/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03615: val_loss improved from 0.09745 to 0.09745, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3616/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03616: val_loss improved from 0.09745 to 0.09745, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3617/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03617: val_loss did not improve from 0.09745\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3618/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03618: val_loss did not improve from 0.09745\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3619/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03619: val_loss improved from 0.09745 to 0.09744, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3620/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03620: val_loss did not improve from 0.09744\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3621/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03621: val_loss did not improve from 0.09744\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3622/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03622: val_loss did not improve from 0.09744\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3623/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03623: val_loss improved from 0.09744 to 0.09744, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3624/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03624: val_loss improved from 0.09744 to 0.09743, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3625/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03625: val_loss did not improve from 0.09743\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3626/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03626: val_loss improved from 0.09743 to 0.09743, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3627/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03627: val_loss improved from 0.09743 to 0.09743, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3628/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03628: val_loss improved from 0.09743 to 0.09743, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3629/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03629: val_loss did not improve from 0.09743\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3630/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03630: val_loss did not improve from 0.09743\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3631/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03631: val_loss improved from 0.09743 to 0.09742, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3632/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03632: val_loss improved from 0.09742 to 0.09742, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3633/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03633: val_loss did not improve from 0.09742\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3634/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03634: val_loss improved from 0.09742 to 0.09742, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3635/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03635: val_loss improved from 0.09742 to 0.09742, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3636/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03636: val_loss improved from 0.09742 to 0.09741, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3637/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03637: val_loss did not improve from 0.09741\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3638/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03638: val_loss improved from 0.09741 to 0.09741, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3639/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03639: val_loss did not improve from 0.09741\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3640/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03640: val_loss did not improve from 0.09741\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3641/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03641: val_loss improved from 0.09741 to 0.09740, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3642/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03642: val_loss improved from 0.09740 to 0.09740, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3643/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03643: val_loss improved from 0.09740 to 0.09740, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3644/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03644: val_loss did not improve from 0.09740\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3645/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03645: val_loss improved from 0.09740 to 0.09740, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3646/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03646: val_loss improved from 0.09740 to 0.09740, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3647/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03647: val_loss improved from 0.09740 to 0.09739, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3648/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03648: val_loss improved from 0.09739 to 0.09739, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3649/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03649: val_loss did not improve from 0.09739\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3650/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03650: val_loss did not improve from 0.09739\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3651/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03651: val_loss improved from 0.09739 to 0.09739, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3652/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03652: val_loss improved from 0.09739 to 0.09739, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3653/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03653: val_loss did not improve from 0.09739\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3654/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0978\n",
      "Epoch 03654: val_loss did not improve from 0.09739\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0978 - val_loss: 0.0974\n",
      "Epoch 3655/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03655: val_loss improved from 0.09739 to 0.09738, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3656/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03656: val_loss improved from 0.09738 to 0.09738, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3657/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03657: val_loss improved from 0.09738 to 0.09738, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3658/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03658: val_loss did not improve from 0.09738\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3659/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03659: val_loss improved from 0.09738 to 0.09738, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3660/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03660: val_loss improved from 0.09738 to 0.09737, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3661/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03661: val_loss did not improve from 0.09737\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3662/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03662: val_loss improved from 0.09737 to 0.09737, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3663/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03663: val_loss did not improve from 0.09737\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3664/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03664: val_loss improved from 0.09737 to 0.09737, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3665/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03665: val_loss did not improve from 0.09737\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3666/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03666: val_loss did not improve from 0.09737\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3667/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03667: val_loss improved from 0.09737 to 0.09736, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3668/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03668: val_loss did not improve from 0.09736\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3669/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03669: val_loss improved from 0.09736 to 0.09736, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3670/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03670: val_loss improved from 0.09736 to 0.09736, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3671/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03671: val_loss improved from 0.09736 to 0.09736, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3672/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03672: val_loss did not improve from 0.09736\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3673/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03673: val_loss did not improve from 0.09736\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3674/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03674: val_loss did not improve from 0.09736\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3675/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03675: val_loss improved from 0.09736 to 0.09735, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3676/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03676: val_loss did not improve from 0.09735\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0977 - val_loss: 0.0974\n",
      "Epoch 3677/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03677: val_loss improved from 0.09735 to 0.09734, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3678/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03678: val_loss improved from 0.09734 to 0.09734, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3679/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03679: val_loss did not improve from 0.09734\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3680/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03680: val_loss improved from 0.09734 to 0.09734, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3681/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03681: val_loss did not improve from 0.09734\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3682/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03682: val_loss improved from 0.09734 to 0.09734, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3683/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03683: val_loss improved from 0.09734 to 0.09734, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3684/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03684: val_loss improved from 0.09734 to 0.09733, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3685/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03685: val_loss improved from 0.09733 to 0.09733, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3686/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03686: val_loss improved from 0.09733 to 0.09733, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3687/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03687: val_loss did not improve from 0.09733\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3688/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03688: val_loss did not improve from 0.09733\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3689/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03689: val_loss improved from 0.09733 to 0.09733, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3690/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03690: val_loss improved from 0.09733 to 0.09733, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3691/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03691: val_loss did not improve from 0.09733\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3692/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03692: val_loss improved from 0.09733 to 0.09732, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3693/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03693: val_loss improved from 0.09732 to 0.09732, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3694/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03694: val_loss did not improve from 0.09732\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3695/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03695: val_loss improved from 0.09732 to 0.09732, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3696/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03696: val_loss improved from 0.09732 to 0.09731, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3697/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03697: val_loss improved from 0.09731 to 0.09731, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3698/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03698: val_loss did not improve from 0.09731\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3699/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03699: val_loss did not improve from 0.09731\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3700/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03700: val_loss improved from 0.09731 to 0.09731, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3701/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03701: val_loss did not improve from 0.09731\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3702/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03702: val_loss improved from 0.09731 to 0.09731, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3703/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03703: val_loss did not improve from 0.09731\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3704/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03704: val_loss improved from 0.09731 to 0.09730, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3705/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03705: val_loss did not improve from 0.09730\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3706/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03706: val_loss improved from 0.09730 to 0.09730, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3707/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03707: val_loss did not improve from 0.09730\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0977 - val_loss: 0.0973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3708/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03708: val_loss improved from 0.09730 to 0.09730, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3709/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0977\n",
      "Epoch 03709: val_loss improved from 0.09730 to 0.09729, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0977 - val_loss: 0.0973\n",
      "Epoch 3710/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03710: val_loss improved from 0.09729 to 0.09729, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3711/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03711: val_loss improved from 0.09729 to 0.09729, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3712/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03712: val_loss did not improve from 0.09729\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3713/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03713: val_loss did not improve from 0.09729\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3714/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03714: val_loss improved from 0.09729 to 0.09729, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3715/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03715: val_loss improved from 0.09729 to 0.09728, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3716/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03716: val_loss improved from 0.09728 to 0.09728, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3717/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03717: val_loss improved from 0.09728 to 0.09728, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3718/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03718: val_loss did not improve from 0.09728\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3719/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03719: val_loss did not improve from 0.09728\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3720/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03720: val_loss improved from 0.09728 to 0.09728, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3721/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03721: val_loss did not improve from 0.09728\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3722/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03722: val_loss improved from 0.09728 to 0.09727, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3723/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03723: val_loss did not improve from 0.09727\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3724/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03724: val_loss improved from 0.09727 to 0.09727, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3725/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03725: val_loss did not improve from 0.09727\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3726/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03726: val_loss improved from 0.09727 to 0.09727, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3727/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03727: val_loss did not improve from 0.09727\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3728/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03728: val_loss improved from 0.09727 to 0.09727, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3729/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03729: val_loss improved from 0.09727 to 0.09726, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3730/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03730: val_loss did not improve from 0.09726\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3731/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03731: val_loss did not improve from 0.09726\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3732/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03732: val_loss improved from 0.09726 to 0.09726, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3733/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03733: val_loss did not improve from 0.09726\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3734/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03734: val_loss improved from 0.09726 to 0.09725, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3735/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03735: val_loss improved from 0.09725 to 0.09725, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3736/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03736: val_loss did not improve from 0.09725\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3737/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03737: val_loss did not improve from 0.09725\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0976 - val_loss: 0.0973\n",
      "Epoch 3738/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03738: val_loss improved from 0.09725 to 0.09724, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3739/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03739: val_loss improved from 0.09724 to 0.09724, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3740/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03740: val_loss improved from 0.09724 to 0.09724, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0976 - val_loss: 0.0972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3741/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03741: val_loss improved from 0.09724 to 0.09724, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3742/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03742: val_loss improved from 0.09724 to 0.09724, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3743/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03743: val_loss did not improve from 0.09724\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3744/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03744: val_loss improved from 0.09724 to 0.09724, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3745/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03745: val_loss improved from 0.09724 to 0.09723, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3746/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03746: val_loss did not improve from 0.09723\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3747/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03747: val_loss did not improve from 0.09723\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3748/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03748: val_loss improved from 0.09723 to 0.09723, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3749/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03749: val_loss did not improve from 0.09723\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3750/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03750: val_loss did not improve from 0.09723\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3751/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03751: val_loss improved from 0.09723 to 0.09722, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3752/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03752: val_loss improved from 0.09722 to 0.09722, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3753/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03753: val_loss did not improve from 0.09722\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3754/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03754: val_loss improved from 0.09722 to 0.09722, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3755/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03755: val_loss did not improve from 0.09722\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3756/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03756: val_loss improved from 0.09722 to 0.09721, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3757/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03757: val_loss improved from 0.09721 to 0.09721, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3758/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03758: val_loss improved from 0.09721 to 0.09721, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3759/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03759: val_loss improved from 0.09721 to 0.09721, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3760/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03760: val_loss improved from 0.09721 to 0.09721, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3761/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03761: val_loss did not improve from 0.09721\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3762/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03762: val_loss improved from 0.09721 to 0.09720, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3763/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03763: val_loss improved from 0.09720 to 0.09720, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3764/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03764: val_loss did not improve from 0.09720\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3765/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03765: val_loss did not improve from 0.09720\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3766/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03766: val_loss improved from 0.09720 to 0.09720, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3767/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0976\n",
      "Epoch 03767: val_loss improved from 0.09720 to 0.09719, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0976 - val_loss: 0.0972\n",
      "Epoch 3768/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03768: val_loss improved from 0.09719 to 0.09719, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3769/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03769: val_loss did not improve from 0.09719\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3770/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03770: val_loss improved from 0.09719 to 0.09719, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3771/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03771: val_loss did not improve from 0.09719\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3772/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03772: val_loss improved from 0.09719 to 0.09718, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3773/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03773: val_loss did not improve from 0.09718\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0975 - val_loss: 0.0972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3774/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03774: val_loss did not improve from 0.09718\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3775/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03775: val_loss improved from 0.09718 to 0.09718, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3776/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03776: val_loss did not improve from 0.09718\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3777/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03777: val_loss improved from 0.09718 to 0.09718, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3778/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03778: val_loss improved from 0.09718 to 0.09717, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3779/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03779: val_loss improved from 0.09717 to 0.09717, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3780/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03780: val_loss did not improve from 0.09717\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3781/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03781: val_loss improved from 0.09717 to 0.09717, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3782/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03782: val_loss improved from 0.09717 to 0.09717, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3783/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03783: val_loss improved from 0.09717 to 0.09717, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3784/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03784: val_loss improved from 0.09717 to 0.09716, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3785/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03785: val_loss improved from 0.09716 to 0.09716, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3786/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03786: val_loss did not improve from 0.09716\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3787/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03787: val_loss improved from 0.09716 to 0.09716, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3788/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03788: val_loss improved from 0.09716 to 0.09716, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3789/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03789: val_loss improved from 0.09716 to 0.09715, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3790/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03790: val_loss did not improve from 0.09715\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3791/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03791: val_loss improved from 0.09715 to 0.09715, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3792/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03792: val_loss did not improve from 0.09715\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3793/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03793: val_loss improved from 0.09715 to 0.09715, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3794/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03794: val_loss did not improve from 0.09715\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3795/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03795: val_loss improved from 0.09715 to 0.09714, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3796/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03796: val_loss improved from 0.09714 to 0.09714, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3797/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03797: val_loss improved from 0.09714 to 0.09714, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3798/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03798: val_loss improved from 0.09714 to 0.09714, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3799/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03799: val_loss did not improve from 0.09714\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0975 - val_loss: 0.0972\n",
      "Epoch 3800/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03800: val_loss improved from 0.09714 to 0.09714, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3801/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03801: val_loss improved from 0.09714 to 0.09714, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3802/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03802: val_loss improved from 0.09714 to 0.09713, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3803/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03803: val_loss improved from 0.09713 to 0.09713, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3804/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03804: val_loss improved from 0.09713 to 0.09713, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3805/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03805: val_loss improved from 0.09713 to 0.09713, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0975 - val_loss: 0.0971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3806/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03806: val_loss improved from 0.09713 to 0.09713, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3807/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03807: val_loss improved from 0.09713 to 0.09713, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3808/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03808: val_loss did not improve from 0.09713\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3809/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03809: val_loss improved from 0.09713 to 0.09712, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3810/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03810: val_loss did not improve from 0.09712\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3811/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03811: val_loss did not improve from 0.09712\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3812/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03812: val_loss improved from 0.09712 to 0.09712, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3813/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03813: val_loss improved from 0.09712 to 0.09711, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3814/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03814: val_loss improved from 0.09711 to 0.09711, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3815/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03815: val_loss improved from 0.09711 to 0.09711, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3816/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03816: val_loss improved from 0.09711 to 0.09711, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3817/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03817: val_loss improved from 0.09711 to 0.09711, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3818/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03818: val_loss did not improve from 0.09711\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3819/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03819: val_loss did not improve from 0.09711\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3820/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03820: val_loss improved from 0.09711 to 0.09711, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3821/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03821: val_loss improved from 0.09711 to 0.09710, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3822/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03822: val_loss did not improve from 0.09710\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3823/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03823: val_loss improved from 0.09710 to 0.09710, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3824/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03824: val_loss improved from 0.09710 to 0.09710, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3825/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03825: val_loss did not improve from 0.09710\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3826/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03826: val_loss improved from 0.09710 to 0.09709, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3827/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03827: val_loss improved from 0.09709 to 0.09709, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3828/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975\n",
      "Epoch 03828: val_loss did not improve from 0.09709\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0975 - val_loss: 0.0971\n",
      "Epoch 3829/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03829: val_loss improved from 0.09709 to 0.09709, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3830/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03830: val_loss did not improve from 0.09709\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3831/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03831: val_loss did not improve from 0.09709\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3832/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03832: val_loss did not improve from 0.09709\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3833/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03833: val_loss improved from 0.09709 to 0.09708, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3834/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03834: val_loss improved from 0.09708 to 0.09708, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3835/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03835: val_loss improved from 0.09708 to 0.09708, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3836/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03836: val_loss did not improve from 0.09708\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3837/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03837: val_loss improved from 0.09708 to 0.09708, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3838/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03838: val_loss improved from 0.09708 to 0.09708, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0974 - val_loss: 0.0971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3839/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03839: val_loss improved from 0.09708 to 0.09707, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3840/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03840: val_loss improved from 0.09707 to 0.09707, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3841/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03841: val_loss did not improve from 0.09707\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3842/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03842: val_loss improved from 0.09707 to 0.09707, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3843/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03843: val_loss improved from 0.09707 to 0.09707, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3844/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03844: val_loss improved from 0.09707 to 0.09707, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3845/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03845: val_loss did not improve from 0.09707\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3846/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03846: val_loss improved from 0.09707 to 0.09706, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3847/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03847: val_loss improved from 0.09706 to 0.09706, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3848/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03848: val_loss did not improve from 0.09706\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3849/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03849: val_loss improved from 0.09706 to 0.09706, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3850/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03850: val_loss did not improve from 0.09706\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3851/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03851: val_loss improved from 0.09706 to 0.09706, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3852/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03852: val_loss improved from 0.09706 to 0.09705, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3853/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03853: val_loss improved from 0.09705 to 0.09705, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3854/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03854: val_loss improved from 0.09705 to 0.09705, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3855/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03855: val_loss improved from 0.09705 to 0.09705, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3856/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03856: val_loss improved from 0.09705 to 0.09705, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3857/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03857: val_loss did not improve from 0.09705\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0974 - val_loss: 0.0971\n",
      "Epoch 3858/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03858: val_loss improved from 0.09705 to 0.09704, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3859/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03859: val_loss did not improve from 0.09704\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3860/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03860: val_loss did not improve from 0.09704\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3861/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03861: val_loss improved from 0.09704 to 0.09704, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3862/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03862: val_loss improved from 0.09704 to 0.09704, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3863/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03863: val_loss did not improve from 0.09704\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3864/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03864: val_loss improved from 0.09704 to 0.09704, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3865/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03865: val_loss improved from 0.09704 to 0.09703, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3866/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03866: val_loss did not improve from 0.09703\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3867/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03867: val_loss did not improve from 0.09703\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3868/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03868: val_loss did not improve from 0.09703\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3869/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03869: val_loss improved from 0.09703 to 0.09703, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3870/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03870: val_loss improved from 0.09703 to 0.09703, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3871/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03871: val_loss improved from 0.09703 to 0.09703, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3872/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03872: val_loss did not improve from 0.09703\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3873/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03873: val_loss did not improve from 0.09703\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3874/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03874: val_loss improved from 0.09703 to 0.09702, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3875/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03875: val_loss improved from 0.09702 to 0.09702, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3876/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03876: val_loss did not improve from 0.09702\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3877/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03877: val_loss improved from 0.09702 to 0.09702, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3878/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03878: val_loss improved from 0.09702 to 0.09702, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3879/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03879: val_loss improved from 0.09702 to 0.09702, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3880/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03880: val_loss improved from 0.09702 to 0.09701, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3881/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03881: val_loss did not improve from 0.09701\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3882/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03882: val_loss improved from 0.09701 to 0.09701, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3883/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03883: val_loss improved from 0.09701 to 0.09701, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3884/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03884: val_loss improved from 0.09701 to 0.09701, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3885/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03885: val_loss improved from 0.09701 to 0.09701, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3886/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03886: val_loss did not improve from 0.09701\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3887/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03887: val_loss did not improve from 0.09701\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3888/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03888: val_loss improved from 0.09701 to 0.09701, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3889/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03889: val_loss improved from 0.09701 to 0.09700, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3890/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03890: val_loss improved from 0.09700 to 0.09700, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3891/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03891: val_loss did not improve from 0.09700\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3892/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03892: val_loss improved from 0.09700 to 0.09700, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3893/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03893: val_loss improved from 0.09700 to 0.09699, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3894/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03894: val_loss improved from 0.09699 to 0.09699, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3895/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0974\n",
      "Epoch 03895: val_loss did not improve from 0.09699\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0974 - val_loss: 0.0970\n",
      "Epoch 3896/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03896: val_loss did not improve from 0.09699\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3897/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03897: val_loss improved from 0.09699 to 0.09699, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3898/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03898: val_loss improved from 0.09699 to 0.09699, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3899/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03899: val_loss did not improve from 0.09699\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3900/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03900: val_loss improved from 0.09699 to 0.09698, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3901/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03901: val_loss did not improve from 0.09698\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3902/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03902: val_loss did not improve from 0.09698\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3903/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03903: val_loss improved from 0.09698 to 0.09698, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3904/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03904: val_loss improved from 0.09698 to 0.09698, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3905/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03905: val_loss improved from 0.09698 to 0.09698, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3906/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03906: val_loss did not improve from 0.09698\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3907/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03907: val_loss did not improve from 0.09698\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3908/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03908: val_loss improved from 0.09698 to 0.09697, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3909/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03909: val_loss did not improve from 0.09697\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3910/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03910: val_loss improved from 0.09697 to 0.09697, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3911/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03911: val_loss did not improve from 0.09697\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3912/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03912: val_loss did not improve from 0.09697\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3913/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03913: val_loss improved from 0.09697 to 0.09697, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3914/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03914: val_loss improved from 0.09697 to 0.09696, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3915/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03915: val_loss did not improve from 0.09696\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3916/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03916: val_loss improved from 0.09696 to 0.09696, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3917/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03917: val_loss did not improve from 0.09696\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3918/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03918: val_loss improved from 0.09696 to 0.09696, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3919/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03919: val_loss improved from 0.09696 to 0.09696, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3920/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03920: val_loss did not improve from 0.09696\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3921/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03921: val_loss improved from 0.09696 to 0.09696, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3922/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03922: val_loss improved from 0.09696 to 0.09695, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3923/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03923: val_loss did not improve from 0.09695\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3924/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03924: val_loss improved from 0.09695 to 0.09695, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3925/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03925: val_loss did not improve from 0.09695\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0973 - val_loss: 0.0970\n",
      "Epoch 3926/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03926: val_loss improved from 0.09695 to 0.09695, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3927/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03927: val_loss improved from 0.09695 to 0.09694, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3928/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03928: val_loss did not improve from 0.09694\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3929/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03929: val_loss did not improve from 0.09694\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3930/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03930: val_loss improved from 0.09694 to 0.09694, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3931/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03931: val_loss did not improve from 0.09694\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3932/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03932: val_loss did not improve from 0.09694\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3933/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03933: val_loss did not improve from 0.09694\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3934/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03934: val_loss improved from 0.09694 to 0.09694, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3935/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03935: val_loss improved from 0.09694 to 0.09693, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3936/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03936: val_loss did not improve from 0.09693\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3937/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03937: val_loss did not improve from 0.09693\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3938/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03938: val_loss improved from 0.09693 to 0.09693, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0973 - val_loss: 0.0969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3939/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03939: val_loss improved from 0.09693 to 0.09693, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3940/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03940: val_loss did not improve from 0.09693\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3941/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03941: val_loss improved from 0.09693 to 0.09692, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3942/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03942: val_loss improved from 0.09692 to 0.09692, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3943/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03943: val_loss improved from 0.09692 to 0.09692, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3944/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03944: val_loss did not improve from 0.09692\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3945/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03945: val_loss improved from 0.09692 to 0.09692, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3946/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03946: val_loss improved from 0.09692 to 0.09692, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3947/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03947: val_loss did not improve from 0.09692\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3948/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03948: val_loss improved from 0.09692 to 0.09692, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3949/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03949: val_loss improved from 0.09692 to 0.09691, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3950/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03950: val_loss improved from 0.09691 to 0.09691, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3951/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03951: val_loss did not improve from 0.09691\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3952/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03952: val_loss improved from 0.09691 to 0.09691, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3953/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03953: val_loss improved from 0.09691 to 0.09691, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3954/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03954: val_loss improved from 0.09691 to 0.09691, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3955/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03955: val_loss improved from 0.09691 to 0.09690, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3956/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03956: val_loss did not improve from 0.09690\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3957/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03957: val_loss did not improve from 0.09690\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3958/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03958: val_loss improved from 0.09690 to 0.09690, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3959/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03959: val_loss improved from 0.09690 to 0.09690, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3960/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03960: val_loss improved from 0.09690 to 0.09690, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3961/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973\n",
      "Epoch 03961: val_loss did not improve from 0.09690\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 3962/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03962: val_loss did not improve from 0.09690\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3963/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03963: val_loss did not improve from 0.09690\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3964/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03964: val_loss improved from 0.09690 to 0.09689, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3965/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03965: val_loss improved from 0.09689 to 0.09689, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3966/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03966: val_loss improved from 0.09689 to 0.09689, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3967/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03967: val_loss did not improve from 0.09689\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3968/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03968: val_loss did not improve from 0.09689\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3969/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03969: val_loss improved from 0.09689 to 0.09689, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3970/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03970: val_loss improved from 0.09689 to 0.09688, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3971/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03971: val_loss did not improve from 0.09688\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0972 - val_loss: 0.0969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3972/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03972: val_loss improved from 0.09688 to 0.09688, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3973/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03973: val_loss did not improve from 0.09688\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3974/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03974: val_loss did not improve from 0.09688\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3975/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03975: val_loss improved from 0.09688 to 0.09688, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3976/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03976: val_loss improved from 0.09688 to 0.09688, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3977/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03977: val_loss did not improve from 0.09688\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3978/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03978: val_loss did not improve from 0.09688\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3979/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03979: val_loss did not improve from 0.09688\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3980/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03980: val_loss improved from 0.09688 to 0.09687, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3981/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03981: val_loss improved from 0.09687 to 0.09687, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3982/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03982: val_loss improved from 0.09687 to 0.09687, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3983/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03983: val_loss did not improve from 0.09687\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3984/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03984: val_loss improved from 0.09687 to 0.09687, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3985/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03985: val_loss improved from 0.09687 to 0.09687, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3986/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03986: val_loss did not improve from 0.09687\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3987/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03987: val_loss improved from 0.09687 to 0.09686, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3988/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03988: val_loss did not improve from 0.09686\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3989/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03989: val_loss improved from 0.09686 to 0.09686, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3990/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03990: val_loss improved from 0.09686 to 0.09686, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3991/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03991: val_loss did not improve from 0.09686\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3992/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03992: val_loss did not improve from 0.09686\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3993/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03993: val_loss improved from 0.09686 to 0.09685, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3994/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03994: val_loss improved from 0.09685 to 0.09685, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3995/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03995: val_loss did not improve from 0.09685\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 3996/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03996: val_loss improved from 0.09685 to 0.09685, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 3997/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03997: val_loss improved from 0.09685 to 0.09685, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 3998/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03998: val_loss improved from 0.09685 to 0.09685, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 3999/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 03999: val_loss improved from 0.09685 to 0.09685, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4000/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04000: val_loss did not improve from 0.09685\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4001/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04001: val_loss did not improve from 0.09685\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0969\n",
      "Epoch 4002/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04002: val_loss improved from 0.09685 to 0.09684, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4003/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04003: val_loss did not improve from 0.09684\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4004/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04004: val_loss improved from 0.09684 to 0.09684, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4005/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04005: val_loss improved from 0.09684 to 0.09684, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4006/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04006: val_loss improved from 0.09684 to 0.09684, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4007/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04007: val_loss did not improve from 0.09684\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4008/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04008: val_loss did not improve from 0.09684\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4009/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04009: val_loss improved from 0.09684 to 0.09683, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4010/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04010: val_loss improved from 0.09683 to 0.09683, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4011/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04011: val_loss improved from 0.09683 to 0.09683, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4012/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04012: val_loss did not improve from 0.09683\n",
      "7/7 [==============================] - 1s 82ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4013/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04013: val_loss improved from 0.09683 to 0.09683, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4014/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04014: val_loss did not improve from 0.09683\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4015/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04015: val_loss improved from 0.09683 to 0.09683, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4016/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04016: val_loss did not improve from 0.09683\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4017/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04017: val_loss improved from 0.09683 to 0.09682, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4018/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04018: val_loss improved from 0.09682 to 0.09682, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4019/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04019: val_loss improved from 0.09682 to 0.09682, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4020/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04020: val_loss did not improve from 0.09682\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4021/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04021: val_loss improved from 0.09682 to 0.09682, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4022/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04022: val_loss improved from 0.09682 to 0.09682, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4023/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04023: val_loss did not improve from 0.09682\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4024/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04024: val_loss did not improve from 0.09682\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4025/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04025: val_loss improved from 0.09682 to 0.09681, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4026/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04026: val_loss did not improve from 0.09681\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4027/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04027: val_loss improved from 0.09681 to 0.09681, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4028/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04028: val_loss did not improve from 0.09681\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4029/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04029: val_loss improved from 0.09681 to 0.09681, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4030/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04030: val_loss did not improve from 0.09681\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4031/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04031: val_loss improved from 0.09681 to 0.09681, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4032/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04032: val_loss did not improve from 0.09681\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4033/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04033: val_loss improved from 0.09681 to 0.09680, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4034/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04034: val_loss improved from 0.09680 to 0.09680, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4035/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04035: val_loss did not improve from 0.09680\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4036/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04036: val_loss did not improve from 0.09680\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4037/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04037: val_loss improved from 0.09680 to 0.09680, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4038/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04038: val_loss did not improve from 0.09680\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4039/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04039: val_loss did not improve from 0.09680\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4040/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04040: val_loss did not improve from 0.09680\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4041/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04041: val_loss improved from 0.09680 to 0.09679, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4042/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04042: val_loss did not improve from 0.09679\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4043/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04043: val_loss did not improve from 0.09679\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4044/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04044: val_loss did not improve from 0.09679\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4045/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0972\n",
      "Epoch 04045: val_loss improved from 0.09679 to 0.09679, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0972 - val_loss: 0.0968\n",
      "Epoch 4046/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04046: val_loss did not improve from 0.09679\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4047/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04047: val_loss improved from 0.09679 to 0.09679, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4048/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04048: val_loss improved from 0.09679 to 0.09678, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4049/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04049: val_loss improved from 0.09678 to 0.09678, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4050/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04050: val_loss did not improve from 0.09678\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4051/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04051: val_loss improved from 0.09678 to 0.09678, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4052/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04052: val_loss did not improve from 0.09678\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4053/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04053: val_loss did not improve from 0.09678\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4054/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04054: val_loss improved from 0.09678 to 0.09678, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4055/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04055: val_loss did not improve from 0.09678\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4056/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04056: val_loss improved from 0.09678 to 0.09677, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4057/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04057: val_loss improved from 0.09677 to 0.09677, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4058/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04058: val_loss did not improve from 0.09677\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4059/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04059: val_loss did not improve from 0.09677\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4060/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04060: val_loss improved from 0.09677 to 0.09677, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4061/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04061: val_loss did not improve from 0.09677\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4062/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04062: val_loss did not improve from 0.09677\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4063/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04063: val_loss did not improve from 0.09677\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4064/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04064: val_loss improved from 0.09677 to 0.09677, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4065/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04065: val_loss improved from 0.09677 to 0.09676, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4066/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04066: val_loss did not improve from 0.09676\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4067/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04067: val_loss improved from 0.09676 to 0.09676, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4068/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04068: val_loss improved from 0.09676 to 0.09676, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4069/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04069: val_loss did not improve from 0.09676\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4070/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04070: val_loss did not improve from 0.09676\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4071/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04071: val_loss improved from 0.09676 to 0.09676, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4072/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04072: val_loss did not improve from 0.09676\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4073/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04073: val_loss improved from 0.09676 to 0.09676, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4074/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04074: val_loss improved from 0.09676 to 0.09675, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4075/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04075: val_loss improved from 0.09675 to 0.09675, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4076/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04076: val_loss improved from 0.09675 to 0.09675, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4077/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04077: val_loss did not improve from 0.09675\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4078/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04078: val_loss did not improve from 0.09675\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4079/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04079: val_loss improved from 0.09675 to 0.09675, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4080/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04080: val_loss did not improve from 0.09675\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0971 - val_loss: 0.0968\n",
      "Epoch 4081/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04081: val_loss improved from 0.09675 to 0.09674, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4082/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04082: val_loss improved from 0.09674 to 0.09674, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4083/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04083: val_loss improved from 0.09674 to 0.09674, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4084/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04084: val_loss did not improve from 0.09674\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4085/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04085: val_loss improved from 0.09674 to 0.09674, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4086/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04086: val_loss did not improve from 0.09674\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4087/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04087: val_loss did not improve from 0.09674\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4088/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04088: val_loss improved from 0.09674 to 0.09673, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4089/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04089: val_loss improved from 0.09673 to 0.09673, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4090/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04090: val_loss did not improve from 0.09673\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4091/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04091: val_loss improved from 0.09673 to 0.09673, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4092/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04092: val_loss improved from 0.09673 to 0.09673, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4093/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04093: val_loss improved from 0.09673 to 0.09673, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4094/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04094: val_loss did not improve from 0.09673\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4095/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04095: val_loss improved from 0.09673 to 0.09673, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4096/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04096: val_loss improved from 0.09673 to 0.09672, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4097/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04097: val_loss did not improve from 0.09672\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4098/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04098: val_loss did not improve from 0.09672\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4099/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04099: val_loss improved from 0.09672 to 0.09672, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4100/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04100: val_loss improved from 0.09672 to 0.09672, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4101/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04101: val_loss did not improve from 0.09672\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4102/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04102: val_loss improved from 0.09672 to 0.09672, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4103/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04103: val_loss did not improve from 0.09672\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4104/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04104: val_loss improved from 0.09672 to 0.09671, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0971 - val_loss: 0.0967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4105/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04105: val_loss did not improve from 0.09671\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4106/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04106: val_loss improved from 0.09671 to 0.09671, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4107/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04107: val_loss did not improve from 0.09671\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4108/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04108: val_loss did not improve from 0.09671\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4109/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04109: val_loss improved from 0.09671 to 0.09671, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4110/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04110: val_loss did not improve from 0.09671\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4111/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04111: val_loss did not improve from 0.09671\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4112/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04112: val_loss did not improve from 0.09671\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4113/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04113: val_loss improved from 0.09671 to 0.09670, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4114/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04114: val_loss improved from 0.09670 to 0.09670, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4115/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04115: val_loss did not improve from 0.09670\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4116/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971\n",
      "Epoch 04116: val_loss did not improve from 0.09670\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 4117/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04117: val_loss did not improve from 0.09670\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4118/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04118: val_loss improved from 0.09670 to 0.09670, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4119/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04119: val_loss improved from 0.09670 to 0.09670, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4120/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04120: val_loss did not improve from 0.09670\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4121/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04121: val_loss did not improve from 0.09670\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4122/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04122: val_loss improved from 0.09670 to 0.09669, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4123/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04123: val_loss improved from 0.09669 to 0.09669, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4124/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04124: val_loss did not improve from 0.09669\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4125/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04125: val_loss did not improve from 0.09669\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4126/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04126: val_loss improved from 0.09669 to 0.09669, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4127/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04127: val_loss did not improve from 0.09669\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4128/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04128: val_loss improved from 0.09669 to 0.09669, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4129/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04129: val_loss did not improve from 0.09669\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4130/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04130: val_loss did not improve from 0.09669\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4131/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04131: val_loss improved from 0.09669 to 0.09668, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4132/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04132: val_loss did not improve from 0.09668\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4133/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04133: val_loss improved from 0.09668 to 0.09668, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4134/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04134: val_loss improved from 0.09668 to 0.09668, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4135/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04135: val_loss did not improve from 0.09668\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4136/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04136: val_loss improved from 0.09668 to 0.09668, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4137/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04137: val_loss improved from 0.09668 to 0.09668, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4138/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04138: val_loss improved from 0.09668 to 0.09667, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0970 - val_loss: 0.0967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4139/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04139: val_loss did not improve from 0.09667\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4140/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04140: val_loss did not improve from 0.09667\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4141/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04141: val_loss improved from 0.09667 to 0.09667, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4142/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04142: val_loss did not improve from 0.09667\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4143/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04143: val_loss improved from 0.09667 to 0.09667, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4144/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04144: val_loss improved from 0.09667 to 0.09667, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4145/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04145: val_loss did not improve from 0.09667\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4146/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04146: val_loss did not improve from 0.09667\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4147/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04147: val_loss improved from 0.09667 to 0.09666, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4148/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04148: val_loss did not improve from 0.09666\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4149/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04149: val_loss did not improve from 0.09666\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4150/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04150: val_loss improved from 0.09666 to 0.09666, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4151/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04151: val_loss improved from 0.09666 to 0.09666, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4152/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04152: val_loss did not improve from 0.09666\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4153/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04153: val_loss did not improve from 0.09666\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4154/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04154: val_loss improved from 0.09666 to 0.09666, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4155/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04155: val_loss improved from 0.09666 to 0.09666, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4156/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04156: val_loss did not improve from 0.09666\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4157/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04157: val_loss did not improve from 0.09666\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4158/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04158: val_loss improved from 0.09666 to 0.09665, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4159/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04159: val_loss improved from 0.09665 to 0.09665, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4160/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04160: val_loss did not improve from 0.09665\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4161/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04161: val_loss did not improve from 0.09665\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4162/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04162: val_loss did not improve from 0.09665\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4163/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04163: val_loss improved from 0.09665 to 0.09665, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4164/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04164: val_loss did not improve from 0.09665\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4165/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04165: val_loss did not improve from 0.09665\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0970 - val_loss: 0.0967\n",
      "Epoch 4166/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04166: val_loss did not improve from 0.09665\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4167/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04167: val_loss improved from 0.09665 to 0.09664, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4168/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04168: val_loss improved from 0.09664 to 0.09664, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4169/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04169: val_loss improved from 0.09664 to 0.09664, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4170/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04170: val_loss did not improve from 0.09664\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4171/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04171: val_loss did not improve from 0.09664\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4172/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04172: val_loss did not improve from 0.09664\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4173/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04173: val_loss improved from 0.09664 to 0.09664, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0970 - val_loss: 0.0966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4174/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04174: val_loss improved from 0.09664 to 0.09663, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4175/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04175: val_loss did not improve from 0.09663\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4176/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04176: val_loss did not improve from 0.09663\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4177/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04177: val_loss did not improve from 0.09663\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4178/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04178: val_loss improved from 0.09663 to 0.09663, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4179/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04179: val_loss improved from 0.09663 to 0.09663, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4180/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04180: val_loss improved from 0.09663 to 0.09663, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4181/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04181: val_loss did not improve from 0.09663\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4182/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04182: val_loss improved from 0.09663 to 0.09663, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4183/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04183: val_loss did not improve from 0.09663\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4184/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04184: val_loss did not improve from 0.09663\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4185/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04185: val_loss did not improve from 0.09663\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4186/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04186: val_loss improved from 0.09663 to 0.09662, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4187/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04187: val_loss did not improve from 0.09662\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4188/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04188: val_loss improved from 0.09662 to 0.09662, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4189/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04189: val_loss improved from 0.09662 to 0.09662, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4190/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04190: val_loss improved from 0.09662 to 0.09662, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4191/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04191: val_loss did not improve from 0.09662\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4192/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04192: val_loss improved from 0.09662 to 0.09661, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4193/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04193: val_loss improved from 0.09661 to 0.09661, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4194/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04194: val_loss did not improve from 0.09661\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4195/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04195: val_loss improved from 0.09661 to 0.09661, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4196/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04196: val_loss improved from 0.09661 to 0.09661, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4197/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04197: val_loss did not improve from 0.09661\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4198/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04198: val_loss improved from 0.09661 to 0.09661, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4199/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04199: val_loss did not improve from 0.09661\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4200/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04200: val_loss improved from 0.09661 to 0.09661, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4201/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0970\n",
      "Epoch 04201: val_loss improved from 0.09661 to 0.09660, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0970 - val_loss: 0.0966\n",
      "Epoch 4202/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04202: val_loss did not improve from 0.09660\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4203/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04203: val_loss did not improve from 0.09660\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4204/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04204: val_loss improved from 0.09660 to 0.09660, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4205/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04205: val_loss did not improve from 0.09660\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4206/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04206: val_loss improved from 0.09660 to 0.09660, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4207/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04207: val_loss improved from 0.09660 to 0.09660, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4208/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04208: val_loss did not improve from 0.09660\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4209/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04209: val_loss improved from 0.09660 to 0.09659, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4210/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04210: val_loss did not improve from 0.09659\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4211/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04211: val_loss improved from 0.09659 to 0.09659, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4212/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04212: val_loss improved from 0.09659 to 0.09659, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4213/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04213: val_loss did not improve from 0.09659\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4214/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04214: val_loss did not improve from 0.09659\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4215/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04215: val_loss improved from 0.09659 to 0.09659, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4216/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04216: val_loss did not improve from 0.09659\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4217/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04217: val_loss did not improve from 0.09659\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4218/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04218: val_loss improved from 0.09659 to 0.09658, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4219/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04219: val_loss improved from 0.09658 to 0.09658, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4220/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04220: val_loss did not improve from 0.09658\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4221/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04221: val_loss did not improve from 0.09658\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4222/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04222: val_loss did not improve from 0.09658\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4223/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04223: val_loss improved from 0.09658 to 0.09658, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4224/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04224: val_loss did not improve from 0.09658\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4225/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04225: val_loss did not improve from 0.09658\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4226/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04226: val_loss improved from 0.09658 to 0.09658, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4227/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04227: val_loss improved from 0.09658 to 0.09658, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4228/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04228: val_loss improved from 0.09658 to 0.09657, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4229/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04229: val_loss did not improve from 0.09657\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4230/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04230: val_loss did not improve from 0.09657\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4231/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04231: val_loss did not improve from 0.09657\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4232/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04232: val_loss improved from 0.09657 to 0.09657, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4233/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04233: val_loss did not improve from 0.09657\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4234/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04234: val_loss did not improve from 0.09657\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4235/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04235: val_loss did not improve from 0.09657\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4236/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04236: val_loss improved from 0.09657 to 0.09657, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4237/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04237: val_loss did not improve from 0.09657\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4238/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04238: val_loss did not improve from 0.09657\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4239/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04239: val_loss improved from 0.09657 to 0.09656, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4240/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04240: val_loss improved from 0.09656 to 0.09656, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4241/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04241: val_loss did not improve from 0.09656\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4242/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04242: val_loss did not improve from 0.09656\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4243/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04243: val_loss improved from 0.09656 to 0.09656, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4244/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04244: val_loss improved from 0.09656 to 0.09655, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4245/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04245: val_loss did not improve from 0.09655\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4246/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04246: val_loss improved from 0.09655 to 0.09655, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4247/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04247: val_loss did not improve from 0.09655\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4248/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04248: val_loss improved from 0.09655 to 0.09655, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4249/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04249: val_loss did not improve from 0.09655\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4250/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04250: val_loss did not improve from 0.09655\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4251/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04251: val_loss improved from 0.09655 to 0.09655, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4252/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04252: val_loss improved from 0.09655 to 0.09655, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4253/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04253: val_loss did not improve from 0.09655\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4254/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04254: val_loss did not improve from 0.09655\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4255/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04255: val_loss improved from 0.09655 to 0.09654, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4256/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04256: val_loss improved from 0.09654 to 0.09654, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4257/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04257: val_loss did not improve from 0.09654\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4258/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04258: val_loss did not improve from 0.09654\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4259/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04259: val_loss improved from 0.09654 to 0.09654, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4260/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04260: val_loss improved from 0.09654 to 0.09654, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4261/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04261: val_loss did not improve from 0.09654\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0969 - val_loss: 0.0966\n",
      "Epoch 4262/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04262: val_loss did not improve from 0.09654\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4263/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04263: val_loss improved from 0.09654 to 0.09653, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4264/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04264: val_loss did not improve from 0.09653\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4265/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04265: val_loss did not improve from 0.09653\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4266/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04266: val_loss improved from 0.09653 to 0.09653, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4267/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04267: val_loss did not improve from 0.09653\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4268/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04268: val_loss did not improve from 0.09653\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4269/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04269: val_loss did not improve from 0.09653\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4270/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04270: val_loss improved from 0.09653 to 0.09653, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4271/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04271: val_loss improved from 0.09653 to 0.09652, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4272/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04272: val_loss improved from 0.09652 to 0.09652, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4273/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04273: val_loss improved from 0.09652 to 0.09652, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4274/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04274: val_loss did not improve from 0.09652\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4275/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04275: val_loss improved from 0.09652 to 0.09652, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4276/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04276: val_loss improved from 0.09652 to 0.09652, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4277/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04277: val_loss improved from 0.09652 to 0.09652, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4278/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04278: val_loss did not improve from 0.09652\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4279/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04279: val_loss did not improve from 0.09652\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4280/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04280: val_loss improved from 0.09652 to 0.09651, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4281/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04281: val_loss improved from 0.09651 to 0.09651, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4282/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04282: val_loss did not improve from 0.09651\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4283/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04283: val_loss did not improve from 0.09651\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4284/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04284: val_loss improved from 0.09651 to 0.09651, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4285/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04285: val_loss did not improve from 0.09651\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4286/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04286: val_loss did not improve from 0.09651\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4287/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04287: val_loss improved from 0.09651 to 0.09651, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4288/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04288: val_loss improved from 0.09651 to 0.09651, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4289/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04289: val_loss did not improve from 0.09651\n",
      "7/7 [==============================] - 1s 101ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4290/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0969\n",
      "Epoch 04290: val_loss improved from 0.09651 to 0.09650, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0969 - val_loss: 0.0965\n",
      "Epoch 4291/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04291: val_loss improved from 0.09650 to 0.09650, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4292/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04292: val_loss did not improve from 0.09650\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4293/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04293: val_loss improved from 0.09650 to 0.09650, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4294/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04294: val_loss improved from 0.09650 to 0.09650, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4295/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04295: val_loss improved from 0.09650 to 0.09650, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4296/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04296: val_loss did not improve from 0.09650\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4297/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04297: val_loss did not improve from 0.09650\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4298/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04298: val_loss improved from 0.09650 to 0.09650, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4299/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04299: val_loss improved from 0.09650 to 0.09649, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4300/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04300: val_loss improved from 0.09649 to 0.09649, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4301/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04301: val_loss improved from 0.09649 to 0.09649, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4302/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04302: val_loss did not improve from 0.09649\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4303/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04303: val_loss improved from 0.09649 to 0.09649, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4304/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04304: val_loss improved from 0.09649 to 0.09649, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4305/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04305: val_loss improved from 0.09649 to 0.09649, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4306/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04306: val_loss did not improve from 0.09649\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4307/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04307: val_loss improved from 0.09649 to 0.09649, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4308/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04308: val_loss improved from 0.09649 to 0.09649, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4309/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04309: val_loss improved from 0.09649 to 0.09648, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4310/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04310: val_loss did not improve from 0.09648\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4311/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04311: val_loss improved from 0.09648 to 0.09648, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4312/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04312: val_loss improved from 0.09648 to 0.09648, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4313/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04313: val_loss did not improve from 0.09648\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4314/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04314: val_loss did not improve from 0.09648\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4315/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04315: val_loss improved from 0.09648 to 0.09648, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4316/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04316: val_loss did not improve from 0.09648\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4317/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04317: val_loss did not improve from 0.09648\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4318/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04318: val_loss did not improve from 0.09648\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4319/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04319: val_loss improved from 0.09648 to 0.09647, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4320/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04320: val_loss did not improve from 0.09647\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4321/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04321: val_loss improved from 0.09647 to 0.09647, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4322/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04322: val_loss improved from 0.09647 to 0.09647, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4323/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04323: val_loss did not improve from 0.09647\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4324/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04324: val_loss improved from 0.09647 to 0.09647, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4325/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04325: val_loss improved from 0.09647 to 0.09646, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4326/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04326: val_loss did not improve from 0.09646\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4327/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04327: val_loss did not improve from 0.09646\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4328/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04328: val_loss improved from 0.09646 to 0.09646, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4329/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04329: val_loss improved from 0.09646 to 0.09646, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4330/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04330: val_loss did not improve from 0.09646\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4331/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04331: val_loss did not improve from 0.09646\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4332/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04332: val_loss did not improve from 0.09646\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4333/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04333: val_loss improved from 0.09646 to 0.09646, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4334/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04334: val_loss did not improve from 0.09646\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4335/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04335: val_loss improved from 0.09646 to 0.09646, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4336/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04336: val_loss improved from 0.09646 to 0.09645, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4337/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04337: val_loss did not improve from 0.09645\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4338/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04338: val_loss did not improve from 0.09645\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4339/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04339: val_loss improved from 0.09645 to 0.09645, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4340/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04340: val_loss did not improve from 0.09645\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4341/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04341: val_loss improved from 0.09645 to 0.09645, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0968 - val_loss: 0.0964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4342/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04342: val_loss did not improve from 0.09645\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4343/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04343: val_loss improved from 0.09645 to 0.09644, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4344/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04344: val_loss did not improve from 0.09644\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4345/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04345: val_loss did not improve from 0.09644\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0968 - val_loss: 0.0965\n",
      "Epoch 4346/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04346: val_loss improved from 0.09644 to 0.09644, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4347/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04347: val_loss did not improve from 0.09644\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4348/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04348: val_loss did not improve from 0.09644\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4349/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04349: val_loss did not improve from 0.09644\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4350/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04350: val_loss did not improve from 0.09644\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4351/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04351: val_loss did not improve from 0.09644\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4352/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04352: val_loss improved from 0.09644 to 0.09643, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4353/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04353: val_loss improved from 0.09643 to 0.09643, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4354/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04354: val_loss improved from 0.09643 to 0.09643, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4355/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04355: val_loss did not improve from 0.09643\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4356/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04356: val_loss did not improve from 0.09643\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4357/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04357: val_loss improved from 0.09643 to 0.09643, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4358/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04358: val_loss improved from 0.09643 to 0.09643, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4359/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04359: val_loss did not improve from 0.09643\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4360/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04360: val_loss improved from 0.09643 to 0.09642, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4361/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04361: val_loss did not improve from 0.09642\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4362/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04362: val_loss did not improve from 0.09642\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4363/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04363: val_loss improved from 0.09642 to 0.09642, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4364/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04364: val_loss improved from 0.09642 to 0.09642, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4365/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04365: val_loss did not improve from 0.09642\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4366/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04366: val_loss improved from 0.09642 to 0.09642, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4367/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04367: val_loss improved from 0.09642 to 0.09642, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 100ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4368/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04368: val_loss improved from 0.09642 to 0.09641, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4369/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04369: val_loss did not improve from 0.09641\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4370/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04370: val_loss did not improve from 0.09641\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4371/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04371: val_loss did not improve from 0.09641\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4372/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04372: val_loss improved from 0.09641 to 0.09641, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4373/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04373: val_loss improved from 0.09641 to 0.09641, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4374/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04374: val_loss did not improve from 0.09641\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4375/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04375: val_loss did not improve from 0.09641\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4376/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04376: val_loss did not improve from 0.09641\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4377/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04377: val_loss improved from 0.09641 to 0.09641, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4378/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0968\n",
      "Epoch 04378: val_loss improved from 0.09641 to 0.09640, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0968 - val_loss: 0.0964\n",
      "Epoch 4379/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04379: val_loss did not improve from 0.09640\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4380/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04380: val_loss improved from 0.09640 to 0.09640, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4381/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04381: val_loss did not improve from 0.09640\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4382/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04382: val_loss improved from 0.09640 to 0.09640, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4383/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04383: val_loss did not improve from 0.09640\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4384/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04384: val_loss improved from 0.09640 to 0.09640, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4385/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04385: val_loss improved from 0.09640 to 0.09639, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4386/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04386: val_loss did not improve from 0.09639\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4387/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04387: val_loss did not improve from 0.09639\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4388/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04388: val_loss improved from 0.09639 to 0.09639, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4389/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04389: val_loss did not improve from 0.09639\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4390/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04390: val_loss improved from 0.09639 to 0.09639, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4391/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04391: val_loss did not improve from 0.09639\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4392/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04392: val_loss did not improve from 0.09639\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4393/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04393: val_loss improved from 0.09639 to 0.09638, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4394/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04394: val_loss did not improve from 0.09638\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4395/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04395: val_loss improved from 0.09638 to 0.09638, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4396/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04396: val_loss did not improve from 0.09638\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4397/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04397: val_loss improved from 0.09638 to 0.09638, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4398/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04398: val_loss improved from 0.09638 to 0.09638, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4399/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04399: val_loss improved from 0.09638 to 0.09638, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4400/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04400: val_loss did not improve from 0.09638\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4401/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04401: val_loss did not improve from 0.09638\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4402/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04402: val_loss improved from 0.09638 to 0.09638, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4403/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04403: val_loss improved from 0.09638 to 0.09638, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4404/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04404: val_loss improved from 0.09638 to 0.09637, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4405/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04405: val_loss did not improve from 0.09637\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4406/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04406: val_loss did not improve from 0.09637\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4407/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04407: val_loss improved from 0.09637 to 0.09637, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4408/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04408: val_loss did not improve from 0.09637\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4409/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04409: val_loss did not improve from 0.09637\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4410/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04410: val_loss improved from 0.09637 to 0.09637, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4411/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04411: val_loss did not improve from 0.09637\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4412/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04412: val_loss improved from 0.09637 to 0.09637, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4413/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04413: val_loss improved from 0.09637 to 0.09636, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4414/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04414: val_loss improved from 0.09636 to 0.09636, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4415/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04415: val_loss did not improve from 0.09636\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4416/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04416: val_loss improved from 0.09636 to 0.09636, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4417/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04417: val_loss did not improve from 0.09636\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4418/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04418: val_loss did not improve from 0.09636\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4419/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04419: val_loss improved from 0.09636 to 0.09636, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4420/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04420: val_loss did not improve from 0.09636\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4421/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04421: val_loss improved from 0.09636 to 0.09636, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4422/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04422: val_loss improved from 0.09636 to 0.09636, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4423/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04423: val_loss improved from 0.09636 to 0.09635, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4424/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04424: val_loss did not improve from 0.09635\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4425/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04425: val_loss improved from 0.09635 to 0.09635, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4426/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04426: val_loss improved from 0.09635 to 0.09635, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4427/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04427: val_loss did not improve from 0.09635\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4428/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04428: val_loss did not improve from 0.09635\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4429/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04429: val_loss did not improve from 0.09635\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4430/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04430: val_loss did not improve from 0.09635\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4431/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04431: val_loss improved from 0.09635 to 0.09635, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4432/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04432: val_loss did not improve from 0.09635\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0967 - val_loss: 0.0964\n",
      "Epoch 4433/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04433: val_loss did not improve from 0.09635\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4434/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04434: val_loss improved from 0.09635 to 0.09634, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4435/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04435: val_loss did not improve from 0.09634\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4436/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04436: val_loss did not improve from 0.09634\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4437/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04437: val_loss did not improve from 0.09634\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4438/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04438: val_loss improved from 0.09634 to 0.09634, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4439/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04439: val_loss did not improve from 0.09634\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4440/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04440: val_loss improved from 0.09634 to 0.09634, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4441/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04441: val_loss improved from 0.09634 to 0.09633, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4442/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04442: val_loss improved from 0.09633 to 0.09633, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4443/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04443: val_loss did not improve from 0.09633\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4444/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04444: val_loss improved from 0.09633 to 0.09633, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4445/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04445: val_loss improved from 0.09633 to 0.09633, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4446/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04446: val_loss improved from 0.09633 to 0.09633, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4447/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04447: val_loss did not improve from 0.09633\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4448/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04448: val_loss did not improve from 0.09633\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4449/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04449: val_loss improved from 0.09633 to 0.09633, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4450/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04450: val_loss did not improve from 0.09633\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4451/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04451: val_loss did not improve from 0.09633\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4452/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04452: val_loss improved from 0.09633 to 0.09632, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4453/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04453: val_loss did not improve from 0.09632\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4454/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04454: val_loss improved from 0.09632 to 0.09632, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4455/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04455: val_loss improved from 0.09632 to 0.09632, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4456/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04456: val_loss did not improve from 0.09632\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4457/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04457: val_loss improved from 0.09632 to 0.09632, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4458/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04458: val_loss did not improve from 0.09632\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4459/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04459: val_loss improved from 0.09632 to 0.09631, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4460/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04460: val_loss improved from 0.09631 to 0.09631, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4461/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04461: val_loss did not improve from 0.09631\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4462/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04462: val_loss improved from 0.09631 to 0.09631, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4463/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04463: val_loss did not improve from 0.09631\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4464/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04464: val_loss improved from 0.09631 to 0.09631, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4465/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04465: val_loss did not improve from 0.09631\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4466/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0967\n",
      "Epoch 04466: val_loss improved from 0.09631 to 0.09631, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0967 - val_loss: 0.0963\n",
      "Epoch 4467/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04467: val_loss did not improve from 0.09631\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4468/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04468: val_loss improved from 0.09631 to 0.09630, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4469/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04469: val_loss did not improve from 0.09630\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4470/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04470: val_loss did not improve from 0.09630\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4471/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04471: val_loss improved from 0.09630 to 0.09630, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4472/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04472: val_loss improved from 0.09630 to 0.09629, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4473/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04473: val_loss improved from 0.09629 to 0.09629, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4474/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04474: val_loss did not improve from 0.09629\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4475/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04475: val_loss improved from 0.09629 to 0.09629, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4476/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04476: val_loss did not improve from 0.09629\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4477/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04477: val_loss improved from 0.09629 to 0.09629, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4478/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04478: val_loss improved from 0.09629 to 0.09629, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4479/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04479: val_loss did not improve from 0.09629\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4480/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04480: val_loss did not improve from 0.09629\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4481/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04481: val_loss improved from 0.09629 to 0.09628, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4482/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04482: val_loss did not improve from 0.09628\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4483/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04483: val_loss improved from 0.09628 to 0.09628, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4484/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04484: val_loss improved from 0.09628 to 0.09627, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4485/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04485: val_loss improved from 0.09627 to 0.09627, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4486/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04486: val_loss improved from 0.09627 to 0.09627, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4487/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04487: val_loss did not improve from 0.09627\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4488/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04488: val_loss improved from 0.09627 to 0.09627, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4489/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04489: val_loss did not improve from 0.09627\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4490/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04490: val_loss did not improve from 0.09627\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4491/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04491: val_loss improved from 0.09627 to 0.09626, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4492/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04492: val_loss improved from 0.09626 to 0.09626, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4493/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04493: val_loss did not improve from 0.09626\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4494/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04494: val_loss did not improve from 0.09626\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4495/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04495: val_loss improved from 0.09626 to 0.09626, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4496/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04496: val_loss improved from 0.09626 to 0.09626, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4497/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04497: val_loss did not improve from 0.09626\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4498/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04498: val_loss improved from 0.09626 to 0.09625, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4499/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04499: val_loss improved from 0.09625 to 0.09625, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4500/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04500: val_loss improved from 0.09625 to 0.09625, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4501/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04501: val_loss did not improve from 0.09625\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4502/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04502: val_loss did not improve from 0.09625\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4503/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04503: val_loss did not improve from 0.09625\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4504/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04504: val_loss improved from 0.09625 to 0.09625, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4505/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04505: val_loss improved from 0.09625 to 0.09624, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4506/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04506: val_loss improved from 0.09624 to 0.09624, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4507/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04507: val_loss did not improve from 0.09624\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0966 - val_loss: 0.0963\n",
      "Epoch 4508/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04508: val_loss improved from 0.09624 to 0.09624, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4509/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04509: val_loss did not improve from 0.09624\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4510/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04510: val_loss did not improve from 0.09624\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4511/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04511: val_loss improved from 0.09624 to 0.09624, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4512/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04512: val_loss did not improve from 0.09624\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4513/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04513: val_loss did not improve from 0.09624\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4514/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04514: val_loss improved from 0.09624 to 0.09623, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4515/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04515: val_loss did not improve from 0.09623\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4516/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04516: val_loss improved from 0.09623 to 0.09623, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4517/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04517: val_loss did not improve from 0.09623\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4518/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04518: val_loss did not improve from 0.09623\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4519/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04519: val_loss did not improve from 0.09623\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4520/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04520: val_loss did not improve from 0.09623\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4521/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04521: val_loss improved from 0.09623 to 0.09623, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4522/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04522: val_loss improved from 0.09623 to 0.09623, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4523/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04523: val_loss improved from 0.09623 to 0.09623, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4524/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04524: val_loss did not improve from 0.09623\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4525/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04525: val_loss did not improve from 0.09623\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4526/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04526: val_loss improved from 0.09623 to 0.09622, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4527/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04527: val_loss improved from 0.09622 to 0.09622, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4528/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04528: val_loss did not improve from 0.09622\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4529/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04529: val_loss improved from 0.09622 to 0.09622, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4530/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04530: val_loss did not improve from 0.09622\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4531/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04531: val_loss did not improve from 0.09622\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4532/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04532: val_loss did not improve from 0.09622\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4533/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04533: val_loss improved from 0.09622 to 0.09622, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4534/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04534: val_loss improved from 0.09622 to 0.09621, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4535/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04535: val_loss did not improve from 0.09621\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4536/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04536: val_loss improved from 0.09621 to 0.09621, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4537/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04537: val_loss did not improve from 0.09621\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4538/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04538: val_loss did not improve from 0.09621\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4539/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04539: val_loss did not improve from 0.09621\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4540/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04540: val_loss improved from 0.09621 to 0.09621, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4541/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04541: val_loss improved from 0.09621 to 0.09620, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4542/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04542: val_loss improved from 0.09620 to 0.09620, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4543/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04543: val_loss did not improve from 0.09620\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4544/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04544: val_loss improved from 0.09620 to 0.09620, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4545/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04545: val_loss improved from 0.09620 to 0.09620, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4546/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04546: val_loss improved from 0.09620 to 0.09620, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4547/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04547: val_loss did not improve from 0.09620\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4548/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04548: val_loss improved from 0.09620 to 0.09620, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4549/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04549: val_loss improved from 0.09620 to 0.09619, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4550/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04550: val_loss did not improve from 0.09619\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4551/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04551: val_loss improved from 0.09619 to 0.09619, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4552/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04552: val_loss did not improve from 0.09619\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4553/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0966\n",
      "Epoch 04553: val_loss improved from 0.09619 to 0.09619, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0966 - val_loss: 0.0962\n",
      "Epoch 4554/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04554: val_loss improved from 0.09619 to 0.09619, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4555/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04555: val_loss did not improve from 0.09619\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4556/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04556: val_loss did not improve from 0.09619\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4557/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04557: val_loss improved from 0.09619 to 0.09618, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4558/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04558: val_loss improved from 0.09618 to 0.09618, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4559/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04559: val_loss did not improve from 0.09618\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4560/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04560: val_loss did not improve from 0.09618\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4561/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04561: val_loss improved from 0.09618 to 0.09618, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 99ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4562/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04562: val_loss improved from 0.09618 to 0.09618, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4563/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04563: val_loss did not improve from 0.09618\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4564/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04564: val_loss improved from 0.09618 to 0.09618, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4565/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04565: val_loss improved from 0.09618 to 0.09617, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4566/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04566: val_loss improved from 0.09617 to 0.09617, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4567/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04567: val_loss did not improve from 0.09617\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4568/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04568: val_loss did not improve from 0.09617\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4569/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04569: val_loss improved from 0.09617 to 0.09617, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4570/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04570: val_loss improved from 0.09617 to 0.09617, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4571/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04571: val_loss did not improve from 0.09617\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4572/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04572: val_loss did not improve from 0.09617\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4573/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04573: val_loss improved from 0.09617 to 0.09617, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4574/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04574: val_loss improved from 0.09617 to 0.09616, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4575/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04575: val_loss did not improve from 0.09616\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4576/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04576: val_loss improved from 0.09616 to 0.09616, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0965 - val_loss: 0.0962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4577/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04577: val_loss improved from 0.09616 to 0.09616, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4578/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04578: val_loss did not improve from 0.09616\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4579/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04579: val_loss did not improve from 0.09616\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4580/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04580: val_loss improved from 0.09616 to 0.09616, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4581/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965- ETA: 0s - loss: 0.0\n",
      "Epoch 04581: val_loss did not improve from 0.09616\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4582/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04582: val_loss improved from 0.09616 to 0.09615, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4583/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04583: val_loss improved from 0.09615 to 0.09615, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4584/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04584: val_loss did not improve from 0.09615\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4585/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04585: val_loss did not improve from 0.09615\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4586/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04586: val_loss improved from 0.09615 to 0.09615, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4587/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04587: val_loss improved from 0.09615 to 0.09615, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4588/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04588: val_loss improved from 0.09615 to 0.09615, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4589/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04589: val_loss improved from 0.09615 to 0.09615, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4590/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04590: val_loss did not improve from 0.09615\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0965 - val_loss: 0.0962\n",
      "Epoch 4591/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04591: val_loss did not improve from 0.09615\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4592/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04592: val_loss improved from 0.09615 to 0.09614, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4593/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04593: val_loss did not improve from 0.09614\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4594/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04594: val_loss improved from 0.09614 to 0.09614, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4595/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04595: val_loss did not improve from 0.09614\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4596/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04596: val_loss did not improve from 0.09614\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4597/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04597: val_loss improved from 0.09614 to 0.09614, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4598/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04598: val_loss did not improve from 0.09614\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4599/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04599: val_loss did not improve from 0.09614\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4600/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04600: val_loss improved from 0.09614 to 0.09613, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4601/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04601: val_loss did not improve from 0.09613\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4602/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04602: val_loss did not improve from 0.09613\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4603/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04603: val_loss improved from 0.09613 to 0.09613, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4604/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04604: val_loss improved from 0.09613 to 0.09613, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4605/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04605: val_loss did not improve from 0.09613\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4606/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04606: val_loss improved from 0.09613 to 0.09613, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4607/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04607: val_loss did not improve from 0.09613\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4608/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04608: val_loss improved from 0.09613 to 0.09613, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4609/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04609: val_loss improved from 0.09613 to 0.09613, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4610/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04610: val_loss did not improve from 0.09613\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0965 - val_loss: 0.0961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4611/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04611: val_loss did not improve from 0.09613\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4612/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04612: val_loss did not improve from 0.09613\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4613/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04613: val_loss improved from 0.09613 to 0.09612, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4614/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04614: val_loss improved from 0.09612 to 0.09612, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4615/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04615: val_loss did not improve from 0.09612\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4616/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04616: val_loss improved from 0.09612 to 0.09612, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4617/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04617: val_loss improved from 0.09612 to 0.09612, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4618/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04618: val_loss did not improve from 0.09612\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4619/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04619: val_loss did not improve from 0.09612\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4620/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04620: val_loss improved from 0.09612 to 0.09612, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 102ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4621/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04621: val_loss did not improve from 0.09612\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4622/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04622: val_loss improved from 0.09612 to 0.09611, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4623/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04623: val_loss improved from 0.09611 to 0.09611, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 98ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4624/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04624: val_loss did not improve from 0.09611\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4625/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04625: val_loss improved from 0.09611 to 0.09611, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4626/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04626: val_loss did not improve from 0.09611\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4627/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04627: val_loss improved from 0.09611 to 0.09611, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4628/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04628: val_loss improved from 0.09611 to 0.09611, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4629/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04629: val_loss did not improve from 0.09611\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4630/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04630: val_loss did not improve from 0.09611\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4631/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04631: val_loss did not improve from 0.09611\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4632/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04632: val_loss improved from 0.09611 to 0.09610, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4633/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04633: val_loss improved from 0.09610 to 0.09610, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4634/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04634: val_loss did not improve from 0.09610\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4635/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04635: val_loss did not improve from 0.09610\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4636/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04636: val_loss improved from 0.09610 to 0.09610, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4637/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04637: val_loss improved from 0.09610 to 0.09610, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4638/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04638: val_loss improved from 0.09610 to 0.09610, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4639/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04639: val_loss did not improve from 0.09610\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4640/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04640: val_loss did not improve from 0.09610\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4641/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04641: val_loss improved from 0.09610 to 0.09609, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4642/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04642: val_loss improved from 0.09609 to 0.09609, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4643/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04643: val_loss did not improve from 0.09609\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4644/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04644: val_loss did not improve from 0.09609\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4645/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04645: val_loss did not improve from 0.09609\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4646/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04646: val_loss improved from 0.09609 to 0.09609, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4647/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04647: val_loss did not improve from 0.09609\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4648/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04648: val_loss did not improve from 0.09609\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4649/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04649: val_loss improved from 0.09609 to 0.09608, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4650/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04650: val_loss did not improve from 0.09608\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4651/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04651: val_loss did not improve from 0.09608\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4652/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0965\n",
      "Epoch 04652: val_loss improved from 0.09608 to 0.09608, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0965 - val_loss: 0.0961\n",
      "Epoch 4653/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04653: val_loss improved from 0.09608 to 0.09608, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4654/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04654: val_loss did not improve from 0.09608\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4655/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04655: val_loss did not improve from 0.09608\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4656/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04656: val_loss did not improve from 0.09608\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4657/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04657: val_loss did not improve from 0.09608\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4658/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04658: val_loss improved from 0.09608 to 0.09607, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4659/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04659: val_loss improved from 0.09607 to 0.09607, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4660/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04660: val_loss did not improve from 0.09607\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4661/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04661: val_loss did not improve from 0.09607\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4662/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04662: val_loss improved from 0.09607 to 0.09607, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4663/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04663: val_loss improved from 0.09607 to 0.09607, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4664/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04664: val_loss improved from 0.09607 to 0.09607, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4665/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04665: val_loss did not improve from 0.09607\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4666/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04666: val_loss did not improve from 0.09607\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4667/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04667: val_loss improved from 0.09607 to 0.09606, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4668/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04668: val_loss improved from 0.09606 to 0.09606, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4669/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04669: val_loss did not improve from 0.09606\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4670/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04670: val_loss did not improve from 0.09606\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4671/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04671: val_loss did not improve from 0.09606\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4672/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04672: val_loss did not improve from 0.09606\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4673/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04673: val_loss improved from 0.09606 to 0.09606, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4674/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04674: val_loss improved from 0.09606 to 0.09606, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4675/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04675: val_loss did not improve from 0.09606\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4676/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04676: val_loss improved from 0.09606 to 0.09606, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4677/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04677: val_loss did not improve from 0.09606\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4678/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04678: val_loss improved from 0.09606 to 0.09606, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4679/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04679: val_loss improved from 0.09606 to 0.09605, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4680/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04680: val_loss did not improve from 0.09605\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4681/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04681: val_loss improved from 0.09605 to 0.09605, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4682/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04682: val_loss did not improve from 0.09605\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4683/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04683: val_loss did not improve from 0.09605\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4684/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04684: val_loss improved from 0.09605 to 0.09605, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0964 - val_loss: 0.0961\n",
      "Epoch 4685/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04685: val_loss improved from 0.09605 to 0.09605, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4686/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04686: val_loss improved from 0.09605 to 0.09605, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4687/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04687: val_loss did not improve from 0.09605\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4688/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04688: val_loss improved from 0.09605 to 0.09605, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 91ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4689/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04689: val_loss did not improve from 0.09605\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4690/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04690: val_loss improved from 0.09605 to 0.09604, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4691/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04691: val_loss did not improve from 0.09604\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4692/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04692: val_loss did not improve from 0.09604\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4693/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04693: val_loss did not improve from 0.09604\n",
      "7/7 [==============================] - 1s 93ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4694/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04694: val_loss did not improve from 0.09604\n",
      "7/7 [==============================] - 1s 97ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4695/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04695: val_loss did not improve from 0.09604\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4696/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04696: val_loss improved from 0.09604 to 0.09604, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4697/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04697: val_loss improved from 0.09604 to 0.09604, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 96ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4698/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04698: val_loss did not improve from 0.09604\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4699/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04699: val_loss did not improve from 0.09604\n",
      "7/7 [==============================] - 1s 89ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4700/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04700: val_loss did not improve from 0.09604\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4701/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04701: val_loss improved from 0.09604 to 0.09603, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 95ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4702/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04702: val_loss did not improve from 0.09603\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4703/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04703: val_loss did not improve from 0.09603\n",
      "7/7 [==============================] - 1s 92ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4704/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04704: val_loss did not improve from 0.09603\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4705/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04705: val_loss did not improve from 0.09603\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4706/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04706: val_loss improved from 0.09603 to 0.09603, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4707/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04707: val_loss did not improve from 0.09603\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4708/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04708: val_loss did not improve from 0.09603\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 4709/5000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0964\n",
      "Epoch 04709: val_loss improved from 0.09603 to 0.09603, saving model to CLAMP28_0.001_Autoencoder_weight.h5\n",
      "7/7 [==============================] - 1s 94ms/step - loss: 0.0964 - val_loss: 0.0960\n",
      "Epoch 04709: early stopping\n"
     ]
    }
   ],
   "source": [
    "new_hist = model.fit(X_train, X_train,\n",
    "          batch_size = batch_size,epochs=epochs,\n",
    "          validation_data=(X_test,X_test),callbacks=[es, mc]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.2502574622631073,\n",
       "  0.24986422061920166,\n",
       "  0.24950721859931946,\n",
       "  0.24916906654834747,\n",
       "  0.24883393943309784,\n",
       "  0.24849548935890198,\n",
       "  0.24815091490745544,\n",
       "  0.24780037999153137,\n",
       "  0.2474425584077835,\n",
       "  0.24707652628421783,\n",
       "  0.2467029094696045,\n",
       "  0.24632121622562408,\n",
       "  0.24593159556388855,\n",
       "  0.24553349614143372,\n",
       "  0.24512630701065063,\n",
       "  0.2447073608636856,\n",
       "  0.24427802860736847,\n",
       "  0.24383710324764252,\n",
       "  0.24338458478450775,\n",
       "  0.2429199367761612,\n",
       "  0.2424434870481491,\n",
       "  0.24195513129234314,\n",
       "  0.2414565235376358,\n",
       "  0.24094611406326294,\n",
       "  0.24042481184005737,\n",
       "  0.23989304900169373,\n",
       "  0.23934876918792725,\n",
       "  0.23879219591617584,\n",
       "  0.23821978271007538,\n",
       "  0.23763492703437805,\n",
       "  0.23703525960445404,\n",
       "  0.236420139670372,\n",
       "  0.23578566312789917,\n",
       "  0.2351176142692566,\n",
       "  0.23441313207149506,\n",
       "  0.2336706668138504,\n",
       "  0.23289260268211365,\n",
       "  0.23201578855514526,\n",
       "  0.23083935678005219,\n",
       "  0.22895650565624237,\n",
       "  0.22647976875305176,\n",
       "  0.2242305427789688,\n",
       "  0.22247609496116638,\n",
       "  0.22093811631202698,\n",
       "  0.2193726897239685,\n",
       "  0.21776379644870758,\n",
       "  0.21617315709590912,\n",
       "  0.21464364230632782,\n",
       "  0.21317574381828308,\n",
       "  0.21173472702503204,\n",
       "  0.21031300723552704,\n",
       "  0.20893573760986328,\n",
       "  0.20759443938732147,\n",
       "  0.20627902448177338,\n",
       "  0.2049878090620041,\n",
       "  0.20374375581741333,\n",
       "  0.20253124833106995,\n",
       "  0.20136350393295288,\n",
       "  0.20025032758712769,\n",
       "  0.19919702410697937,\n",
       "  0.19819830358028412,\n",
       "  0.1972457468509674,\n",
       "  0.19635191559791565,\n",
       "  0.19550573825836182,\n",
       "  0.19468988478183746,\n",
       "  0.19393181800842285,\n",
       "  0.1931908279657364,\n",
       "  0.19248680770397186,\n",
       "  0.1918116807937622,\n",
       "  0.19116616249084473,\n",
       "  0.19055037200450897,\n",
       "  0.18995343148708344,\n",
       "  0.18937595188617706,\n",
       "  0.1888168603181839,\n",
       "  0.18825003504753113,\n",
       "  0.18769265711307526,\n",
       "  0.1871384233236313,\n",
       "  0.1865980476140976,\n",
       "  0.18606293201446533,\n",
       "  0.1855282485485077,\n",
       "  0.18499433994293213,\n",
       "  0.18445909023284912,\n",
       "  0.1839207410812378,\n",
       "  0.18338242173194885,\n",
       "  0.1828485131263733,\n",
       "  0.18230628967285156,\n",
       "  0.18176189064979553,\n",
       "  0.18121376633644104,\n",
       "  0.18066106736660004,\n",
       "  0.18010787665843964,\n",
       "  0.17954593896865845,\n",
       "  0.17897476255893707,\n",
       "  0.17839598655700684,\n",
       "  0.1778007447719574,\n",
       "  0.17720197141170502,\n",
       "  0.17656433582305908,\n",
       "  0.175893634557724,\n",
       "  0.1752028614282608,\n",
       "  0.17450222373008728,\n",
       "  0.17378740012645721,\n",
       "  0.17305876314640045,\n",
       "  0.17233940958976746,\n",
       "  0.17159603536128998,\n",
       "  0.17084535956382751,\n",
       "  0.17007392644882202,\n",
       "  0.1693103164434433,\n",
       "  0.16854450106620789,\n",
       "  0.16777461767196655,\n",
       "  0.16701403260231018,\n",
       "  0.1662471890449524,\n",
       "  0.16548317670822144,\n",
       "  0.16473306715488434,\n",
       "  0.16398659348487854,\n",
       "  0.16325515508651733,\n",
       "  0.16252581775188446,\n",
       "  0.1618157923221588,\n",
       "  0.16109883785247803,\n",
       "  0.1604124903678894,\n",
       "  0.15971747040748596,\n",
       "  0.15903633832931519,\n",
       "  0.15836133062839508,\n",
       "  0.15769898891448975,\n",
       "  0.15703360736370087,\n",
       "  0.15639014542102814,\n",
       "  0.15575556457042694,\n",
       "  0.15512065589427948,\n",
       "  0.1545080840587616,\n",
       "  0.15390467643737793,\n",
       "  0.1533142775297165,\n",
       "  0.15273021161556244,\n",
       "  0.15216436982154846,\n",
       "  0.15159979462623596,\n",
       "  0.1510549783706665,\n",
       "  0.15052425861358643,\n",
       "  0.14998847246170044,\n",
       "  0.14948129653930664,\n",
       "  0.14898115396499634,\n",
       "  0.14848875999450684,\n",
       "  0.14800608158111572,\n",
       "  0.14753709733486176,\n",
       "  0.14707204699516296,\n",
       "  0.1466122418642044,\n",
       "  0.14617136120796204,\n",
       "  0.1457386463880539,\n",
       "  0.14531493186950684,\n",
       "  0.14490389823913574,\n",
       "  0.14449888467788696,\n",
       "  0.14410725235939026,\n",
       "  0.14372403919696808,\n",
       "  0.14334943890571594,\n",
       "  0.14298057556152344,\n",
       "  0.14261925220489502,\n",
       "  0.14225530624389648,\n",
       "  0.14190447330474854,\n",
       "  0.14155426621437073,\n",
       "  0.1412132829427719,\n",
       "  0.14088654518127441,\n",
       "  0.14056704938411713,\n",
       "  0.1402469128370285,\n",
       "  0.13994622230529785,\n",
       "  0.139644593000412,\n",
       "  0.139356330037117,\n",
       "  0.1390671730041504,\n",
       "  0.13879281282424927,\n",
       "  0.13850781321525574,\n",
       "  0.13823048770427704,\n",
       "  0.13796335458755493,\n",
       "  0.13770495355129242,\n",
       "  0.1374557912349701,\n",
       "  0.13719861209392548,\n",
       "  0.13696202635765076,\n",
       "  0.13672995567321777,\n",
       "  0.13650377094745636,\n",
       "  0.13628101348876953,\n",
       "  0.13606585562229156,\n",
       "  0.13585397601127625,\n",
       "  0.13564534485340118,\n",
       "  0.13543900847434998,\n",
       "  0.1352437287569046,\n",
       "  0.1350410282611847,\n",
       "  0.1348492056131363,\n",
       "  0.13466107845306396,\n",
       "  0.13447757065296173,\n",
       "  0.13429783284664154,\n",
       "  0.13412165641784668,\n",
       "  0.13395120203495026,\n",
       "  0.13378790020942688,\n",
       "  0.133625328540802,\n",
       "  0.13346809148788452,\n",
       "  0.13330934941768646,\n",
       "  0.13316506147384644,\n",
       "  0.1330130398273468,\n",
       "  0.13286468386650085,\n",
       "  0.1327204406261444,\n",
       "  0.13257700204849243,\n",
       "  0.1324411779642105,\n",
       "  0.13230179250240326,\n",
       "  0.1321665644645691,\n",
       "  0.13203144073486328,\n",
       "  0.13190022110939026,\n",
       "  0.13177645206451416,\n",
       "  0.13163986802101135,\n",
       "  0.13151170313358307,\n",
       "  0.13138678669929504,\n",
       "  0.13126404583454132,\n",
       "  0.13114023208618164,\n",
       "  0.1310139149427414,\n",
       "  0.13089311122894287,\n",
       "  0.13077406585216522,\n",
       "  0.13065257668495178,\n",
       "  0.13053518533706665,\n",
       "  0.13041941821575165,\n",
       "  0.130301833152771,\n",
       "  0.13019049167633057,\n",
       "  0.1300729513168335,\n",
       "  0.1299622505903244,\n",
       "  0.12985260784626007,\n",
       "  0.1297408491373062,\n",
       "  0.1296304613351822,\n",
       "  0.12952664494514465,\n",
       "  0.1294192671775818,\n",
       "  0.12931641936302185,\n",
       "  0.12921075522899628,\n",
       "  0.12910495698451996,\n",
       "  0.1290050446987152,\n",
       "  0.1288990080356598,\n",
       "  0.12880079448223114,\n",
       "  0.1286974847316742,\n",
       "  0.12859758734703064,\n",
       "  0.12849363684654236,\n",
       "  0.12838990986347198,\n",
       "  0.12828128039836884,\n",
       "  0.128178671002388,\n",
       "  0.1280757039785385,\n",
       "  0.1279720813035965,\n",
       "  0.12786699831485748,\n",
       "  0.127764031291008,\n",
       "  0.12765702605247498,\n",
       "  0.12755468487739563,\n",
       "  0.12744079530239105,\n",
       "  0.12733608484268188,\n",
       "  0.12722569704055786,\n",
       "  0.12711074948310852,\n",
       "  0.12699797749519348,\n",
       "  0.12688519060611725,\n",
       "  0.12677408754825592,\n",
       "  0.12666864693164825,\n",
       "  0.12656180560588837,\n",
       "  0.12645095586776733,\n",
       "  0.1263454407453537,\n",
       "  0.12623971700668335,\n",
       "  0.1261354684829712,\n",
       "  0.1260305792093277,\n",
       "  0.1259268820285797,\n",
       "  0.12582500278949738,\n",
       "  0.12572640180587769,\n",
       "  0.1256314069032669,\n",
       "  0.12552973628044128,\n",
       "  0.1254245638847351,\n",
       "  0.12532876431941986,\n",
       "  0.12523318827152252,\n",
       "  0.12514466047286987,\n",
       "  0.1250576376914978,\n",
       "  0.12497236579656601,\n",
       "  0.12488735467195511,\n",
       "  0.12480414658784866,\n",
       "  0.12472159415483475,\n",
       "  0.12463761866092682,\n",
       "  0.12455827742815018,\n",
       "  0.12447890639305115,\n",
       "  0.12440149486064911,\n",
       "  0.12432603538036346,\n",
       "  0.12425024807453156,\n",
       "  0.12417621910572052,\n",
       "  0.12410000711679459,\n",
       "  0.12402618676424026,\n",
       "  0.12395266443490982,\n",
       "  0.12388241291046143,\n",
       "  0.12381411343812943,\n",
       "  0.1237395778298378,\n",
       "  0.12367315590381622,\n",
       "  0.12360136955976486,\n",
       "  0.1235356554389,\n",
       "  0.12347237020730972,\n",
       "  0.12340252846479416,\n",
       "  0.12333918362855911,\n",
       "  0.12327118217945099,\n",
       "  0.12320999801158905,\n",
       "  0.12314578145742416,\n",
       "  0.12307974696159363,\n",
       "  0.12301719933748245,\n",
       "  0.1229563057422638,\n",
       "  0.12289440631866455,\n",
       "  0.12283165752887726,\n",
       "  0.12276957184076309,\n",
       "  0.1227140873670578,\n",
       "  0.12265325337648392,\n",
       "  0.12259555608034134,\n",
       "  0.12253651767969131,\n",
       "  0.12247784435749054,\n",
       "  0.12242138385772705,\n",
       "  0.12236475944519043,\n",
       "  0.12230882793664932,\n",
       "  0.12225400656461716,\n",
       "  0.12219839543104172,\n",
       "  0.12214402854442596,\n",
       "  0.12208468466997147,\n",
       "  0.12203023582696915,\n",
       "  0.12197338789701462,\n",
       "  0.12191854417324066,\n",
       "  0.12186577916145325,\n",
       "  0.12181226909160614,\n",
       "  0.12175892293453217,\n",
       "  0.12170904874801636,\n",
       "  0.12165475636720657,\n",
       "  0.12160026282072067,\n",
       "  0.12155529111623764,\n",
       "  0.1215081438422203,\n",
       "  0.12145186215639114,\n",
       "  0.12140363454818726,\n",
       "  0.12135147303342819,\n",
       "  0.12129807472229004,\n",
       "  0.12124833464622498,\n",
       "  0.12119898200035095,\n",
       "  0.12115561217069626,\n",
       "  0.1211046651005745,\n",
       "  0.12105827778577805,\n",
       "  0.12100868672132492,\n",
       "  0.12096239626407623,\n",
       "  0.12091589719057083,\n",
       "  0.12086846679449081,\n",
       "  0.12083014100790024,\n",
       "  0.12077881395816803,\n",
       "  0.12073570489883423,\n",
       "  0.12068897485733032,\n",
       "  0.12064454704523087,\n",
       "  0.12060074508190155,\n",
       "  0.12055943161249161,\n",
       "  0.12051436305046082,\n",
       "  0.12046850472688675,\n",
       "  0.12042678892612457,\n",
       "  0.12038253247737885,\n",
       "  0.12034060806035995,\n",
       "  0.12029827386140823,\n",
       "  0.12025633454322815,\n",
       "  0.12021542340517044,\n",
       "  0.12017613649368286,\n",
       "  0.12013205140829086,\n",
       "  0.12009113281965256,\n",
       "  0.12005199491977692,\n",
       "  0.12001192569732666,\n",
       "  0.11997158080339432,\n",
       "  0.1199326291680336,\n",
       "  0.1198931485414505,\n",
       "  0.1198563501238823,\n",
       "  0.1198204830288887,\n",
       "  0.11977823078632355,\n",
       "  0.11973997950553894,\n",
       "  0.11970321834087372,\n",
       "  0.11966487020254135,\n",
       "  0.11962924897670746,\n",
       "  0.11959393322467804,\n",
       "  0.11955885589122772,\n",
       "  0.11952057480812073,\n",
       "  0.11948409676551819,\n",
       "  0.11945036798715591,\n",
       "  0.11941628903150558,\n",
       "  0.11937965452671051,\n",
       "  0.1193445548415184,\n",
       "  0.1193121001124382,\n",
       "  0.11927570402622223,\n",
       "  0.119242362678051,\n",
       "  0.11920928955078125,\n",
       "  0.11917641758918762,\n",
       "  0.11914120614528656,\n",
       "  0.11910867691040039,\n",
       "  0.11907637119293213,\n",
       "  0.11904528737068176,\n",
       "  0.11901126801967621,\n",
       "  0.11898116022348404,\n",
       "  0.11894597858190536,\n",
       "  0.11891752481460571,\n",
       "  0.11888864636421204,\n",
       "  0.11885891109704971,\n",
       "  0.11882265657186508,\n",
       "  0.11878956854343414,\n",
       "  0.11875931918621063,\n",
       "  0.11872918903827667,\n",
       "  0.11869896203279495,\n",
       "  0.11866824328899384,\n",
       "  0.11864639818668365,\n",
       "  0.11861171573400497,\n",
       "  0.1185806393623352,\n",
       "  0.1185510903596878,\n",
       "  0.11852430552244186,\n",
       "  0.11849408596754074,\n",
       "  0.11846626549959183,\n",
       "  0.11843972653150558,\n",
       "  0.11841066926717758,\n",
       "  0.11838261038064957,\n",
       "  0.118354432284832,\n",
       "  0.11833319067955017,\n",
       "  0.11830727756023407,\n",
       "  0.11827719956636429,\n",
       "  0.11824772506952286,\n",
       "  0.1182222068309784,\n",
       "  0.11819271743297577,\n",
       "  0.11816614866256714,\n",
       "  0.11814109236001968,\n",
       "  0.11811412125825882,\n",
       "  0.11808858811855316,\n",
       "  0.11806169152259827,\n",
       "  0.11803391575813293,\n",
       "  0.11800982058048248,\n",
       "  0.11798654496669769,\n",
       "  0.11796516180038452,\n",
       "  0.11793848127126694,\n",
       "  0.11790817975997925,\n",
       "  0.11788825690746307,\n",
       "  0.11787187308073044,\n",
       "  0.11784309148788452,\n",
       "  0.11780717968940735,\n",
       "  0.11777956038713455,\n",
       "  0.11775697767734528,\n",
       "  0.11773188412189484,\n",
       "  0.11770696192979813,\n",
       "  0.11768664419651031,\n",
       "  0.11765746772289276,\n",
       "  0.11763084679841995,\n",
       "  0.1176103949546814,\n",
       "  0.11758783459663391,\n",
       "  0.11756504327058792,\n",
       "  0.11754105985164642,\n",
       "  0.11751560121774673,\n",
       "  0.11749036610126495,\n",
       "  0.11746899038553238,\n",
       "  0.11745139956474304,\n",
       "  0.11743301153182983,\n",
       "  0.11741545051336288,\n",
       "  0.11737781018018723,\n",
       "  0.11735161393880844,\n",
       "  0.11733226478099823,\n",
       "  0.11731251329183578,\n",
       "  0.11728610843420029,\n",
       "  0.11726272851228714,\n",
       "  0.1172405406832695,\n",
       "  0.11722024530172348,\n",
       "  0.11720164120197296,\n",
       "  0.11717583984136581,\n",
       "  0.1171555295586586,\n",
       "  0.1171317994594574,\n",
       "  0.11711108684539795,\n",
       "  0.11708880215883255,\n",
       "  0.11706525087356567,\n",
       "  0.11704397201538086,\n",
       "  0.11702153086662292,\n",
       "  0.11700016260147095,\n",
       "  0.11698099970817566,\n",
       "  0.11695776879787445,\n",
       "  0.11693833023309708,\n",
       "  0.11692285537719727,\n",
       "  0.11690273135900497,\n",
       "  0.11688327044248581,\n",
       "  0.11685177683830261,\n",
       "  0.11682909727096558,\n",
       "  0.11680950224399567,\n",
       "  0.11678571999073029,\n",
       "  0.1167682632803917,\n",
       "  0.11674854904413223,\n",
       "  0.11672291904687881,\n",
       "  0.11670052260160446,\n",
       "  0.11667926609516144,\n",
       "  0.11666081100702286,\n",
       "  0.11664959043264389,\n",
       "  0.11662096530199051,\n",
       "  0.11660110950469971,\n",
       "  0.11657688021659851,\n",
       "  0.1165577843785286,\n",
       "  0.1165330782532692,\n",
       "  0.1165108010172844,\n",
       "  0.11649049818515778,\n",
       "  0.1164698526263237,\n",
       "  0.11645034700632095,\n",
       "  0.11642700433731079,\n",
       "  0.11640504002571106,\n",
       "  0.11639023572206497,\n",
       "  0.11636841297149658,\n",
       "  0.11634407937526703,\n",
       "  0.11632415652275085,\n",
       "  0.11631210148334503,\n",
       "  0.11628281325101852,\n",
       "  0.11625920236110687,\n",
       "  0.11623892933130264,\n",
       "  0.11621855199337006,\n",
       "  0.11619684100151062,\n",
       "  0.11617711186408997,\n",
       "  0.11615711450576782,\n",
       "  0.1161358505487442,\n",
       "  0.1161145567893982,\n",
       "  0.1160944253206253,\n",
       "  0.11608151346445084,\n",
       "  0.11605746299028397,\n",
       "  0.1160331517457962,\n",
       "  0.11601705849170685,\n",
       "  0.11599139869213104,\n",
       "  0.11597111821174622,\n",
       "  0.1159483790397644,\n",
       "  0.11593189835548401,\n",
       "  0.11590860784053802,\n",
       "  0.11588741838932037,\n",
       "  0.11587126553058624,\n",
       "  0.11584842205047607,\n",
       "  0.11582677066326141,\n",
       "  0.1158096194267273,\n",
       "  0.1157844141125679,\n",
       "  0.1157655268907547,\n",
       "  0.11574554443359375,\n",
       "  0.11572194844484329,\n",
       "  0.11570423096418381,\n",
       "  0.11568300426006317,\n",
       "  0.1156659722328186,\n",
       "  0.1156449168920517,\n",
       "  0.11562027782201767,\n",
       "  0.11559730023145676,\n",
       "  0.11558075994253159,\n",
       "  0.11556265503168106,\n",
       "  0.11554013192653656,\n",
       "  0.11552289128303528,\n",
       "  0.11550023406744003,\n",
       "  0.1154831051826477,\n",
       "  0.11545509099960327,\n",
       "  0.1154368594288826,\n",
       "  0.11541464179754257,\n",
       "  0.11539532989263535,\n",
       "  0.11537458747625351,\n",
       "  0.1153537705540657,\n",
       "  0.11533553153276443,\n",
       "  0.11531589180231094,\n",
       "  0.11529181152582169,\n",
       "  0.11527355015277863,\n",
       "  0.11525280773639679,\n",
       "  0.11523450165987015,\n",
       "  0.11521545052528381,\n",
       "  0.11519812792539597,\n",
       "  0.11517692357301712,\n",
       "  0.11515089869499207,\n",
       "  0.11513221263885498,\n",
       "  0.1151152029633522,\n",
       "  0.11509276181459427,\n",
       "  0.11507339775562286,\n",
       "  0.11504950374364853,\n",
       "  0.11503035575151443,\n",
       "  0.11500991880893707,\n",
       "  0.11499206721782684,\n",
       "  0.1149710938334465,\n",
       "  0.11494807153940201,\n",
       "  0.11492820084095001,\n",
       "  0.1149042397737503,\n",
       "  0.1148863136768341,\n",
       "  0.11486617475748062,\n",
       "  0.11484487354755402,\n",
       "  0.11482357233762741,\n",
       "  0.11480071395635605,\n",
       "  0.11478129029273987,\n",
       "  0.1147577315568924,\n",
       "  0.11473624408245087,\n",
       "  0.11471441388130188,\n",
       "  0.11468813568353653,\n",
       "  0.11466455459594727,\n",
       "  0.11464125663042068,\n",
       "  0.11461818963289261,\n",
       "  0.1145978569984436,\n",
       "  0.11457277089357376,\n",
       "  0.11454818397760391,\n",
       "  0.1145285815000534,\n",
       "  0.1145029067993164,\n",
       "  0.11447975784540176,\n",
       "  0.11446704715490341,\n",
       "  0.11444377899169922,\n",
       "  0.11441837251186371,\n",
       "  0.11438848823308945,\n",
       "  0.1143648624420166,\n",
       "  0.11434367299079895,\n",
       "  0.11431870609521866,\n",
       "  0.11429673433303833,\n",
       "  0.11427817493677139,\n",
       "  0.11425173282623291,\n",
       "  0.11422864347696304,\n",
       "  0.11420657485723495,\n",
       "  0.11418412625789642,\n",
       "  0.11416162550449371,\n",
       "  0.11413729190826416,\n",
       "  0.11411961168050766,\n",
       "  0.11409780383110046,\n",
       "  0.1140756830573082,\n",
       "  0.11404990404844284,\n",
       "  0.1140301376581192,\n",
       "  0.11400685459375381,\n",
       "  0.11398107558488846,\n",
       "  0.11395878344774246,\n",
       "  0.11393534392118454,\n",
       "  0.11391397565603256,\n",
       "  0.11389005184173584,\n",
       "  0.1138690933585167,\n",
       "  0.11384468525648117,\n",
       "  0.11382334679365158,\n",
       "  0.11379800736904144,\n",
       "  0.11377823352813721,\n",
       "  0.11375642567873001,\n",
       "  0.1137329712510109,\n",
       "  0.11370866000652313,\n",
       "  0.11368950456380844,\n",
       "  0.11366605013608932,\n",
       "  0.11364249885082245,\n",
       "  0.11361997574567795,\n",
       "  0.11359558999538422,\n",
       "  0.11357614398002625,\n",
       "  0.11354906111955643,\n",
       "  0.11352478712797165,\n",
       "  0.11350223422050476,\n",
       "  0.1134808212518692,\n",
       "  0.11345693469047546,\n",
       "  0.11343556642532349,\n",
       "  0.1134127601981163,\n",
       "  0.11339260637760162,\n",
       "  0.11336833983659744,\n",
       "  0.11334554851055145,\n",
       "  0.11332292854785919,\n",
       "  0.11330017447471619,\n",
       "  0.11328142881393433,\n",
       "  0.11326011270284653,\n",
       "  0.11323647201061249,\n",
       "  0.11321266740560532,\n",
       "  0.11318870633840561,\n",
       "  0.11316625773906708,\n",
       "  0.11314461380243301,\n",
       "  0.113124780356884,\n",
       "  0.11311353743076324,\n",
       "  0.1130831241607666,\n",
       "  0.11306141316890717,\n",
       "  0.11303619295358658,\n",
       "  0.11301317811012268,\n",
       "  0.11299358308315277,\n",
       "  0.11297253519296646,\n",
       "  0.11295422166585922,\n",
       "  0.11293130367994308,\n",
       "  0.11290979385375977,\n",
       "  0.11288580298423767,\n",
       "  0.11286808550357819,\n",
       "  0.1128445640206337,\n",
       "  0.11282858997583389,\n",
       "  0.11280300468206406,\n",
       "  0.11278092861175537,\n",
       "  0.11276137828826904,\n",
       "  0.11274097859859467,\n",
       "  0.11272091418504715,\n",
       "  0.11270800232887268,\n",
       "  0.11268024891614914,\n",
       "  0.11265815049409866,\n",
       "  0.11263703554868698,\n",
       "  0.11261530965566635,\n",
       "  0.11259781569242477,\n",
       "  0.11257383227348328,\n",
       "  0.1125541403889656,\n",
       "  0.11253470182418823,\n",
       "  0.11251264065504074,\n",
       "  0.11249252408742905,\n",
       "  0.11247547715902328,\n",
       "  0.11245931684970856,\n",
       "  0.11243768781423569,\n",
       "  0.11241374164819717,\n",
       "  0.11238963901996613,\n",
       "  0.11237083375453949,\n",
       "  0.11235472559928894,\n",
       "  0.11233186721801758,\n",
       "  0.11231010407209396,\n",
       "  0.11228840053081512,\n",
       "  0.11226776987314224,\n",
       "  0.11224989593029022,\n",
       "  0.11222808063030243,\n",
       "  0.11220942437648773,\n",
       "  0.11220242828130722,\n",
       "  0.1121712476015091,\n",
       "  0.11215159296989441,\n",
       "  0.11213096976280212,\n",
       "  0.11211427301168442,\n",
       "  0.11209706962108612,\n",
       "  0.11208073049783707,\n",
       "  0.1120542362332344,\n",
       "  0.11203620582818985,\n",
       "  0.11201681196689606,\n",
       "  0.11199997365474701,\n",
       "  0.11198166757822037,\n",
       "  0.11196740716695786,\n",
       "  0.11194860190153122,\n",
       "  0.11192160844802856,\n",
       "  0.11190371215343475,\n",
       "  0.1118895411491394,\n",
       "  0.11186951398849487,\n",
       "  0.11184847354888916,\n",
       "  0.1118311658501625,\n",
       "  0.11181996762752533,\n",
       "  0.11179943382740021,\n",
       "  0.11178124696016312,\n",
       "  0.11176450550556183,\n",
       "  0.11174788326025009,\n",
       "  0.11173630505800247,\n",
       "  0.11171619594097137,\n",
       "  0.11168798059225082,\n",
       "  0.11166752129793167,\n",
       "  0.11165154725313187,\n",
       "  0.11163211613893509,\n",
       "  0.11161305755376816,\n",
       "  0.11159581691026688,\n",
       "  0.11157750338315964,\n",
       "  0.11156126111745834,\n",
       "  0.11154035478830338,\n",
       "  0.11152530461549759,\n",
       "  0.1115066185593605,\n",
       "  0.1114843562245369,\n",
       "  0.11147391051054001,\n",
       "  0.11144731938838959,\n",
       "  0.11143439263105392,\n",
       "  0.11142729967832565,\n",
       "  0.1114022433757782,\n",
       "  0.1113937646150589,\n",
       "  0.11137200146913528,\n",
       "  0.11134795099496841,\n",
       "  0.11133519560098648,\n",
       "  0.1113169863820076,\n",
       "  0.11129847913980484,\n",
       "  0.1112760379910469,\n",
       "  0.11126425117254257,\n",
       "  0.11124508827924728,\n",
       "  0.11123001575469971,\n",
       "  0.11121349781751633,\n",
       "  0.11119644343852997,\n",
       "  0.11118140816688538,\n",
       "  0.11116638779640198,\n",
       "  0.11115160584449768,\n",
       "  0.11113812029361725,\n",
       "  0.11111737787723541,\n",
       "  0.11110353469848633,\n",
       "  0.1110902652144432,\n",
       "  0.11107361316680908,\n",
       "  0.11105602979660034,\n",
       "  0.11103907972574234,\n",
       "  0.11102244257926941,\n",
       "  0.11100957542657852,\n",
       "  0.11099362373352051,\n",
       "  0.11097870767116547,\n",
       "  0.11096126586198807,\n",
       "  0.11094638705253601,\n",
       "  0.11093153059482574,\n",
       "  0.11091592162847519,\n",
       "  0.11090023070573807,\n",
       "  0.11088857054710388,\n",
       "  0.11086877435445786,\n",
       "  0.11085307598114014,\n",
       "  0.11083714663982391,\n",
       "  0.11082170903682709,\n",
       "  0.110805943608284,\n",
       "  0.1107906699180603,\n",
       "  0.11077659577131271,\n",
       "  0.11076123267412186,\n",
       "  0.11074531078338623,\n",
       "  0.1107284352183342,\n",
       "  0.11071643233299255,\n",
       "  0.1106979250907898,\n",
       "  0.11068138480186462,\n",
       "  0.11066567152738571,\n",
       "  0.11065065115690231,\n",
       "  0.1106356829404831,\n",
       "  0.11062461137771606,\n",
       "  0.11060638725757599,\n",
       "  0.11059007048606873,\n",
       "  0.11057554185390472,\n",
       "  0.11056613177061081,\n",
       "  0.11054632067680359,\n",
       "  0.11053769290447235,\n",
       "  0.11051913350820541,\n",
       "  0.1105024665594101,\n",
       "  0.11048924922943115,\n",
       "  0.11047188192605972,\n",
       "  0.11045964062213898,\n",
       "  0.11044204980134964,\n",
       "  0.1104293167591095,\n",
       "  0.11041226983070374,\n",
       "  0.11040311306715012,\n",
       "  0.11038311570882797,\n",
       "  0.11036954820156097,\n",
       "  0.11035367101430893,\n",
       "  0.11033912003040314,\n",
       "  0.110325388610363,\n",
       "  0.11031071096658707,\n",
       "  0.11029575765132904,\n",
       "  0.11028622090816498,\n",
       "  0.11026915907859802,\n",
       "  0.110256627202034,\n",
       "  0.11023692786693573,\n",
       "  0.11022236943244934,\n",
       "  0.11020871251821518,\n",
       "  0.11019829660654068,\n",
       "  0.11018043756484985,\n",
       "  0.11016585677862167,\n",
       "  0.11015616357326508,\n",
       "  0.11013959348201752,\n",
       "  0.11012417078018188,\n",
       "  0.11011646687984467,\n",
       "  0.1100972592830658,\n",
       "  0.11008679121732712,\n",
       "  0.11007598787546158,\n",
       "  0.11006186157464981,\n",
       "  0.1100505068898201,\n",
       "  0.11004064977169037,\n",
       "  0.11001885682344437,\n",
       "  0.11000387370586395,\n",
       "  0.10998878628015518,\n",
       "  0.10997574031352997,\n",
       "  0.10996213555335999,\n",
       "  0.10994906723499298,\n",
       "  0.1099398136138916,\n",
       "  0.1099221482872963,\n",
       "  0.1099085733294487,\n",
       "  0.1098935455083847,\n",
       "  0.10987938940525055,\n",
       "  0.10986907035112381,\n",
       "  0.10985839366912842,\n",
       "  0.10983949154615402,\n",
       "  0.10982824116945267,\n",
       "  0.10982051491737366,\n",
       "  0.1098020151257515,\n",
       "  0.10978657752275467,\n",
       "  0.10977232456207275,\n",
       "  0.109759621322155,\n",
       "  0.10974716395139694,\n",
       "  0.10973521322011948,\n",
       "  0.1097227931022644,\n",
       "  0.10971244424581528,\n",
       "  0.10970400273799896,\n",
       "  0.10968202352523804,\n",
       "  0.1096692681312561,\n",
       "  0.10965852439403534,\n",
       "  0.10964612662792206,\n",
       "  0.10963340103626251,\n",
       "  0.10961805284023285,\n",
       "  0.10960397124290466,\n",
       "  0.10959360003471375,\n",
       "  0.10958779603242874,\n",
       "  0.10957436263561249,\n",
       "  0.10955965518951416,\n",
       "  0.10953887552022934,\n",
       "  0.10952486842870712,\n",
       "  0.10951151698827744,\n",
       "  0.10949503630399704,\n",
       "  0.10947824269533157,\n",
       "  0.10946217179298401,\n",
       "  0.10944809764623642,\n",
       "  0.10943532735109329,\n",
       "  0.1094183549284935,\n",
       "  0.10940523445606232,\n",
       "  0.10939247906208038,\n",
       "  0.10937796533107758,\n",
       "  0.109367735683918,\n",
       "  0.10934719443321228,\n",
       "  0.10933355242013931,\n",
       "  0.10931500792503357,\n",
       "  0.1092986911535263,\n",
       "  0.10928287357091904,\n",
       "  0.10926882177591324,\n",
       "  0.10925346612930298,\n",
       "  0.1092403456568718,\n",
       "  0.10922475904226303,\n",
       "  0.10921002924442291,\n",
       "  0.10919777303934097,\n",
       "  0.10918475687503815,\n",
       "  0.10916705429553986,\n",
       "  0.10915424674749374,\n",
       "  0.1091417446732521,\n",
       "  0.10912534594535828,\n",
       "  0.10911068320274353,\n",
       "  0.10909701138734818,\n",
       "  0.10908357799053192,\n",
       "  0.10906906425952911,\n",
       "  0.10905670374631882,\n",
       "  0.10904210060834885,\n",
       "  0.10902990400791168,\n",
       "  0.10901724547147751,\n",
       "  0.10900228470563889,\n",
       "  0.10899428278207779,\n",
       "  0.10897704213857651,\n",
       "  0.10896451026201248,\n",
       "  0.1089593693614006,\n",
       "  0.10894212871789932,\n",
       "  0.10892384499311447,\n",
       "  0.108912393450737,\n",
       "  0.1088959351181984,\n",
       "  0.10888355225324631,\n",
       "  0.10887452214956284,\n",
       "  0.10885968059301376,\n",
       "  0.10884661972522736,\n",
       "  0.10883495211601257,\n",
       "  0.108824722468853,\n",
       "  0.10880965739488602,\n",
       "  0.10880046337842941,\n",
       "  0.10879187285900116,\n",
       "  0.10877276957035065,\n",
       "  0.10875973850488663,\n",
       "  0.10874546319246292,\n",
       "  0.10873580724000931,\n",
       "  0.10872012376785278,\n",
       "  0.10870799422264099,\n",
       "  0.10869938135147095,\n",
       "  0.10868813842535019,\n",
       "  0.10867606848478317,\n",
       "  0.10866040736436844,\n",
       "  0.10864778608083725,\n",
       "  0.10863251239061356,\n",
       "  0.10862734913825989,\n",
       "  0.10861088335514069,\n",
       "  0.10859925299882889,\n",
       "  0.10858660191297531,\n",
       "  0.10857133567333221,\n",
       "  0.10855893790721893,\n",
       "  0.10854826122522354,\n",
       "  0.10853423923254013,\n",
       "  0.10852181166410446,\n",
       "  0.10851103812456131,\n",
       "  0.10849989205598831,\n",
       "  0.10848633199930191,\n",
       "  0.1084764301776886,\n",
       "  0.10846199095249176,\n",
       "  0.10844847559928894,\n",
       "  0.10843837261199951,\n",
       "  0.10842476785182953,\n",
       "  0.10841366648674011,\n",
       "  0.10839978605508804,\n",
       "  0.10838709026575089,\n",
       "  0.10837484151124954,\n",
       "  0.10836408287286758,\n",
       "  0.10835397988557816,\n",
       "  0.10833761096000671,\n",
       "  0.10832614451646805,\n",
       "  0.10831475257873535,\n",
       "  0.10830380022525787,\n",
       "  0.10829424113035202,\n",
       "  0.10828107595443726,\n",
       "  0.10826627165079117,\n",
       "  0.10826178640127182,\n",
       "  0.10824588686227798,\n",
       "  0.10823771357536316,\n",
       "  0.10821817815303802,\n",
       "  0.1082085594534874,\n",
       "  0.10819496959447861,\n",
       "  0.10818430781364441,\n",
       "  0.10816922783851624,\n",
       "  0.10815926641225815,\n",
       "  0.10814641416072845,\n",
       "  0.10813511162996292,\n",
       "  0.10812295973300934,\n",
       "  0.10810651630163193,\n",
       "  0.1080934926867485,\n",
       "  0.10808458179235458,\n",
       "  0.10806956887245178,\n",
       "  0.10805786401033401,\n",
       "  0.10804988443851471,\n",
       "  0.10803341865539551,\n",
       "  0.10802404582500458,\n",
       "  0.10801132768392563,\n",
       "  0.10800478607416153,\n",
       "  0.10799260437488556,\n",
       "  0.10797984898090363,\n",
       "  0.10797300934791565,\n",
       "  0.10795768350362778,\n",
       "  0.10794412344694138,\n",
       "  0.10793498158454895,\n",
       "  0.10792266577482224,\n",
       "  0.10791802406311035,\n",
       "  0.10789921879768372,\n",
       "  0.10788989812135696,\n",
       "  0.10787581652402878,\n",
       "  0.10786762088537216,\n",
       "  0.10785435140132904,\n",
       "  0.1078447550535202,\n",
       "  0.10783638060092926,\n",
       "  0.10783172398805618,\n",
       "  0.10781386494636536,\n",
       "  0.10780195891857147,\n",
       "  0.1077909842133522,\n",
       "  0.10778044164180756,\n",
       "  0.10776790231466293,\n",
       "  0.10775715112686157,\n",
       "  0.10774775594472885,\n",
       "  0.10773215442895889,\n",
       "  0.10772344470024109,\n",
       "  0.10771214216947556,\n",
       "  0.10770172625780106,\n",
       "  0.10769104212522507,\n",
       "  0.10768232494592667,\n",
       "  0.10767004638910294,\n",
       "  ...],\n",
       " 'val_loss': [0.2500217854976654,\n",
       "  0.24965280294418335,\n",
       "  0.2493094503879547,\n",
       "  0.24897366762161255,\n",
       "  0.24863675236701965,\n",
       "  0.24829289317131042,\n",
       "  0.24794402718544006,\n",
       "  0.24758996069431305,\n",
       "  0.24722814559936523,\n",
       "  0.24685847759246826,\n",
       "  0.24648137390613556,\n",
       "  0.24609647691249847,\n",
       "  0.24570205807685852,\n",
       "  0.24529969692230225,\n",
       "  0.24488632380962372,\n",
       "  0.2444610297679901,\n",
       "  0.24402554333209991,\n",
       "  0.24357792735099792,\n",
       "  0.24311821162700653,\n",
       "  0.2426469475030899,\n",
       "  0.2421633005142212,\n",
       "  0.24166840314865112,\n",
       "  0.24116189777851105,\n",
       "  0.2406446784734726,\n",
       "  0.24011634290218353,\n",
       "  0.23957522213459015,\n",
       "  0.23902124166488647,\n",
       "  0.23845258355140686,\n",
       "  0.23786814510822296,\n",
       "  0.23726792633533478,\n",
       "  0.23665349185466766,\n",
       "  0.23602187633514404,\n",
       "  0.2353583574295044,\n",
       "  0.2346612513065338,\n",
       "  0.23392611742019653,\n",
       "  0.23316115140914917,\n",
       "  0.2323346585035324,\n",
       "  0.23135550320148468,\n",
       "  0.22976741194725037,\n",
       "  0.227477565407753,\n",
       "  0.2250557541847229,\n",
       "  0.22322623431682587,\n",
       "  0.22168748080730438,\n",
       "  0.2201475352048874,\n",
       "  0.21855568885803223,\n",
       "  0.21692417562007904,\n",
       "  0.2153385579586029,\n",
       "  0.21381407976150513,\n",
       "  0.21234385669231415,\n",
       "  0.21090592443943024,\n",
       "  0.20950612425804138,\n",
       "  0.2081311047077179,\n",
       "  0.2067817747592926,\n",
       "  0.2054707258939743,\n",
       "  0.20420001447200775,\n",
       "  0.20296604931354523,\n",
       "  0.2017766684293747,\n",
       "  0.20063775777816772,\n",
       "  0.19955748319625854,\n",
       "  0.1985238790512085,\n",
       "  0.19754666090011597,\n",
       "  0.19662785530090332,\n",
       "  0.19575601816177368,\n",
       "  0.1949228048324585,\n",
       "  0.19413243234157562,\n",
       "  0.19336703419685364,\n",
       "  0.19263596832752228,\n",
       "  0.19193674623966217,\n",
       "  0.19126567244529724,\n",
       "  0.19062453508377075,\n",
       "  0.1900050938129425,\n",
       "  0.18940788507461548,\n",
       "  0.1888306587934494,\n",
       "  0.1882486492395401,\n",
       "  0.18767352402210236,\n",
       "  0.1871057003736496,\n",
       "  0.1865498125553131,\n",
       "  0.1860010176897049,\n",
       "  0.1854555755853653,\n",
       "  0.18490496277809143,\n",
       "  0.18435516953468323,\n",
       "  0.18380610644817352,\n",
       "  0.1832597702741623,\n",
       "  0.18271538615226746,\n",
       "  0.1821652352809906,\n",
       "  0.18161171674728394,\n",
       "  0.1810540109872818,\n",
       "  0.18049880862236023,\n",
       "  0.1799386441707611,\n",
       "  0.17936906218528748,\n",
       "  0.17879071831703186,\n",
       "  0.1782066375017166,\n",
       "  0.17761121690273285,\n",
       "  0.17700351774692535,\n",
       "  0.17637592554092407,\n",
       "  0.17571215331554413,\n",
       "  0.17501798272132874,\n",
       "  0.17430737614631653,\n",
       "  0.17358800768852234,\n",
       "  0.1728588044643402,\n",
       "  0.17213095724582672,\n",
       "  0.17138122022151947,\n",
       "  0.17061716318130493,\n",
       "  0.16984038054943085,\n",
       "  0.16907155513763428,\n",
       "  0.16829752922058105,\n",
       "  0.16751880943775177,\n",
       "  0.16674551367759705,\n",
       "  0.1659645140171051,\n",
       "  0.16519109904766083,\n",
       "  0.16442683339118958,\n",
       "  0.16367168724536896,\n",
       "  0.16291752457618713,\n",
       "  0.16217607259750366,\n",
       "  0.16145509481430054,\n",
       "  0.16073593497276306,\n",
       "  0.1600371152162552,\n",
       "  0.1593317985534668,\n",
       "  0.15863946080207825,\n",
       "  0.15795697271823883,\n",
       "  0.15728271007537842,\n",
       "  0.15661075711250305,\n",
       "  0.1559535712003708,\n",
       "  0.15530475974082947,\n",
       "  0.15466557443141937,\n",
       "  0.15404319763183594,\n",
       "  0.15343064069747925,\n",
       "  0.15283530950546265,\n",
       "  0.1522497534751892,\n",
       "  0.1516723483800888,\n",
       "  0.15110555291175842,\n",
       "  0.15055236220359802,\n",
       "  0.15001069009304047,\n",
       "  0.14947649836540222,\n",
       "  0.14896142482757568,\n",
       "  0.1484537422657013,\n",
       "  0.1479588896036148,\n",
       "  0.1474756896495819,\n",
       "  0.14700858294963837,\n",
       "  0.1465432196855545,\n",
       "  0.1460912674665451,\n",
       "  0.14565357565879822,\n",
       "  0.14521870017051697,\n",
       "  0.14479485154151917,\n",
       "  0.14438365399837494,\n",
       "  0.14398349821567535,\n",
       "  0.1435967981815338,\n",
       "  0.1432148814201355,\n",
       "  0.14284329116344452,\n",
       "  0.14248034358024597,\n",
       "  0.14212322235107422,\n",
       "  0.1417693942785263,\n",
       "  0.14142033457756042,\n",
       "  0.1410774439573288,\n",
       "  0.14074371755123138,\n",
       "  0.1404169350862503,\n",
       "  0.14009957015514374,\n",
       "  0.1397927850484848,\n",
       "  0.139490008354187,\n",
       "  0.13919267058372498,\n",
       "  0.1389041244983673,\n",
       "  0.13862337172031403,\n",
       "  0.13834618031978607,\n",
       "  0.13806717097759247,\n",
       "  0.13779659569263458,\n",
       "  0.1375316083431244,\n",
       "  0.13727203011512756,\n",
       "  0.13701346516609192,\n",
       "  0.13676077127456665,\n",
       "  0.13652068376541138,\n",
       "  0.13628488779067993,\n",
       "  0.13605600595474243,\n",
       "  0.13583241403102875,\n",
       "  0.1356191486120224,\n",
       "  0.13540826737880707,\n",
       "  0.13520421087741852,\n",
       "  0.1350044310092926,\n",
       "  0.13480857014656067,\n",
       "  0.1346164345741272,\n",
       "  0.13443182408809662,\n",
       "  0.13425002992153168,\n",
       "  0.13407030701637268,\n",
       "  0.13389667868614197,\n",
       "  0.13372519612312317,\n",
       "  0.1335594654083252,\n",
       "  0.13339608907699585,\n",
       "  0.13323695957660675,\n",
       "  0.13308081030845642,\n",
       "  0.13293035328388214,\n",
       "  0.1327788233757019,\n",
       "  0.1326318085193634,\n",
       "  0.13248507678508759,\n",
       "  0.13234247267246246,\n",
       "  0.13219963014125824,\n",
       "  0.13205939531326294,\n",
       "  0.13192163407802582,\n",
       "  0.13178637623786926,\n",
       "  0.13165420293807983,\n",
       "  0.13152332603931427,\n",
       "  0.13139645755290985,\n",
       "  0.13126811385154724,\n",
       "  0.1311425417661667,\n",
       "  0.13101693987846375,\n",
       "  0.13089250028133392,\n",
       "  0.1307714432477951,\n",
       "  0.13065241277217865,\n",
       "  0.13053667545318604,\n",
       "  0.13041821122169495,\n",
       "  0.13030250370502472,\n",
       "  0.13018733263015747,\n",
       "  0.13007599115371704,\n",
       "  0.12996304035186768,\n",
       "  0.12985219061374664,\n",
       "  0.12974421679973602,\n",
       "  0.12963521480560303,\n",
       "  0.12953045964241028,\n",
       "  0.129424586892128,\n",
       "  0.12932188808918,\n",
       "  0.12922023236751556,\n",
       "  0.12911520898342133,\n",
       "  0.1290145367383957,\n",
       "  0.12891198694705963,\n",
       "  0.1288122832775116,\n",
       "  0.12871383130550385,\n",
       "  0.12860997021198273,\n",
       "  0.12851420044898987,\n",
       "  0.12840966880321503,\n",
       "  0.12830978631973267,\n",
       "  0.12820649147033691,\n",
       "  0.12810224294662476,\n",
       "  0.12799707055091858,\n",
       "  0.12789386510849,\n",
       "  0.12779372930526733,\n",
       "  0.12768718600273132,\n",
       "  0.12758031487464905,\n",
       "  0.12747664749622345,\n",
       "  0.12737320363521576,\n",
       "  0.12726835906505585,\n",
       "  0.12716339528560638,\n",
       "  0.12705546617507935,\n",
       "  0.12694595754146576,\n",
       "  0.12682807445526123,\n",
       "  0.1267194151878357,\n",
       "  0.1265982687473297,\n",
       "  0.1264900118112564,\n",
       "  0.126381978392601,\n",
       "  0.12627001106739044,\n",
       "  0.1261538565158844,\n",
       "  0.12604358792304993,\n",
       "  0.12593288719654083,\n",
       "  0.1258203089237213,\n",
       "  0.12571275234222412,\n",
       "  0.12560401856899261,\n",
       "  0.125496968626976,\n",
       "  0.12538987398147583,\n",
       "  0.12529011070728302,\n",
       "  0.125183567404747,\n",
       "  0.12508319318294525,\n",
       "  0.1249871626496315,\n",
       "  0.12489137798547745,\n",
       "  0.12479353696107864,\n",
       "  0.12470553070306778,\n",
       "  0.12462271004915237,\n",
       "  0.12453384697437286,\n",
       "  0.12444745749235153,\n",
       "  0.12436679005622864,\n",
       "  0.1242830753326416,\n",
       "  0.1242065280675888,\n",
       "  0.12412093579769135,\n",
       "  0.12404412031173706,\n",
       "  0.12396636605262756,\n",
       "  0.12388930469751358,\n",
       "  0.12381169199943542,\n",
       "  0.12373750656843185,\n",
       "  0.1236647367477417,\n",
       "  0.12358876317739487,\n",
       "  0.12351886928081512,\n",
       "  0.12344659864902496,\n",
       "  0.12337607890367508,\n",
       "  0.12330551445484161,\n",
       "  0.12323518842458725,\n",
       "  0.12317225337028503,\n",
       "  0.1230987012386322,\n",
       "  0.12303005158901215,\n",
       "  0.1229633241891861,\n",
       "  0.12289705872535706,\n",
       "  0.12283127009868622,\n",
       "  0.12276757508516312,\n",
       "  0.12270575761795044,\n",
       "  0.12264124304056168,\n",
       "  0.12257800251245499,\n",
       "  0.12251316756010056,\n",
       "  0.12245147675275803,\n",
       "  0.12238872796297073,\n",
       "  0.12233074009418488,\n",
       "  0.12226756662130356,\n",
       "  0.12220703810453415,\n",
       "  0.12214625626802444,\n",
       "  0.12208464741706848,\n",
       "  0.12202265858650208,\n",
       "  0.12196347117424011,\n",
       "  0.12190733850002289,\n",
       "  0.1218494325876236,\n",
       "  0.12179087847471237,\n",
       "  0.12172950804233551,\n",
       "  0.12167798727750778,\n",
       "  0.12161076813936234,\n",
       "  0.12155366688966751,\n",
       "  0.12149963527917862,\n",
       "  0.1214371770620346,\n",
       "  0.12138541042804718,\n",
       "  0.12132541835308075,\n",
       "  0.12127332389354706,\n",
       "  0.12122616916894913,\n",
       "  0.12116605788469315,\n",
       "  0.12111303955316544,\n",
       "  0.12107158452272415,\n",
       "  0.1210067868232727,\n",
       "  0.12097090482711792,\n",
       "  0.12090333551168442,\n",
       "  0.12084341049194336,\n",
       "  0.12080097198486328,\n",
       "  0.12074112147092819,\n",
       "  0.12069028615951538,\n",
       "  0.12065259367227554,\n",
       "  0.12059617787599564,\n",
       "  0.1205446869134903,\n",
       "  0.12049882113933563,\n",
       "  0.12044612318277359,\n",
       "  0.12040156126022339,\n",
       "  0.1203535944223404,\n",
       "  0.12030903249979019,\n",
       "  0.12026464939117432,\n",
       "  0.12021242082118988,\n",
       "  0.12016763538122177,\n",
       "  0.12012305110692978,\n",
       "  0.12007596343755722,\n",
       "  0.12003422528505325,\n",
       "  0.11998636275529861,\n",
       "  0.11994220316410065,\n",
       "  0.11990151554346085,\n",
       "  0.11985397338867188,\n",
       "  0.11981276422739029,\n",
       "  0.11977209895849228,\n",
       "  0.1197248324751854,\n",
       "  0.11968923360109329,\n",
       "  0.11964196711778641,\n",
       "  0.11960183084011078,\n",
       "  0.11956270784139633,\n",
       "  0.11951952427625656,\n",
       "  0.11948414891958237,\n",
       "  0.11943820863962173,\n",
       "  0.11940058320760727,\n",
       "  0.11935855448246002,\n",
       "  0.1193230077624321,\n",
       "  0.11927875876426697,\n",
       "  0.11924407631158829,\n",
       "  0.11920298635959625,\n",
       "  0.11916415393352509,\n",
       "  0.11912880837917328,\n",
       "  0.11908915638923645,\n",
       "  0.11905423551797867,\n",
       "  0.11902318894863129,\n",
       "  0.1189800575375557,\n",
       "  0.11894703656435013,\n",
       "  0.1189093366265297,\n",
       "  0.11887047439813614,\n",
       "  0.11883941292762756,\n",
       "  0.11879909783601761,\n",
       "  0.11876460164785385,\n",
       "  0.11873255670070648,\n",
       "  0.11869855970144272,\n",
       "  0.11866074800491333,\n",
       "  0.11862590909004211,\n",
       "  0.11859497427940369,\n",
       "  0.11856109648942947,\n",
       "  0.11852486431598663,\n",
       "  0.11849678307771683,\n",
       "  0.11846455186605453,\n",
       "  0.1184302344918251,\n",
       "  0.11840125918388367,\n",
       "  0.11836975067853928,\n",
       "  0.1183338463306427,\n",
       "  0.11830727756023407,\n",
       "  0.11826959252357483,\n",
       "  0.11824418604373932,\n",
       "  0.11820883303880692,\n",
       "  0.11818293482065201,\n",
       "  0.11815008521080017,\n",
       "  0.11812750995159149,\n",
       "  0.11809340119361877,\n",
       "  0.11806435883045197,\n",
       "  0.11802970618009567,\n",
       "  0.1180097907781601,\n",
       "  0.11797287315130234,\n",
       "  0.1179451122879982,\n",
       "  0.11792316287755966,\n",
       "  0.1178874522447586,\n",
       "  0.11786089092493057,\n",
       "  0.11783362925052643,\n",
       "  0.1178051307797432,\n",
       "  0.11778150498867035,\n",
       "  0.11774744093418121,\n",
       "  0.1177312508225441,\n",
       "  0.11769507080316544,\n",
       "  0.11767121404409409,\n",
       "  0.1176430806517601,\n",
       "  0.11761610954999924,\n",
       "  0.11759372055530548,\n",
       "  0.11756009608507156,\n",
       "  0.11753352731466293,\n",
       "  0.11750964820384979,\n",
       "  0.11748246103525162,\n",
       "  0.1174667477607727,\n",
       "  0.11743348836898804,\n",
       "  0.1174061968922615,\n",
       "  0.11739218235015869,\n",
       "  0.11735300719738007,\n",
       "  0.11733294278383255,\n",
       "  0.11733968555927277,\n",
       "  0.11727875471115112,\n",
       "  0.11725567281246185,\n",
       "  0.11723817884922028,\n",
       "  0.1172051951289177,\n",
       "  0.11719880998134613,\n",
       "  0.11715982109308243,\n",
       "  0.11714702099561691,\n",
       "  0.11711029708385468,\n",
       "  0.11709139496088028,\n",
       "  0.11706525832414627,\n",
       "  0.11704126000404358,\n",
       "  0.11702229082584381,\n",
       "  0.11699732393026352,\n",
       "  0.11696960777044296,\n",
       "  0.11695312708616257,\n",
       "  0.11692748218774796,\n",
       "  0.11690957844257355,\n",
       "  0.11691708117723465,\n",
       "  0.11685500293970108,\n",
       "  0.11683614552021027,\n",
       "  0.11681181192398071,\n",
       "  0.11678678542375565,\n",
       "  0.11677346378564835,\n",
       "  0.11674974858760834,\n",
       "  0.1167224794626236,\n",
       "  0.11670859903097153,\n",
       "  0.11667896807193756,\n",
       "  0.1166585311293602,\n",
       "  0.11664661765098572,\n",
       "  0.11661789566278458,\n",
       "  0.11660640686750412,\n",
       "  0.11657319962978363,\n",
       "  0.11655005812644958,\n",
       "  0.11653319001197815,\n",
       "  0.11650868505239487,\n",
       "  0.11648999899625778,\n",
       "  0.116462342441082,\n",
       "  0.11644333600997925,\n",
       "  0.11642469465732574,\n",
       "  0.11639922112226486,\n",
       "  0.11637584120035172,\n",
       "  0.11639093607664108,\n",
       "  0.11633402854204178,\n",
       "  0.11631378531455994,\n",
       "  0.11629163473844528,\n",
       "  0.11627166718244553,\n",
       "  0.11624686419963837,\n",
       "  0.11624449491500854,\n",
       "  0.11620622873306274,\n",
       "  0.11618448793888092,\n",
       "  0.11616390198469162,\n",
       "  0.11614655703306198,\n",
       "  0.11612041294574738,\n",
       "  0.11611634492874146,\n",
       "  0.11607722193002701,\n",
       "  0.11605406552553177,\n",
       "  0.1160333976149559,\n",
       "  0.11603117734193802,\n",
       "  0.11599074304103851,\n",
       "  0.11597945541143417,\n",
       "  0.11595027893781662,\n",
       "  0.11594323813915253,\n",
       "  0.11590688675642014,\n",
       "  0.11589384824037552,\n",
       "  0.11586576700210571,\n",
       "  0.11584506183862686,\n",
       "  0.11582859605550766,\n",
       "  0.1158004105091095,\n",
       "  0.11580517143011093,\n",
       "  0.11576219648122787,\n",
       "  0.1157374307513237,\n",
       "  0.11572343111038208,\n",
       "  0.11569785326719284,\n",
       "  0.11567896604537964,\n",
       "  0.11565669625997543,\n",
       "  0.11563549190759659,\n",
       "  0.11561363935470581,\n",
       "  0.11559762805700302,\n",
       "  0.11557336151599884,\n",
       "  0.11557447165250778,\n",
       "  0.1155296340584755,\n",
       "  0.115509994328022,\n",
       "  0.1155041977763176,\n",
       "  0.11546745896339417,\n",
       "  0.11545178294181824,\n",
       "  0.11542961001396179,\n",
       "  0.11540916562080383,\n",
       "  0.11538554728031158,\n",
       "  0.11536967009305954,\n",
       "  0.11534406244754791,\n",
       "  0.11532992869615555,\n",
       "  0.11530912667512894,\n",
       "  0.11528384685516357,\n",
       "  0.11526854336261749,\n",
       "  0.11524047702550888,\n",
       "  0.11522099375724792,\n",
       "  0.11520735174417496,\n",
       "  0.11517827957868576,\n",
       "  0.1151563823223114,\n",
       "  0.11514877527952194,\n",
       "  0.11511477828025818,\n",
       "  0.11509531736373901,\n",
       "  0.11507643014192581,\n",
       "  0.11505348980426788,\n",
       "  0.11503270268440247,\n",
       "  0.11503196507692337,\n",
       "  0.11499260365962982,\n",
       "  0.11497394740581512,\n",
       "  0.1149643212556839,\n",
       "  0.11493135243654251,\n",
       "  0.1149146780371666,\n",
       "  0.11489221453666687,\n",
       "  0.11487220972776413,\n",
       "  0.11485037207603455,\n",
       "  0.11483055353164673,\n",
       "  0.11481042206287384,\n",
       "  0.11479674279689789,\n",
       "  0.11476802080869675,\n",
       "  0.11475614458322525,\n",
       "  0.1147293746471405,\n",
       "  0.11470591276884079,\n",
       "  0.11468778550624847,\n",
       "  0.11467179656028748,\n",
       "  0.11466563493013382,\n",
       "  0.11462612450122833,\n",
       "  0.11460664868354797,\n",
       "  0.11458801478147507,\n",
       "  0.11456767469644547,\n",
       "  0.11456289142370224,\n",
       "  0.11452631652355194,\n",
       "  0.11450639367103577,\n",
       "  0.11449719965457916,\n",
       "  0.11446531862020493,\n",
       "  0.11444686353206635,\n",
       "  0.11443382501602173,\n",
       "  0.11440357565879822,\n",
       "  0.11438450217247009,\n",
       "  0.11436301469802856,\n",
       "  0.11434271186590195,\n",
       "  0.11432600766420364,\n",
       "  0.1143031045794487,\n",
       "  0.11428172886371613,\n",
       "  0.11426088958978653,\n",
       "  0.11423705518245697,\n",
       "  0.11422266066074371,\n",
       "  0.11418526619672775,\n",
       "  0.11416973918676376,\n",
       "  0.1141376718878746,\n",
       "  0.11411871761083603,\n",
       "  0.11409447342157364,\n",
       "  0.11406462639570236,\n",
       "  0.11405076086521149,\n",
       "  0.11402340233325958,\n",
       "  0.11399564146995544,\n",
       "  0.11397256702184677,\n",
       "  0.11396057158708572,\n",
       "  0.11392553150653839,\n",
       "  0.11392169445753098,\n",
       "  0.11389363557100296,\n",
       "  0.11385425925254822,\n",
       "  0.11383893340826035,\n",
       "  0.11380892246961594,\n",
       "  0.11379225552082062,\n",
       "  0.11376632004976273,\n",
       "  0.11374069005250931,\n",
       "  0.11373066157102585,\n",
       "  0.1136968806385994,\n",
       "  0.11367256939411163,\n",
       "  0.11366396397352219,\n",
       "  0.11362507939338684,\n",
       "  0.1136016994714737,\n",
       "  0.11358889192342758,\n",
       "  0.11356083303689957,\n",
       "  0.11355701833963394,\n",
       "  0.11351535469293594,\n",
       "  0.11349011957645416,\n",
       "  0.11348878592252731,\n",
       "  0.11344446241855621,\n",
       "  0.11342848092317581,\n",
       "  0.11339953541755676,\n",
       "  0.11337956041097641,\n",
       "  0.11335615068674088,\n",
       "  0.11333578079938889,\n",
       "  0.1133127212524414,\n",
       "  0.11328493058681488,\n",
       "  0.11326713114976883,\n",
       "  0.11323702335357666,\n",
       "  0.1132139340043068,\n",
       "  0.11320379376411438,\n",
       "  0.11317078769207001,\n",
       "  0.11314749717712402,\n",
       "  0.11313370615243912,\n",
       "  0.1131073534488678,\n",
       "  0.11307907104492188,\n",
       "  0.11305385082960129,\n",
       "  0.11305300891399384,\n",
       "  0.11300761997699738,\n",
       "  0.11298581957817078,\n",
       "  0.11296433210372925,\n",
       "  0.11294103413820267,\n",
       "  0.11291691660881042,\n",
       "  0.11289285868406296,\n",
       "  0.11287924647331238,\n",
       "  0.11284758150577545,\n",
       "  0.11282631009817123,\n",
       "  0.11280550807714462,\n",
       "  0.11277782917022705,\n",
       "  0.11275637149810791,\n",
       "  0.11273467540740967,\n",
       "  0.11274275183677673,\n",
       "  0.11268889904022217,\n",
       "  0.11266916990280151,\n",
       "  0.11264779418706894,\n",
       "  0.11262081563472748,\n",
       "  0.11259955912828445,\n",
       "  0.1125837117433548,\n",
       "  0.11256422102451324,\n",
       "  0.11253813654184341,\n",
       "  0.11252222955226898,\n",
       "  0.11248882114887238,\n",
       "  0.11246806383132935,\n",
       "  0.11245016753673553,\n",
       "  0.11242369562387466,\n",
       "  0.11240265518426895,\n",
       "  0.11239711195230484,\n",
       "  0.1123596578836441,\n",
       "  0.11234081536531448,\n",
       "  0.11232787370681763,\n",
       "  0.11229775100946426,\n",
       "  0.11227486282587051,\n",
       "  0.11225780099630356,\n",
       "  0.11224323511123657,\n",
       "  0.11221441626548767,\n",
       "  0.11220207810401917,\n",
       "  0.11217751353979111,\n",
       "  0.11216448247432709,\n",
       "  0.11214251071214676,\n",
       "  0.11211279034614563,\n",
       "  0.11209136247634888,\n",
       "  0.11207643896341324,\n",
       "  0.11205624043941498,\n",
       "  0.11203131079673767,\n",
       "  0.11201335489749908,\n",
       "  0.11199533939361572,\n",
       "  0.11197087913751602,\n",
       "  0.11195310950279236,\n",
       "  0.11193326860666275,\n",
       "  0.11194541305303574,\n",
       "  0.11189218610525131,\n",
       "  0.11187165975570679,\n",
       "  0.11185382306575775,\n",
       "  0.11183269321918488,\n",
       "  0.11180996894836426,\n",
       "  0.1118100956082344,\n",
       "  0.11177096515893936,\n",
       "  0.11175128817558289,\n",
       "  0.11173198372125626,\n",
       "  0.11171521246433258,\n",
       "  0.11169426143169403,\n",
       "  0.11167948693037033,\n",
       "  0.11166387796401978,\n",
       "  0.11163965612649918,\n",
       "  0.11162105947732925,\n",
       "  0.1115950345993042,\n",
       "  0.11157628148794174,\n",
       "  0.11156453937292099,\n",
       "  0.11155794560909271,\n",
       "  0.11152169108390808,\n",
       "  0.1115034818649292,\n",
       "  0.11148601025342941,\n",
       "  0.11147648841142654,\n",
       "  0.11144588887691498,\n",
       "  0.11143486946821213,\n",
       "  0.11143267899751663,\n",
       "  0.11139241605997086,\n",
       "  0.11137325316667557,\n",
       "  0.11136992275714874,\n",
       "  0.11133734881877899,\n",
       "  0.1113196462392807,\n",
       "  0.11130966246128082,\n",
       "  0.11128383129835129,\n",
       "  0.11126517504453659,\n",
       "  0.11125858128070831,\n",
       "  0.11123602092266083,\n",
       "  0.11122111231088638,\n",
       "  0.11120035499334335,\n",
       "  0.11120431125164032,\n",
       "  0.11115846782922745,\n",
       "  0.11114031076431274,\n",
       "  0.11112874001264572,\n",
       "  0.11110389977693558,\n",
       "  0.11108584702014923,\n",
       "  0.11107785254716873,\n",
       "  0.11104734987020493,\n",
       "  0.1110338643193245,\n",
       "  0.11101818084716797,\n",
       "  0.11100281774997711,\n",
       "  0.1109808161854744,\n",
       "  0.1109582930803299,\n",
       "  0.11096451431512833,\n",
       "  0.1109248697757721,\n",
       "  0.11090841889381409,\n",
       "  0.11092116683721542,\n",
       "  0.11087203025817871,\n",
       "  0.11085931956768036,\n",
       "  0.11088069528341293,\n",
       "  0.11082461476325989,\n",
       "  0.11080840975046158,\n",
       "  0.11079289019107819,\n",
       "  0.11078912019729614,\n",
       "  0.11075921356678009,\n",
       "  0.11074195057153702,\n",
       "  0.11072944104671478,\n",
       "  0.11071564257144928,\n",
       "  0.11069319397211075,\n",
       "  0.11067689210176468,\n",
       "  0.11066193133592606,\n",
       "  0.11065425723791122,\n",
       "  0.11063259840011597,\n",
       "  0.11061973869800568,\n",
       "  0.11061862856149673,\n",
       "  0.11058495193719864,\n",
       "  0.11056934297084808,\n",
       "  0.1105692908167839,\n",
       "  0.11054161190986633,\n",
       "  0.11053566634654999,\n",
       "  0.11051025986671448,\n",
       "  0.11050306260585785,\n",
       "  0.11047758162021637,\n",
       "  0.11046420782804489,\n",
       "  0.11045441776514053,\n",
       "  0.11043337732553482,\n",
       "  0.11042141914367676,\n",
       "  0.11040518432855606,\n",
       "  0.11038985103368759,\n",
       "  0.11038681864738464,\n",
       "  0.11036032438278198,\n",
       "  0.11034467071294785,\n",
       "  0.11033458262681961,\n",
       "  0.1103147640824318,\n",
       "  0.11030065268278122,\n",
       "  0.1102859154343605,\n",
       "  0.11027472466230392,\n",
       "  0.11026430130004883,\n",
       "  0.11023770272731781,\n",
       "  0.1102270632982254,\n",
       "  0.11021880060434341,\n",
       "  0.11019409447908401,\n",
       "  0.11018047481775284,\n",
       "  0.11016284674406052,\n",
       "  0.11015171557664871,\n",
       "  0.11013167351484299,\n",
       "  0.1101369857788086,\n",
       "  0.11010272055864334,\n",
       "  0.11009173840284348,\n",
       "  0.11007332056760788,\n",
       "  0.11007210612297058,\n",
       "  0.11005081981420517,\n",
       "  0.11002696305513382,\n",
       "  0.11002632230520248,\n",
       "  0.11000455170869827,\n",
       "  0.10999324917793274,\n",
       "  0.10997354239225388,\n",
       "  0.10995615273714066,\n",
       "  0.1099461168050766,\n",
       "  0.10993797332048416,\n",
       "  0.10991410911083221,\n",
       "  0.10991305112838745,\n",
       "  0.10988403111696243,\n",
       "  0.10987112671136856,\n",
       "  0.10985986888408661,\n",
       "  0.10984022915363312,\n",
       "  0.10983119159936905,\n",
       "  0.10981456935405731,\n",
       "  0.10980254411697388,\n",
       "  0.10979267954826355,\n",
       "  0.10977198928594589,\n",
       "  0.1097552478313446,\n",
       "  0.10974588990211487,\n",
       "  0.10973714292049408,\n",
       "  0.10971631109714508,\n",
       "  0.1097152829170227,\n",
       "  0.1096879318356514,\n",
       "  0.10967494547367096,\n",
       "  0.10966603457927704,\n",
       "  0.10964608937501907,\n",
       "  0.10963408648967743,\n",
       "  0.10963799804449081,\n",
       "  0.10960458964109421,\n",
       "  0.10960856825113297,\n",
       "  0.10958707332611084,\n",
       "  0.109563909471035,\n",
       "  0.10955458879470825,\n",
       "  0.10953883826732635,\n",
       "  0.10954215377569199,\n",
       "  0.10952162742614746,\n",
       "  0.10949976742267609,\n",
       "  0.10948596149682999,\n",
       "  0.10947584360837936,\n",
       "  0.10946592688560486,\n",
       "  0.10945471376180649,\n",
       "  0.1094307079911232,\n",
       "  0.1094183623790741,\n",
       "  0.10940726101398468,\n",
       "  0.10939377546310425,\n",
       "  0.1093822568655014,\n",
       "  0.10938651114702225,\n",
       "  0.10935068875551224,\n",
       "  0.1093360036611557,\n",
       "  0.10932757705450058,\n",
       "  0.10932902991771698,\n",
       "  0.10930125415325165,\n",
       "  0.10928347706794739,\n",
       "  0.10927864909172058,\n",
       "  0.10926268994808197,\n",
       "  0.10925459116697311,\n",
       "  0.10923020541667938,\n",
       "  0.10922004282474518,\n",
       "  0.10924166440963745,\n",
       "  0.1091998964548111,\n",
       "  0.10918550938367844,\n",
       "  0.10916632413864136,\n",
       "  0.10915765911340714,\n",
       "  0.10915599763393402,\n",
       "  0.10913801193237305,\n",
       "  0.10911715775728226,\n",
       "  0.10910942405462265,\n",
       "  0.10909423977136612,\n",
       "  0.10907679796218872,\n",
       "  0.10908903926610947,\n",
       "  0.10905849933624268,\n",
       "  0.10903345793485641,\n",
       "  0.10901159793138504,\n",
       "  0.10902130603790283,\n",
       "  0.10898690670728683,\n",
       "  0.10896569490432739,\n",
       "  0.10895548015832901,\n",
       "  0.10895058512687683,\n",
       "  0.10892165452241898,\n",
       "  0.10892055928707123,\n",
       "  0.10888902842998505,\n",
       "  0.1088772788643837,\n",
       "  0.10889329016208649,\n",
       "  0.10884492099285126,\n",
       "  0.10882756114006042,\n",
       "  0.10881493240594864,\n",
       "  0.1087987944483757,\n",
       "  0.10879594087600708,\n",
       "  0.10876939445734024,\n",
       "  0.10876138508319855,\n",
       "  0.1087413802742958,\n",
       "  0.10873865336179733,\n",
       "  0.10871735215187073,\n",
       "  0.10869809985160828,\n",
       "  0.10870350152254105,\n",
       "  0.10868451744318008,\n",
       "  0.10866449773311615,\n",
       "  0.10865352302789688,\n",
       "  0.10864356905221939,\n",
       "  0.1086239218711853,\n",
       "  0.10860875248908997,\n",
       "  0.10860349982976913,\n",
       "  0.10858103632926941,\n",
       "  0.10857453942298889,\n",
       "  0.10855656862258911,\n",
       "  0.10854757577180862,\n",
       "  0.1085239127278328,\n",
       "  0.10852143168449402,\n",
       "  0.1085175946354866,\n",
       "  0.10848595947027206,\n",
       "  0.10847461223602295,\n",
       "  0.10847220569849014,\n",
       "  0.10846412181854248,\n",
       "  0.10845628380775452,\n",
       "  0.10841942578554153,\n",
       "  0.10841198265552521,\n",
       "  0.10840112715959549,\n",
       "  0.1083805114030838,\n",
       "  0.10837086290121078,\n",
       "  0.10837004333734512,\n",
       "  0.10834356397390366,\n",
       "  0.10833021253347397,\n",
       "  0.10832536965608597,\n",
       "  0.1083308607339859,\n",
       "  0.1082979366183281,\n",
       "  0.10829347372055054,\n",
       "  0.10828200727701187,\n",
       "  0.10825737565755844,\n",
       "  0.10825502127408981,\n",
       "  0.10823261737823486,\n",
       "  0.10822971165180206,\n",
       "  0.10821030288934708,\n",
       "  0.10821663588285446,\n",
       "  0.10819613933563232,\n",
       "  0.10817422717809677,\n",
       "  0.10815814137458801,\n",
       "  0.1081601157784462,\n",
       "  0.10813486576080322,\n",
       "  0.108127161860466,\n",
       "  0.1081402525305748,\n",
       "  0.10809922963380814,\n",
       "  0.10808771848678589,\n",
       "  0.1080816462635994,\n",
       "  0.108062744140625,\n",
       "  0.1080525815486908,\n",
       "  0.10804296284914017,\n",
       "  0.10802805423736572,\n",
       "  0.10803072899580002,\n",
       "  0.10800138115882874,\n",
       "  0.10799257457256317,\n",
       "  0.10798989981412888,\n",
       "  0.10797161608934402,\n",
       "  0.1079639121890068,\n",
       "  0.1079481989145279,\n",
       "  0.10792983323335648,\n",
       "  0.10791874676942825,\n",
       "  0.10791128873825073,\n",
       "  0.10789595544338226,\n",
       "  0.10788485407829285,\n",
       "  0.10786929726600647,\n",
       "  0.10786216706037521,\n",
       "  0.10785384476184845,\n",
       "  0.10784490406513214,\n",
       "  0.10782576352357864,\n",
       "  0.10781413316726685,\n",
       "  0.10780882090330124,\n",
       "  0.10780061036348343,\n",
       "  0.1077801063656807,\n",
       "  0.10777134448289871,\n",
       "  0.1077958345413208,\n",
       "  0.10774993151426315,\n",
       "  0.10775025933980942,\n",
       "  0.10772732645273209,\n",
       "  0.10771597176790237,\n",
       "  0.1077023446559906,\n",
       "  0.10770358890295029,\n",
       "  0.10769283026456833,\n",
       "  0.10766930133104324,\n",
       "  0.10765829682350159,\n",
       "  0.10764862596988678,\n",
       "  0.10763872414827347,\n",
       "  0.10762515664100647,\n",
       "  0.10761748999357224,\n",
       "  0.10761246085166931,\n",
       "  0.10759884119033813,\n",
       "  0.1075906902551651,\n",
       "  0.10757216066122055,\n",
       "  0.10756611824035645,\n",
       "  0.10756412148475647,\n",
       "  0.1075437143445015,\n",
       "  0.10752914100885391,\n",
       "  0.10754836350679398,\n",
       "  0.10751872509717941,\n",
       "  0.10750102996826172,\n",
       "  0.10748914629220963,\n",
       "  0.1074787825345993,\n",
       "  0.10750369727611542,\n",
       "  0.10746084898710251,\n",
       "  0.10745078325271606,\n",
       "  0.10744944959878922,\n",
       "  0.10742770880460739,\n",
       "  0.10741974413394928,\n",
       "  0.10740917921066284,\n",
       "  0.10740941017866135,\n",
       "  0.10740606486797333,\n",
       "  0.10737895220518112,\n",
       "  0.1073671504855156,\n",
       "  0.10737436264753342,\n",
       "  0.10734738409519196,\n",
       "  0.10734251141548157,\n",
       "  0.10733101516962051,\n",
       "  0.10731737315654755,\n",
       "  0.10732138156890869,\n",
       "  0.10729895532131195,\n",
       "  0.10728898644447327,\n",
       "  0.10728221386671066,\n",
       "  0.10727909952402115,\n",
       "  0.10726617276668549,\n",
       "  0.10725794732570648,\n",
       "  0.10724829137325287,\n",
       "  ...]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "hist = {}\n",
    "try: \n",
    "    prev_hist = pickle.load(open(f'28_Autoencoder_{batch_size}_{learning_rate}history', \"rb\"))\n",
    "except:\n",
    "    hist = new_hist.history\n",
    "else:\n",
    "    for key in prev_hist.keys():\n",
    "        hist[key] = prev_hist[key] + new_hist.history[key]\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'28_Autoencoder_{batch_size}_{learning_rate}history', 'wb') as file_pi:\n",
    "        pickle.dump(hist, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in hist.keys():\n",
    "    hist[key] = hist[key][-3624:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "loss_ax.plot(hist['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist['val_loss'], 'r', label='val loss')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([hist['val_loss'][i] - hist['loss'][i] for i in range(len(hist['loss']))], 'g', label='loss - val loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# print(y_pred[0])\n",
    "# print(X_test.shape)\n",
    "# print(((np.round(y_pred[0]) == X_test[0]) + (np.round(y_pred[1]) == X_test[1]))/ 2)\n",
    "l = len(X_test)\n",
    "acc = sum([np.round(y_pred[i])==X_test[i] for i in range(l)])/l\n",
    "# acc = sum([y_pred[i]==X_test[i] for i in range(l)])/l\n",
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(f'28_Autoencoder_{batch_size}_{learning_rate}_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
