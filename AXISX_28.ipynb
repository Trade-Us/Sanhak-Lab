{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1221</th>\n",
       "      <th>1222</th>\n",
       "      <th>1223</th>\n",
       "      <th>1224</th>\n",
       "      <th>1225</th>\n",
       "      <th>1226</th>\n",
       "      <th>1227</th>\n",
       "      <th>1228</th>\n",
       "      <th>1229</th>\n",
       "      <th>1230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.873603</td>\n",
       "      <td>2.015115</td>\n",
       "      <td>2.033436</td>\n",
       "      <td>1.987634</td>\n",
       "      <td>1.932988</td>\n",
       "      <td>1.971524</td>\n",
       "      <td>2.002796</td>\n",
       "      <td>1.975631</td>\n",
       "      <td>1.870761</td>\n",
       "      <td>1.691660</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.417595</td>\n",
       "      <td>3.417911</td>\n",
       "      <td>3.412541</td>\n",
       "      <td>3.379690</td>\n",
       "      <td>3.314304</td>\n",
       "      <td>3.225544</td>\n",
       "      <td>3.105196</td>\n",
       "      <td>2.983900</td>\n",
       "      <td>2.844916</td>\n",
       "      <td>2.692980</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.328519</td>\n",
       "      <td>3.335468</td>\n",
       "      <td>3.329150</td>\n",
       "      <td>3.301038</td>\n",
       "      <td>3.280822</td>\n",
       "      <td>3.235020</td>\n",
       "      <td>3.174372</td>\n",
       "      <td>3.097615</td>\n",
       "      <td>3.018646</td>\n",
       "      <td>2.927043</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.184310</td>\n",
       "      <td>0.159040</td>\n",
       "      <td>0.189680</td>\n",
       "      <td>0.249064</td>\n",
       "      <td>0.339088</td>\n",
       "      <td>0.448065</td>\n",
       "      <td>0.658121</td>\n",
       "      <td>0.966098</td>\n",
       "      <td>1.277234</td>\n",
       "      <td>1.586790</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.182731</td>\n",
       "      <td>0.134718</td>\n",
       "      <td>0.157145</td>\n",
       "      <td>0.259804</td>\n",
       "      <td>0.454066</td>\n",
       "      <td>0.663175</td>\n",
       "      <td>0.883339</td>\n",
       "      <td>1.119612</td>\n",
       "      <td>1.411480</td>\n",
       "      <td>1.787370</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>3.328519</td>\n",
       "      <td>3.331362</td>\n",
       "      <td>3.322201</td>\n",
       "      <td>3.297879</td>\n",
       "      <td>3.246075</td>\n",
       "      <td>3.152261</td>\n",
       "      <td>3.023068</td>\n",
       "      <td>2.872713</td>\n",
       "      <td>2.711301</td>\n",
       "      <td>2.533464</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>0.203894</td>\n",
       "      <td>0.151775</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.256329</td>\n",
       "      <td>0.390891</td>\n",
       "      <td>0.613267</td>\n",
       "      <td>0.849856</td>\n",
       "      <td>1.153411</td>\n",
       "      <td>1.513191</td>\n",
       "      <td>1.939305</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>0.204842</td>\n",
       "      <td>0.149564</td>\n",
       "      <td>0.153986</td>\n",
       "      <td>0.207369</td>\n",
       "      <td>0.311923</td>\n",
       "      <td>0.477441</td>\n",
       "      <td>0.644222</td>\n",
       "      <td>0.861228</td>\n",
       "      <td>1.049173</td>\n",
       "      <td>1.251016</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>0.312871</td>\n",
       "      <td>0.254118</td>\n",
       "      <td>0.295813</td>\n",
       "      <td>0.400999</td>\n",
       "      <td>0.551987</td>\n",
       "      <td>0.652119</td>\n",
       "      <td>0.694762</td>\n",
       "      <td>0.754778</td>\n",
       "      <td>0.927561</td>\n",
       "      <td>1.158781</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>0.302447</td>\n",
       "      <td>0.242115</td>\n",
       "      <td>0.278756</td>\n",
       "      <td>0.391523</td>\n",
       "      <td>0.512819</td>\n",
       "      <td>0.572519</td>\n",
       "      <td>0.583890</td>\n",
       "      <td>0.622427</td>\n",
       "      <td>0.750988</td>\n",
       "      <td>0.932299</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1330 rows × 1231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     1.873603  2.015115  2.033436  1.987634  1.932988  1.971524  2.002796   \n",
       "1     3.417595  3.417911  3.412541  3.379690  3.314304  3.225544  3.105196   \n",
       "2     3.328519  3.335468  3.329150  3.301038  3.280822  3.235020  3.174372   \n",
       "3     0.184310  0.159040  0.189680  0.249064  0.339088  0.448065  0.658121   \n",
       "4     0.182731  0.134718  0.157145  0.259804  0.454066  0.663175  0.883339   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1325  3.328519  3.331362  3.322201  3.297879  3.246075  3.152261  3.023068   \n",
       "1326  0.203894  0.151775  0.163462  0.256329  0.390891  0.613267  0.849856   \n",
       "1327  0.204842  0.149564  0.153986  0.207369  0.311923  0.477441  0.644222   \n",
       "1328  0.312871  0.254118  0.295813  0.400999  0.551987  0.652119  0.694762   \n",
       "1329  0.302447  0.242115  0.278756  0.391523  0.512819  0.572519  0.583890   \n",
       "\n",
       "          7         8         9     ...  1221  1222  1223  1224  1225  1226  \\\n",
       "0     1.975631  1.870761  1.691660  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1     2.983900  2.844916  2.692980  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2     3.097615  3.018646  2.927043  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3     0.966098  1.277234  1.586790  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4     1.119612  1.411480  1.787370  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "...        ...       ...       ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "1325  2.872713  2.711301  2.533464  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1326  1.153411  1.513191  1.939305  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1327  0.861228  1.049173  1.251016  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1328  0.754778  0.927561  1.158781  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1329  0.622427  0.750988  0.932299  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "      1227  1228  1229  1230  \n",
       "0      NaN   NaN   NaN   NaN  \n",
       "1      NaN   NaN   NaN   NaN  \n",
       "2      NaN   NaN   NaN   NaN  \n",
       "3      NaN   NaN   NaN   NaN  \n",
       "4      NaN   NaN   NaN   NaN  \n",
       "...    ...   ...   ...   ...  \n",
       "1325   NaN   NaN   NaN   NaN  \n",
       "1326   NaN   NaN   NaN   NaN  \n",
       "1327   NaN   NaN   NaN   NaN  \n",
       "1328   NaN   NaN   NaN   NaN  \n",
       "1329   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[1330 rows x 1231 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from readFile import split_into_values, toRPdata\n",
    "# columns 와 value는 사용자 입력\n",
    "df = pd.read_csv('resources/AXISX_resample.csv')\n",
    "columns = ['chip', 'wire', 'segment']\n",
    "value = ['value']\n",
    "#df = pd.read_csv('resources/Dataset1.csv')\n",
    "#columns = ['Process', 'Step']\n",
    "#value = ['Value']\n",
    "\n",
    "df = df.loc[:, columns + value] #('chip', 'wire', 'value')는 사용자 입력\n",
    "size = 28\n",
    "result = split_into_values(df, columns)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "\n",
    "# 2. 시계열 셋 크기 변경\n",
    "result_ = TimeSeriesResampler(sz=size).fit_transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1330, 1, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = result_.reshape(result_.shape[0], 1, size)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1330, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X = toRPdata(data)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1330, 28, 28)\n",
      "[[[0.         0.07514914 0.19119404 ... 0.04976746 0.41776875 0.5447409 ]\n",
      "  [0.06989648 0.         0.24772673 ... 0.11618537 0.369074   0.50666542]\n",
      "  [0.23639049 0.32930417 0.         ... 0.17485847 0.58245481 0.67351246]\n",
      "  ...\n",
      "  [0.05237398 0.13145899 0.14883365 ... 0.         0.45425611 0.57327115]\n",
      "  [0.59966554 0.56958075 0.6762071  ... 0.61958917 0.         0.21807855]\n",
      "  [1.         1.         1.         ... 1.         0.27890085 0.        ]]\n",
      "\n",
      " [[0.         0.70618007 1.         ... 1.         1.         0.03924244]\n",
      "  [0.41389539 0.         0.56413348 ... 0.58281525 0.30759798 0.39089522]\n",
      "  [0.94959208 0.91399501 0.         ... 0.04286122 0.58856437 0.94761395]\n",
      "  ...\n",
      "  [0.99211535 0.98654737 0.04478057 ... 0.         0.65970119 0.99180594]\n",
      "  [0.59776745 0.31371884 0.3705008  ... 0.3974819  0.         0.58198287]\n",
      "  [0.03776062 0.64175365 0.9602349  ... 0.96193928 0.93683058 0.        ]]\n",
      "\n",
      " [[0.         0.99169452 0.99079821 ... 0.21232637 0.99046306 0.99194303]\n",
      "  [0.92654611 0.         0.10791845 ... 0.91094992 0.1482719  0.02992114]\n",
      "  [0.83553865 0.09740649 0.         ... 0.80061916 0.03642276 0.12441312]\n",
      "  ...\n",
      "  [0.17513961 0.80424029 0.7831142  ... 0.         0.77521462 0.81009764]\n",
      "  [0.8059028  0.12912613 0.03514277 ... 0.76469085 0.         0.15518367]\n",
      "  [0.95536388 0.03084403 0.14209112 ... 0.94588645 0.18368924 0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         1.         1.         ... 1.         0.82623664 0.06828379]\n",
      "  [0.57297677 0.         0.40220844 ... 0.24779825 0.22015452 0.543818  ]\n",
      "  [0.95848923 0.67282389 0.         ... 0.25830106 0.9241915  0.95565471]\n",
      "  ...\n",
      "  [0.76173283 0.32943057 0.20527763 ... 0.         0.56486777 0.74546305]\n",
      "  [0.45242584 0.2103941  0.52798026 ... 0.40605706 0.         0.4150354 ]\n",
      "  [0.06391915 0.88844373 0.93331261 ... 0.91608718 0.70950515 0.        ]]\n",
      "\n",
      " [[0.         0.09220933 0.30541204 ... 0.25421039 0.04645826 0.02284412]\n",
      "  [0.0844246  0.         0.19520315 ... 0.14832418 0.04188856 0.06350908]\n",
      "  [0.23395834 0.16332215 0.         ... 0.0392226  0.19836938 0.21645879]\n",
      "  ...\n",
      "  [0.2026856  0.12916578 0.04082381 ... 0.         0.16564376 0.18447166]\n",
      "  [0.04439571 0.04371992 0.24745734 ... 0.19852882 0.         0.02256577]\n",
      "  [0.02233392 0.06781601 0.27625707 ... 0.22619895 0.02308674 0.        ]]\n",
      "\n",
      " [[0.         0.13058453 0.91076151 ... 0.45422501 0.33674851 0.10738021]\n",
      "  [0.11550179 0.         0.69006514 ... 0.28625942 0.18235167 0.02052419]\n",
      "  [0.47664845 0.40830683 0.         ... 0.23892909 0.30041059 0.42045085]\n",
      "  ...\n",
      "  [0.31234851 0.22255186 0.31393801 ... 0.         0.08078289 0.23850835]\n",
      "  [0.25191613 0.15422794 0.42940987 ... 0.08788228 0.         0.17158673]\n",
      "  [0.09696779 0.02095426 0.7254792  ... 0.31321203 0.20712697 0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def MinMax(data):\n",
    "    MMS = MinMaxScaler().fit(data)\n",
    "    scaled = MMS.transform(data)\n",
    "    return scaled\n",
    "\n",
    "# print(X[0].shape)\n",
    "# scaled_data = MinMax(X[0])\n",
    "# print(scaled_data.shape)\n",
    "# scaled_data = []\n",
    "\n",
    "X_scaled = np.empty((X.shape[0], size, size))\n",
    "for i, data in enumerate(X):\n",
    "    X_scaled[i] = MinMax(data)\n",
    "print(X_scaled.shape)\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1330, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = np.expand_dims(X_scaled, axis=3)\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "optimizer='Adam'\n",
    "loss='binary_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 2)         290       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14, 14, 2)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 2)           38        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 7, 7, 2)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 14, 14, 2)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 16)        304       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 1)         145       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 937\n",
      "Trainable params: 937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from utils import split_data, normalization_tool\n",
    "from agent import Autoencoder_Agent\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_data(X_scaled, X_scaled) #데이터 분리\n",
    "\n",
    "agent_28 = Autoencoder_Agent(28,optimizer,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 0.0689 - val_loss: 0.0691\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 0.0689 - val_loss: 0.0703\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 0.0689 - val_loss: 0.0690\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 0.0687 - val_loss: 0.0690\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 0.0687 - val_loss: 0.0692\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0687 - val_loss: 0.06910s -\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 0.0689 - val_loss: 0.0691\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0687 - val_loss: 0.0687\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0693 - val_loss: 0.0698\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0691 - val_loss: 0.0690\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0688 - val_loss: 0.0689\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0685 - val_loss: 0.0693\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0686 - val_loss: 0.0743\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0702 - val_loss: 0.0703\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0688 - val_loss: 0.0693\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0684 - val_loss: 0.0694\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0685 - val_loss: 0.0688\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0684 - val_loss: 0.0697\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0683 - val_loss: 0.0714\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0690 - val_loss: 0.0703\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0689 - val_loss: 0.0699\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0684 - val_loss: 0.0683\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0681 - val_loss: 0.0690\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0684 - val_loss: 0.0686\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0683 - val_loss: 0.0683\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0689 - val_loss: 0.0692\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0685 - val_loss: 0.0688\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0685 - val_loss: 0.0687\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.0684 - val_loss: 0.0682\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0679 - val_loss: 0.0682\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0679 - val_loss: 0.0684\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0681 - val_loss: 0.0680\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0678 - val_loss: 0.0679\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0677 - val_loss: 0.0679\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.0683 - val_loss: 0.0681\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0679 - val_loss: 0.0678\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0681 - val_loss: 0.0679\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.0681 - val_loss: 0.0678\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0676 - val_loss: 0.0676\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0675 - val_loss: 0.0676\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0677 - val_loss: 0.0689\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0676 - val_loss: 0.0682\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0681 - val_loss: 0.0687\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0678 - val_loss: 0.0674\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.0673 - val_loss: 0.0677\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0675 - val_loss: 0.0674\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0676 - val_loss: 0.0674\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0674 - val_loss: 0.0688\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0676 - val_loss: 0.0692\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0678 - val_loss: 0.0685\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0677 - val_loss: 0.0696\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0680 - val_loss: 0.0672\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0670 - val_loss: 0.0669\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0669 - val_loss: 0.0674\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 0.0669 - val_loss: 0.0669\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.0672 - val_loss: 0.0686\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0670 - val_loss: 0.0669\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 0.0670 - val_loss: 0.0679\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.0670 - val_loss: 0.0669\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 0.0669 - val_loss: 0.0669\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 0.0669 - val_loss: 0.0668\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.0670 - val_loss: 0.0728\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 2s 64ms/step - loss: 0.0680 - val_loss: 0.0667\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 2s 62ms/step - loss: 0.0664 - val_loss: 0.0668 ETA: 0s - loss: 0.06\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0665 - val_loss: 0.0667\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0666 - val_loss: 0.0662\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0663 - val_loss: 0.0668\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0667 - val_loss: 0.0663\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 2s 61ms/step - loss: 0.0665 - val_loss: 0.0662\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0663 - val_loss: 0.0662\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.0661 - val_loss: 0.0661\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0661 - val_loss: 0.0661\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0660 - val_loss: 0.0671\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0665 - val_loss: 0.0663\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0662 - val_loss: 0.0658\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 2s 60ms/step - loss: 0.0661 - val_loss: 0.0670\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0662 - val_loss: 0.0656\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0659 - val_loss: 0.0656\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0658 - val_loss: 0.0656\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 2s 56ms/step - loss: 0.0659 - val_loss: 0.0661\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0659 - val_loss: 0.0659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0656 - val_loss: 0.0652\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0656 - val_loss: 0.0658\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0672 - val_loss: 0.0661\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0657 - val_loss: 0.0652\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0655 - val_loss: 0.0659\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0658 - val_loss: 0.0654\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0652 - val_loss: 0.0651\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.0652 - val_loss: 0.0654\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 2s 57ms/step - loss: 0.0654 - val_loss: 0.0649\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.0652 - val_loss: 0.0646\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 0.0653 - val_loss: 0.0664\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 0.0654 - val_loss: 0.0658\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 0.0656 - val_loss: 0.0659\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 0.0654 - val_loss: 0.0652\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 0.0655 - val_loss: 0.0654\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 0.0653 - val_loss: 0.0643\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 0.0647 - val_loss: 0.0644\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 0.0648 - val_loss: 0.0648\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 0.0646 - val_loss: 0.0646\n"
     ]
    }
   ],
   "source": [
    "agent_28.train(X_train,batch_size,epochs,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5053133 , 0.48805618, 0.66479856, ..., 0.56436193, 0.9252325 ,\n",
       "        0.8303621 ],\n",
       "       [0.8209416 , 0.6567595 , 1.267438  , ..., 0.44634992, 1.0505612 ,\n",
       "        0.54547375],\n",
       "       [0.77935916, 0.6363776 , 1.1770104 , ..., 0.5061642 , 1.0290796 ,\n",
       "        0.5964327 ],\n",
       "       ...,\n",
       "       [0.64755416, 0.72422695, 1.4444054 , ..., 0.9654461 , 0.7657862 ,\n",
       "        1.0887597 ],\n",
       "       [0.928068  , 0.8842468 , 0.98599744, ..., 0.8287606 , 0.63107663,\n",
       "        0.64822793],\n",
       "       [0.42672443, 0.42755905, 0.6016034 , ..., 0.509621  , 0.92615926,\n",
       "        0.69785625]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = agent_28.feature_extract(X_train)\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
