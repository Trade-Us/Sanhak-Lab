{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1221</th>\n",
       "      <th>1222</th>\n",
       "      <th>1223</th>\n",
       "      <th>1224</th>\n",
       "      <th>1225</th>\n",
       "      <th>1226</th>\n",
       "      <th>1227</th>\n",
       "      <th>1228</th>\n",
       "      <th>1229</th>\n",
       "      <th>1230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.873603</td>\n",
       "      <td>2.015115</td>\n",
       "      <td>2.033436</td>\n",
       "      <td>1.987634</td>\n",
       "      <td>1.932988</td>\n",
       "      <td>1.971524</td>\n",
       "      <td>2.002796</td>\n",
       "      <td>1.975631</td>\n",
       "      <td>1.870761</td>\n",
       "      <td>1.691660</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.417595</td>\n",
       "      <td>3.417911</td>\n",
       "      <td>3.412541</td>\n",
       "      <td>3.379690</td>\n",
       "      <td>3.314304</td>\n",
       "      <td>3.225544</td>\n",
       "      <td>3.105196</td>\n",
       "      <td>2.983900</td>\n",
       "      <td>2.844916</td>\n",
       "      <td>2.692980</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.328519</td>\n",
       "      <td>3.335468</td>\n",
       "      <td>3.329150</td>\n",
       "      <td>3.301038</td>\n",
       "      <td>3.280822</td>\n",
       "      <td>3.235020</td>\n",
       "      <td>3.174372</td>\n",
       "      <td>3.097615</td>\n",
       "      <td>3.018646</td>\n",
       "      <td>2.927043</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.184310</td>\n",
       "      <td>0.159040</td>\n",
       "      <td>0.189680</td>\n",
       "      <td>0.249064</td>\n",
       "      <td>0.339088</td>\n",
       "      <td>0.448065</td>\n",
       "      <td>0.658121</td>\n",
       "      <td>0.966098</td>\n",
       "      <td>1.277234</td>\n",
       "      <td>1.586790</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.182731</td>\n",
       "      <td>0.134718</td>\n",
       "      <td>0.157145</td>\n",
       "      <td>0.259804</td>\n",
       "      <td>0.454066</td>\n",
       "      <td>0.663175</td>\n",
       "      <td>0.883339</td>\n",
       "      <td>1.119612</td>\n",
       "      <td>1.411480</td>\n",
       "      <td>1.787370</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>3.328519</td>\n",
       "      <td>3.331362</td>\n",
       "      <td>3.322201</td>\n",
       "      <td>3.297879</td>\n",
       "      <td>3.246075</td>\n",
       "      <td>3.152261</td>\n",
       "      <td>3.023068</td>\n",
       "      <td>2.872713</td>\n",
       "      <td>2.711301</td>\n",
       "      <td>2.533464</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>0.203894</td>\n",
       "      <td>0.151775</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>0.256329</td>\n",
       "      <td>0.390891</td>\n",
       "      <td>0.613267</td>\n",
       "      <td>0.849856</td>\n",
       "      <td>1.153411</td>\n",
       "      <td>1.513191</td>\n",
       "      <td>1.939305</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>0.204842</td>\n",
       "      <td>0.149564</td>\n",
       "      <td>0.153986</td>\n",
       "      <td>0.207369</td>\n",
       "      <td>0.311923</td>\n",
       "      <td>0.477441</td>\n",
       "      <td>0.644222</td>\n",
       "      <td>0.861228</td>\n",
       "      <td>1.049173</td>\n",
       "      <td>1.251016</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>0.312871</td>\n",
       "      <td>0.254118</td>\n",
       "      <td>0.295813</td>\n",
       "      <td>0.400999</td>\n",
       "      <td>0.551987</td>\n",
       "      <td>0.652119</td>\n",
       "      <td>0.694762</td>\n",
       "      <td>0.754778</td>\n",
       "      <td>0.927561</td>\n",
       "      <td>1.158781</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>0.302447</td>\n",
       "      <td>0.242115</td>\n",
       "      <td>0.278756</td>\n",
       "      <td>0.391523</td>\n",
       "      <td>0.512819</td>\n",
       "      <td>0.572519</td>\n",
       "      <td>0.583890</td>\n",
       "      <td>0.622427</td>\n",
       "      <td>0.750988</td>\n",
       "      <td>0.932299</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1330 rows × 1231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     1.873603  2.015115  2.033436  1.987634  1.932988  1.971524  2.002796   \n",
       "1     3.417595  3.417911  3.412541  3.379690  3.314304  3.225544  3.105196   \n",
       "2     3.328519  3.335468  3.329150  3.301038  3.280822  3.235020  3.174372   \n",
       "3     0.184310  0.159040  0.189680  0.249064  0.339088  0.448065  0.658121   \n",
       "4     0.182731  0.134718  0.157145  0.259804  0.454066  0.663175  0.883339   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1325  3.328519  3.331362  3.322201  3.297879  3.246075  3.152261  3.023068   \n",
       "1326  0.203894  0.151775  0.163462  0.256329  0.390891  0.613267  0.849856   \n",
       "1327  0.204842  0.149564  0.153986  0.207369  0.311923  0.477441  0.644222   \n",
       "1328  0.312871  0.254118  0.295813  0.400999  0.551987  0.652119  0.694762   \n",
       "1329  0.302447  0.242115  0.278756  0.391523  0.512819  0.572519  0.583890   \n",
       "\n",
       "          7         8         9     ...  1221  1222  1223  1224  1225  1226  \\\n",
       "0     1.975631  1.870761  1.691660  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1     2.983900  2.844916  2.692980  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "2     3.097615  3.018646  2.927043  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "3     0.966098  1.277234  1.586790  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "4     1.119612  1.411480  1.787370  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "...        ...       ...       ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "1325  2.872713  2.711301  2.533464  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1326  1.153411  1.513191  1.939305  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1327  0.861228  1.049173  1.251016  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1328  0.754778  0.927561  1.158781  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "1329  0.622427  0.750988  0.932299  ...   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "      1227  1228  1229  1230  \n",
       "0      NaN   NaN   NaN   NaN  \n",
       "1      NaN   NaN   NaN   NaN  \n",
       "2      NaN   NaN   NaN   NaN  \n",
       "3      NaN   NaN   NaN   NaN  \n",
       "4      NaN   NaN   NaN   NaN  \n",
       "...    ...   ...   ...   ...  \n",
       "1325   NaN   NaN   NaN   NaN  \n",
       "1326   NaN   NaN   NaN   NaN  \n",
       "1327   NaN   NaN   NaN   NaN  \n",
       "1328   NaN   NaN   NaN   NaN  \n",
       "1329   NaN   NaN   NaN   NaN  \n",
       "\n",
       "[1330 rows x 1231 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from readFile import split_into_values, toRPdata\n",
    "# columns 와 value는 사용자 입력\n",
    "df = pd.read_csv('resources/AXISX_resample.csv')\n",
    "columns = ['chip', 'wire', 'segment']\n",
    "value = ['value']\n",
    "#df = pd.read_csv('resources/Dataset1.csv')\n",
    "#columns = ['Process', 'Step']\n",
    "#value = ['Value']\n",
    "\n",
    "df = df.loc[:, columns + value] #('chip', 'wire', 'value')는 사용자 입력\n",
    "size = 28\n",
    "result = split_into_values(df, columns)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
    "\n",
    "# 2. 시계열 셋 크기 변경\n",
    "result_ = TimeSeriesResampler(sz=size).fit_transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1330, 1, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = result_.reshape(result_.shape[0], 1, size)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1330, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X = toRPdata(data)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1330, 28, 28)\n",
      "[[[0.         0.07514914 0.19119404 ... 0.04976746 0.41776875 0.5447409 ]\n",
      "  [0.06989648 0.         0.24772673 ... 0.11618537 0.369074   0.50666542]\n",
      "  [0.23639049 0.32930417 0.         ... 0.17485847 0.58245481 0.67351246]\n",
      "  ...\n",
      "  [0.05237398 0.13145899 0.14883365 ... 0.         0.45425611 0.57327115]\n",
      "  [0.59966554 0.56958075 0.6762071  ... 0.61958917 0.         0.21807855]\n",
      "  [1.         1.         1.         ... 1.         0.27890085 0.        ]]\n",
      "\n",
      " [[0.         0.70618007 1.         ... 1.         1.         0.03924244]\n",
      "  [0.41389539 0.         0.56413348 ... 0.58281525 0.30759798 0.39089522]\n",
      "  [0.94959208 0.91399501 0.         ... 0.04286122 0.58856437 0.94761395]\n",
      "  ...\n",
      "  [0.99211535 0.98654737 0.04478057 ... 0.         0.65970119 0.99180594]\n",
      "  [0.59776745 0.31371884 0.3705008  ... 0.3974819  0.         0.58198287]\n",
      "  [0.03776062 0.64175365 0.9602349  ... 0.96193928 0.93683058 0.        ]]\n",
      "\n",
      " [[0.         0.99169452 0.99079821 ... 0.21232637 0.99046306 0.99194303]\n",
      "  [0.92654611 0.         0.10791845 ... 0.91094992 0.1482719  0.02992114]\n",
      "  [0.83553865 0.09740649 0.         ... 0.80061916 0.03642276 0.12441312]\n",
      "  ...\n",
      "  [0.17513961 0.80424029 0.7831142  ... 0.         0.77521462 0.81009764]\n",
      "  [0.8059028  0.12912613 0.03514277 ... 0.76469085 0.         0.15518367]\n",
      "  [0.95536388 0.03084403 0.14209112 ... 0.94588645 0.18368924 0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         1.         1.         ... 1.         0.82623664 0.06828379]\n",
      "  [0.57297677 0.         0.40220844 ... 0.24779825 0.22015452 0.543818  ]\n",
      "  [0.95848923 0.67282389 0.         ... 0.25830106 0.9241915  0.95565471]\n",
      "  ...\n",
      "  [0.76173283 0.32943057 0.20527763 ... 0.         0.56486777 0.74546305]\n",
      "  [0.45242584 0.2103941  0.52798026 ... 0.40605706 0.         0.4150354 ]\n",
      "  [0.06391915 0.88844373 0.93331261 ... 0.91608718 0.70950515 0.        ]]\n",
      "\n",
      " [[0.         0.09220933 0.30541204 ... 0.25421039 0.04645826 0.02284412]\n",
      "  [0.0844246  0.         0.19520315 ... 0.14832418 0.04188856 0.06350908]\n",
      "  [0.23395834 0.16332215 0.         ... 0.0392226  0.19836938 0.21645879]\n",
      "  ...\n",
      "  [0.2026856  0.12916578 0.04082381 ... 0.         0.16564376 0.18447166]\n",
      "  [0.04439571 0.04371992 0.24745734 ... 0.19852882 0.         0.02256577]\n",
      "  [0.02233392 0.06781601 0.27625707 ... 0.22619895 0.02308674 0.        ]]\n",
      "\n",
      " [[0.         0.13058453 0.91076151 ... 0.45422501 0.33674851 0.10738021]\n",
      "  [0.11550179 0.         0.69006514 ... 0.28625942 0.18235167 0.02052419]\n",
      "  [0.47664845 0.40830683 0.         ... 0.23892909 0.30041059 0.42045085]\n",
      "  ...\n",
      "  [0.31234851 0.22255186 0.31393801 ... 0.         0.08078289 0.23850835]\n",
      "  [0.25191613 0.15422794 0.42940987 ... 0.08788228 0.         0.17158673]\n",
      "  [0.09696779 0.02095426 0.7254792  ... 0.31321203 0.20712697 0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def MinMax(data):\n",
    "    MMS = MinMaxScaler().fit(data)\n",
    "    scaled = MMS.transform(data)\n",
    "    return scaled\n",
    "\n",
    "# print(X[0].shape)\n",
    "# scaled_data = MinMax(X[0])\n",
    "# print(scaled_data.shape)\n",
    "# scaled_data = []\n",
    "\n",
    "X_scaled = np.empty((X.shape[0], size, size))\n",
    "for i, data in enumerate(X):\n",
    "    X_scaled[i] = MinMax(data)\n",
    "print(X_scaled.shape)\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1330, 28, 28, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled = np.expand_dims(X_scaled, axis=3)\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "optimizer='Adam'\n",
    "loss='binary_crossentropy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 2)         290       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 2)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 2)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 2)           38        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 2)           0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 14, 14, 2)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 16)        304       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 1)         145       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 937\n",
      "Trainable params: 937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.1071 - val_loss: 0.0968\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0911 - val_loss: 0.0856\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0854 - val_loss: 0.0818\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0826 - val_loss: 0.0816\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0816 - val_loss: 0.0786\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0795 - val_loss: 0.0771\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0777 - val_loss: 0.0744\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0757 - val_loss: 0.0731\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0745 - val_loss: 0.0716\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0710\n"
     ]
    }
   ],
   "source": [
    "from utils import split_data, normalization_tool\n",
    "from agent import Autoencoder_Agent\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_data(X_scaled, X_scaled) #데이터 분리\n",
    "\n",
    "agent_28 = Autoencoder_Agent(28,optimizer,learning_rate)\n",
    "agent_28.train(X_train,batch_size,epochs,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0700\n",
      "Epoch 2/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0691\n",
      "Epoch 3/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0710 - val_loss: 0.0698\n",
      "Epoch 4/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0708 - val_loss: 0.0685\n",
      "Epoch 5/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0702 - val_loss: 0.0688\n",
      "Epoch 6/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0699 - val_loss: 0.0682\n",
      "Epoch 7/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0699 - val_loss: 0.0680\n",
      "Epoch 8/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0691 - val_loss: 0.0670\n",
      "Epoch 9/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0687 - val_loss: 0.0666\n",
      "Epoch 10/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0684 - val_loss: 0.0677\n",
      "Epoch 11/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0683 - val_loss: 0.0690\n",
      "Epoch 12/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0685 - val_loss: 0.0658\n",
      "Epoch 13/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0676 - val_loss: 0.0671\n",
      "Epoch 14/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0679 - val_loss: 0.0655\n",
      "Epoch 15/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0674 - val_loss: 0.0654\n",
      "Epoch 16/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0670 - val_loss: 0.0650\n",
      "Epoch 17/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0670 - val_loss: 0.0662\n",
      "Epoch 18/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0672 - val_loss: 0.0647\n",
      "Epoch 19/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0667 - val_loss: 0.0648\n",
      "Epoch 20/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0667 - val_loss: 0.0650\n",
      "Epoch 21/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0664 - val_loss: 0.0643\n",
      "Epoch 22/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0663 - val_loss: 0.0642\n",
      "Epoch 23/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0662 - val_loss: 0.0680\n",
      "Epoch 24/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0666 - val_loss: 0.0641\n",
      "Epoch 25/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0661 - val_loss: 0.0678\n",
      "Epoch 26/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0664 - val_loss: 0.0645\n",
      "Epoch 27/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0659 - val_loss: 0.0637\n",
      "Epoch 28/500\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.064 - 0s 4ms/step - loss: 0.0655 - val_loss: 0.0641\n",
      "Epoch 29/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0654 - val_loss: 0.0642\n",
      "Epoch 30/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0657 - val_loss: 0.0635\n",
      "Epoch 31/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0655 - val_loss: 0.0660\n",
      "Epoch 32/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0658 - val_loss: 0.0631\n",
      "Epoch 33/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0650 - val_loss: 0.0637\n",
      "Epoch 34/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0650 - val_loss: 0.0632\n",
      "Epoch 35/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0653 - val_loss: 0.0631\n",
      "Epoch 36/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0647 - val_loss: 0.0639\n",
      "Epoch 37/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0658 - val_loss: 0.0629\n",
      "Epoch 38/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0645 - val_loss: 0.0624\n",
      "Epoch 39/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0643 - val_loss: 0.0626\n",
      "Epoch 40/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0646 - val_loss: 0.0622\n",
      "Epoch 41/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0644 - val_loss: 0.0630\n",
      "Epoch 42/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0649 - val_loss: 0.0623\n",
      "Epoch 43/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0647 - val_loss: 0.0632\n",
      "Epoch 44/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0643 - val_loss: 0.0623\n",
      "Epoch 45/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0645 - val_loss: 0.0624\n",
      "Epoch 46/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0639 - val_loss: 0.0625\n",
      "Epoch 47/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0639 - val_loss: 0.0625\n",
      "Epoch 48/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0647 - val_loss: 0.0626\n",
      "Epoch 49/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0639 - val_loss: 0.0628\n",
      "Epoch 50/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0639 - val_loss: 0.0632\n",
      "Epoch 51/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0644 - val_loss: 0.0622\n",
      "Epoch 52/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0636 - val_loss: 0.0616\n",
      "Epoch 53/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0635 - val_loss: 0.0614\n",
      "Epoch 54/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0637 - val_loss: 0.0612\n",
      "Epoch 55/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0640 - val_loss: 0.0626\n",
      "Epoch 56/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0638 - val_loss: 0.0629\n",
      "Epoch 57/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0636 - val_loss: 0.0620\n",
      "Epoch 58/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0637 - val_loss: 0.0617\n",
      "Epoch 59/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0633 - val_loss: 0.0619\n",
      "Epoch 60/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0637 - val_loss: 0.0612\n",
      "Epoch 61/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0633 - val_loss: 0.0610\n",
      "Epoch 62/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0632 - val_loss: 0.0616\n",
      "Epoch 63/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0633 - val_loss: 0.0626\n",
      "Epoch 64/500\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.062 - 0s 4ms/step - loss: 0.0631 - val_loss: 0.0614\n",
      "Epoch 65/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0630 - val_loss: 0.0618\n",
      "Epoch 66/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0634 - val_loss: 0.0616\n",
      "Epoch 67/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0637 - val_loss: 0.0611\n",
      "Epoch 68/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0634 - val_loss: 0.0610\n",
      "Epoch 69/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0636 - val_loss: 0.0620\n",
      "Epoch 70/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0637 - val_loss: 0.0616\n",
      "Epoch 71/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0632 - val_loss: 0.0611\n",
      "Epoch 72/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0631 - val_loss: 0.0614\n",
      "Epoch 73/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0632 - val_loss: 0.0625\n",
      "Epoch 74/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0638 - val_loss: 0.0609\n",
      "Epoch 75/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0628 - val_loss: 0.0615\n",
      "Epoch 76/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0630 - val_loss: 0.0612\n",
      "Epoch 77/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0627 - val_loss: 0.0605\n",
      "Epoch 78/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0640 - val_loss: 0.0609\n",
      "Epoch 79/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0630 - val_loss: 0.0635\n",
      "Epoch 80/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0637 - val_loss: 0.0622\n",
      "Epoch 81/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0632 - val_loss: 0.0608\n",
      "Epoch 82/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0627 - val_loss: 0.0612\n",
      "Epoch 83/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0626 - val_loss: 0.0608\n",
      "Epoch 84/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0625 - val_loss: 0.0610\n",
      "Epoch 85/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0625 - val_loss: 0.0619\n",
      "Epoch 86/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0627 - val_loss: 0.0629\n",
      "Epoch 87/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0636 - val_loss: 0.0605\n",
      "Epoch 88/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0626 - val_loss: 0.0606\n",
      "Epoch 89/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0627 - val_loss: 0.0607\n",
      "Epoch 90/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0626 - val_loss: 0.0606\n",
      "Epoch 91/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0623 - val_loss: 0.0606\n",
      "Epoch 92/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0624 - val_loss: 0.0608\n",
      "Epoch 93/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0629 - val_loss: 0.0640\n",
      "Epoch 94/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0639 - val_loss: 0.0603\n",
      "Epoch 95/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0625 - val_loss: 0.0611\n",
      "Epoch 96/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0623 - val_loss: 0.0606\n",
      "Epoch 97/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0622 - val_loss: 0.0606\n",
      "Epoch 98/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0622 - val_loss: 0.0602\n",
      "Epoch 99/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0625 - val_loss: 0.0604\n",
      "Epoch 100/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0621 - val_loss: 0.0603\n",
      "Epoch 101/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0621 - val_loss: 0.0608\n",
      "Epoch 102/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0623 - val_loss: 0.0638\n",
      "Epoch 103/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0631 - val_loss: 0.0618\n",
      "Epoch 104/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0628 - val_loss: 0.0603\n",
      "Epoch 105/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0622 - val_loss: 0.0604\n",
      "Epoch 106/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0622 - val_loss: 0.0604\n",
      "Epoch 107/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0618 - val_loss: 0.0604\n",
      "Epoch 108/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0625 - val_loss: 0.0602\n",
      "Epoch 109/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0621 - val_loss: 0.0608\n",
      "Epoch 110/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0621 - val_loss: 0.0598\n",
      "Epoch 111/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0617 - val_loss: 0.0596\n",
      "Epoch 112/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0617 - val_loss: 0.0631\n",
      "Epoch 113/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0653 - val_loss: 0.0611\n",
      "Epoch 114/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0623 - val_loss: 0.0597\n",
      "Epoch 115/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0616 - val_loss: 0.0597\n",
      "Epoch 116/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0616 - val_loss: 0.0617\n",
      "Epoch 117/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0621 - val_loss: 0.0598\n",
      "Epoch 118/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.0602\n",
      "Epoch 119/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0616 - val_loss: 0.0593\n",
      "Epoch 120/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0619 - val_loss: 0.0610\n",
      "Epoch 121/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0617 - val_loss: 0.0601\n",
      "Epoch 122/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.0592\n",
      "Epoch 123/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.0591\n",
      "Epoch 124/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 125/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0620 - val_loss: 0.0602\n",
      "Epoch 126/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0614 - val_loss: 0.0597\n",
      "Epoch 127/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0616 - val_loss: 0.0591\n",
      "Epoch 128/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0614 - val_loss: 0.0614\n",
      "Epoch 129/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0616 - val_loss: 0.0593\n",
      "Epoch 130/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0617 - val_loss: 0.0600\n",
      "Epoch 131/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0618 - val_loss: 0.0608\n",
      "Epoch 132/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0618 - val_loss: 0.0602\n",
      "Epoch 133/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0614 - val_loss: 0.0592\n",
      "Epoch 134/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0614 - val_loss: 0.0590\n",
      "Epoch 135/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0617 - val_loss: 0.0588\n",
      "Epoch 136/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0613 - val_loss: 0.0606\n",
      "Epoch 137/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.0605\n",
      "Epoch 138/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0622 - val_loss: 0.0607\n",
      "Epoch 139/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.0587\n",
      "Epoch 140/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0610 - val_loss: 0.0606\n",
      "Epoch 141/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0618 - val_loss: 0.0608\n",
      "Epoch 142/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0613 - val_loss: 0.0589\n",
      "Epoch 143/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0609 - val_loss: 0.0597\n",
      "Epoch 144/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0612 - val_loss: 0.0599\n",
      "Epoch 145/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0612 - val_loss: 0.0590\n",
      "Epoch 146/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0614 - val_loss: 0.0590\n",
      "Epoch 147/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0613 - val_loss: 0.0587\n",
      "Epoch 148/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0609 - val_loss: 0.0587\n",
      "Epoch 149/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0610 - val_loss: 0.0588\n",
      "Epoch 150/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0613 - val_loss: 0.0593\n",
      "Epoch 151/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0606 - val_loss: 0.0585\n",
      "Epoch 152/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0612 - val_loss: 0.0591\n",
      "Epoch 153/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0607 - val_loss: 0.0586\n",
      "Epoch 154/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.0594\n",
      "Epoch 155/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0612 - val_loss: 0.0592\n",
      "Epoch 156/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0607 - val_loss: 0.0581\n",
      "Epoch 157/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0604 - val_loss: 0.0593\n",
      "Epoch 158/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0622 - val_loss: 0.0603\n",
      "Epoch 159/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0610 - val_loss: 0.0585\n",
      "Epoch 160/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.0596\n",
      "Epoch 161/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0610 - val_loss: 0.0590\n",
      "Epoch 162/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.0620\n",
      "Epoch 163/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0609 - val_loss: 0.0585\n",
      "Epoch 164/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0604 - val_loss: 0.0584\n",
      "Epoch 165/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0602 - val_loss: 0.0591\n",
      "Epoch 166/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0603 - val_loss: 0.0580\n",
      "Epoch 167/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0602 - val_loss: 0.0597\n",
      "Epoch 168/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0603 - val_loss: 0.0589\n",
      "Epoch 169/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0608 - val_loss: 0.0594\n",
      "Epoch 170/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.0585\n",
      "Epoch 171/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0604 - val_loss: 0.0582\n",
      "Epoch 172/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0602 - val_loss: 0.0578\n",
      "Epoch 173/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0598 - val_loss: 0.0582\n",
      "Epoch 174/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.0579\n",
      "Epoch 175/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0600 - val_loss: 0.0582\n",
      "Epoch 176/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.0574\n",
      "Epoch 177/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0600 - val_loss: 0.0580\n",
      "Epoch 178/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0600 - val_loss: 0.0586\n",
      "Epoch 179/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.0580\n",
      "Epoch 180/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.0580\n",
      "Epoch 181/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.0594\n",
      "Epoch 182/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0603 - val_loss: 0.0602\n",
      "Epoch 183/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0603 - val_loss: 0.0577\n",
      "Epoch 184/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0597 - val_loss: 0.0579\n",
      "Epoch 185/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.0574\n",
      "Epoch 186/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0599 - val_loss: 0.0604\n",
      "Epoch 187/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0600 - val_loss: 0.0601\n",
      "Epoch 188/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0611 - val_loss: 0.0598\n",
      "Epoch 189/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0604 - val_loss: 0.0573\n",
      "Epoch 190/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0597 - val_loss: 0.0573\n",
      "Epoch 191/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0599 - val_loss: 0.0579\n",
      "Epoch 192/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0597 - val_loss: 0.0572\n",
      "Epoch 193/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.0571\n",
      "Epoch 194/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.0575\n",
      "Epoch 195/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0594 - val_loss: 0.0573\n",
      "Epoch 196/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.0573\n",
      "Epoch 197/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0599 - val_loss: 0.0598\n",
      "Epoch 198/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0600 - val_loss: 0.0577\n",
      "Epoch 199/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.0591\n",
      "Epoch 200/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0599 - val_loss: 0.0575\n",
      "Epoch 201/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.0573\n",
      "Epoch 202/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0594 - val_loss: 0.0570\n",
      "Epoch 203/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.0574\n",
      "Epoch 204/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0596 - val_loss: 0.0573\n",
      "Epoch 205/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.0580\n",
      "Epoch 206/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0594 - val_loss: 0.0571\n",
      "Epoch 207/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0568\n",
      "Epoch 208/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0573\n",
      "Epoch 209/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0602 - val_loss: 0.0606\n",
      "Epoch 210/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.0569\n",
      "Epoch 211/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0596 - val_loss: 0.0573\n",
      "Epoch 212/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0594 - val_loss: 0.0574\n",
      "Epoch 213/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0583\n",
      "Epoch 214/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0597 - val_loss: 0.0569\n",
      "Epoch 215/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0576\n",
      "Epoch 216/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0569\n",
      "Epoch 217/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0571\n",
      "Epoch 218/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0590 - val_loss: 0.0580\n",
      "Epoch 219/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0568\n",
      "Epoch 220/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0567\n",
      "Epoch 221/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0568\n",
      "Epoch 222/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0590 - val_loss: 0.0575\n",
      "Epoch 223/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.0577\n",
      "Epoch 224/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0577\n",
      "Epoch 225/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0590 - val_loss: 0.0579\n",
      "Epoch 226/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0571\n",
      "Epoch 227/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.0576\n",
      "Epoch 228/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0570\n",
      "Epoch 229/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0573\n",
      "Epoch 230/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0596 - val_loss: 0.0571\n",
      "Epoch 231/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0566\n",
      "Epoch 232/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0590 - val_loss: 0.0571\n",
      "Epoch 233/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.0581\n",
      "Epoch 234/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.0570\n",
      "Epoch 235/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0581\n",
      "Epoch 236/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.0573\n",
      "Epoch 237/500\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.059 - 0s 4ms/step - loss: 0.0593 - val_loss: 0.0567\n",
      "Epoch 238/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0574\n",
      "Epoch 239/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0576\n",
      "Epoch 240/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0565\n",
      "Epoch 241/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0594 - val_loss: 0.0600\n",
      "Epoch 242/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0599 - val_loss: 0.0572\n",
      "Epoch 243/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0577\n",
      "Epoch 244/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0589 - val_loss: 0.0568\n",
      "Epoch 245/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0569\n",
      "Epoch 246/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0589 - val_loss: 0.0566\n",
      "Epoch 247/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0566\n",
      "Epoch 248/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0589\n",
      "Epoch 249/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.0574\n",
      "Epoch 250/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.0578\n",
      "Epoch 251/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0573\n",
      "Epoch 252/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0587 - val_loss: 0.0568\n",
      "Epoch 253/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0587 - val_loss: 0.0565\n",
      "Epoch 254/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0567\n",
      "Epoch 255/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0590 - val_loss: 0.0568\n",
      "Epoch 256/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0596 - val_loss: 0.0601\n",
      "Epoch 257/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0596 - val_loss: 0.0565\n",
      "Epoch 258/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0587 - val_loss: 0.0562\n",
      "Epoch 259/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0575\n",
      "Epoch 260/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0587 - val_loss: 0.0564\n",
      "Epoch 261/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0562\n",
      "Epoch 262/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0588\n",
      "Epoch 263/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0568\n",
      "Epoch 264/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0589 - val_loss: 0.0564\n",
      "Epoch 265/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0573\n",
      "Epoch 266/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.0593\n",
      "Epoch 267/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0596 - val_loss: 0.0569\n",
      "Epoch 268/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0569\n",
      "Epoch 269/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0568\n",
      "Epoch 270/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0565\n",
      "Epoch 271/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0560\n",
      "Epoch 272/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0587 - val_loss: 0.0572\n",
      "Epoch 273/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0587 - val_loss: 0.0561\n",
      "Epoch 274/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0566\n",
      "Epoch 275/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0568\n",
      "Epoch 276/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0564\n",
      "Epoch 277/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0562\n",
      "Epoch 278/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0590 - val_loss: 0.0579\n",
      "Epoch 279/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0587 - val_loss: 0.0573\n",
      "Epoch 280/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0590 - val_loss: 0.0597\n",
      "Epoch 281/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0597 - val_loss: 0.0582\n",
      "Epoch 282/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0597 - val_loss: 0.0571\n",
      "Epoch 283/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0567\n",
      "Epoch 284/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0560\n",
      "Epoch 285/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0577\n",
      "Epoch 286/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0587 - val_loss: 0.0562\n",
      "Epoch 287/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0562\n",
      "Epoch 288/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0563\n",
      "Epoch 289/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0589 - val_loss: 0.0578\n",
      "Epoch 290/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0589 - val_loss: 0.0562\n",
      "Epoch 291/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0566\n",
      "Epoch 292/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.0566\n",
      "Epoch 293/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0588 - val_loss: 0.0569\n",
      "Epoch 294/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0561\n",
      "Epoch 295/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0577\n",
      "Epoch 296/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0567\n",
      "Epoch 297/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0581\n",
      "Epoch 298/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0587 - val_loss: 0.0585\n",
      "Epoch 299/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0585\n",
      "Epoch 300/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0587 - val_loss: 0.0562\n",
      "Epoch 301/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0589 - val_loss: 0.0568\n",
      "Epoch 302/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0588 - val_loss: 0.0564\n",
      "Epoch 303/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0573\n",
      "Epoch 304/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0569\n",
      "Epoch 305/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0587 - val_loss: 0.0570\n",
      "Epoch 306/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0560\n",
      "Epoch 307/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0558\n",
      "Epoch 308/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0560\n",
      "Epoch 309/500\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.058 - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0560\n",
      "Epoch 310/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0562\n",
      "Epoch 311/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0562\n",
      "Epoch 312/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0571\n",
      "Epoch 313/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0590 - val_loss: 0.0559\n",
      "Epoch 314/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0557\n",
      "Epoch 315/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0568\n",
      "Epoch 316/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0589 - val_loss: 0.0560\n",
      "Epoch 317/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0557\n",
      "Epoch 318/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0560\n",
      "Epoch 319/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0588 - val_loss: 0.0575\n",
      "Epoch 320/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0590 - val_loss: 0.0595\n",
      "Epoch 321/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0607 - val_loss: 0.0579\n",
      "Epoch 322/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0562\n",
      "Epoch 323/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0563\n",
      "Epoch 324/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0561\n",
      "Epoch 325/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0559\n",
      "Epoch 326/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0558\n",
      "Epoch 327/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0583\n",
      "Epoch 328/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0590 - val_loss: 0.0558\n",
      "Epoch 329/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0559\n",
      "Epoch 330/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0570\n",
      "Epoch 331/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0587 - val_loss: 0.0561\n",
      "Epoch 332/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0560\n",
      "Epoch 333/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0576\n",
      "Epoch 334/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0574\n",
      "Epoch 335/500\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.059 - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0574\n",
      "Epoch 336/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0588 - val_loss: 0.0581\n",
      "Epoch 337/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0556\n",
      "Epoch 338/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0577\n",
      "Epoch 339/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0560\n",
      "Epoch 340/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0578 - val_loss: 0.0559\n",
      "Epoch 341/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0560\n",
      "Epoch 342/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0561\n",
      "Epoch 343/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0573\n",
      "Epoch 344/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0594 - val_loss: 0.0563\n",
      "Epoch 345/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0557\n",
      "Epoch 346/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0565\n",
      "Epoch 347/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0563\n",
      "Epoch 348/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0580\n",
      "Epoch 349/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0572\n",
      "Epoch 350/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0570\n",
      "Epoch 351/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0557\n",
      "Epoch 352/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0562\n",
      "Epoch 353/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0557\n",
      "Epoch 354/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0565\n",
      "Epoch 355/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0558\n",
      "Epoch 356/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0563\n",
      "Epoch 357/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0561\n",
      "Epoch 358/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0556\n",
      "Epoch 359/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0580\n",
      "Epoch 360/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.0567\n",
      "Epoch 361/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0598 - val_loss: 0.0573\n",
      "Epoch 362/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0557\n",
      "Epoch 363/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0555\n",
      "Epoch 364/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0571\n",
      "Epoch 365/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0591 - val_loss: 0.0612\n",
      "Epoch 366/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.0555\n",
      "Epoch 367/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0563\n",
      "Epoch 368/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0560\n",
      "Epoch 369/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0580 - val_loss: 0.0558\n",
      "Epoch 370/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.0564\n",
      "Epoch 371/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.0558\n",
      "Epoch 372/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0557\n",
      "Epoch 373/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0559\n",
      "Epoch 374/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0560\n",
      "Epoch 375/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0581\n",
      "Epoch 376/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.0555\n",
      "Epoch 377/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0572\n",
      "Epoch 378/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0568\n",
      "Epoch 379/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0571\n",
      "Epoch 380/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0588 - val_loss: 0.0564\n",
      "Epoch 381/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0561\n",
      "Epoch 382/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0576\n",
      "Epoch 383/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0556\n",
      "Epoch 384/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0557\n",
      "Epoch 385/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0577\n",
      "Epoch 386/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0563\n",
      "Epoch 387/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0557\n",
      "Epoch 388/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0556\n",
      "Epoch 389/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0555\n",
      "Epoch 390/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.0555\n",
      "Epoch 391/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0559\n",
      "Epoch 392/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0561\n",
      "Epoch 393/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0563\n",
      "Epoch 394/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0562\n",
      "Epoch 395/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0557\n",
      "Epoch 396/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.0555\n",
      "Epoch 397/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0560\n",
      "Epoch 398/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0561\n",
      "Epoch 399/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0588 - val_loss: 0.0566\n",
      "Epoch 400/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0559\n",
      "Epoch 401/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0569\n",
      "Epoch 402/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0555\n",
      "Epoch 403/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0578 - val_loss: 0.0558\n",
      "Epoch 404/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0555\n",
      "Epoch 405/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0563\n",
      "Epoch 406/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0559\n",
      "Epoch 407/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0578 - val_loss: 0.0554\n",
      "Epoch 408/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0562\n",
      "Epoch 409/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0584\n",
      "Epoch 410/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0561\n",
      "Epoch 411/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0563\n",
      "Epoch 412/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0578 - val_loss: 0.0557\n",
      "Epoch 413/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.0554\n",
      "Epoch 414/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0597\n",
      "Epoch 415/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0602 - val_loss: 0.0561\n",
      "Epoch 416/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0576\n",
      "Epoch 417/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0560\n",
      "Epoch 418/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0578\n",
      "Epoch 419/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0553\n",
      "Epoch 420/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0579 - val_loss: 0.0561\n",
      "Epoch 421/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0577 - val_loss: 0.0556\n",
      "Epoch 422/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0559\n",
      "Epoch 423/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0557\n",
      "Epoch 424/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0556\n",
      "Epoch 425/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0561\n",
      "Epoch 426/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0561\n",
      "Epoch 427/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0575\n",
      "Epoch 428/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0584 - val_loss: 0.0563\n",
      "Epoch 429/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0569\n",
      "Epoch 430/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0572\n",
      "Epoch 431/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0588 - val_loss: 0.0580\n",
      "Epoch 432/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0560\n",
      "Epoch 433/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0578 - val_loss: 0.0552\n",
      "Epoch 434/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0576 - val_loss: 0.0560\n",
      "Epoch 435/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0605\n",
      "Epoch 436/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0587 - val_loss: 0.0571\n",
      "Epoch 437/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0579 - val_loss: 0.0558\n",
      "Epoch 438/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0554\n",
      "Epoch 439/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0554\n",
      "Epoch 440/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0568\n",
      "Epoch 441/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0557\n",
      "Epoch 442/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0588\n",
      "Epoch 443/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0589 - val_loss: 0.0562\n",
      "Epoch 444/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0578 - val_loss: 0.0559\n",
      "Epoch 445/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.0556\n",
      "Epoch 446/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.0554\n",
      "Epoch 447/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0576 - val_loss: 0.0552\n",
      "Epoch 448/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0578 - val_loss: 0.0558\n",
      "Epoch 449/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0575 - val_loss: 0.0569\n",
      "Epoch 450/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0570\n",
      "Epoch 451/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0594 - val_loss: 0.0578\n",
      "Epoch 452/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.0571\n",
      "Epoch 453/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0555\n",
      "Epoch 454/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0556\n",
      "Epoch 455/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0578 - val_loss: 0.0560\n",
      "Epoch 456/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0567\n",
      "Epoch 457/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0562\n",
      "Epoch 458/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0559\n",
      "Epoch 459/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.0558\n",
      "Epoch 460/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0556\n",
      "Epoch 461/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0584\n",
      "Epoch 462/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0557\n",
      "Epoch 463/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.0558\n",
      "Epoch 464/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0557\n",
      "Epoch 465/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0576 - val_loss: 0.0566\n",
      "Epoch 466/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.0553\n",
      "Epoch 467/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0563\n",
      "Epoch 468/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0557\n",
      "Epoch 469/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0578 - val_loss: 0.0577\n",
      "Epoch 470/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0573\n",
      "Epoch 471/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0580 - val_loss: 0.0552\n",
      "Epoch 472/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.0560\n",
      "Epoch 473/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0552\n",
      "Epoch 474/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0575 - val_loss: 0.0558\n",
      "Epoch 475/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0578 - val_loss: 0.0555\n",
      "Epoch 476/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0576 - val_loss: 0.0557\n",
      "Epoch 477/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0579 - val_loss: 0.0556\n",
      "Epoch 478/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0576 - val_loss: 0.0578\n",
      "Epoch 479/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.0557\n",
      "Epoch 480/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0589 - val_loss: 0.0558\n",
      "Epoch 481/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.0566\n",
      "Epoch 482/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.0555\n",
      "Epoch 483/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0575 - val_loss: 0.0570\n",
      "Epoch 484/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.0563\n",
      "Epoch 485/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0575\n",
      "Epoch 486/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0583 - val_loss: 0.0571\n",
      "Epoch 487/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0578 - val_loss: 0.0556\n",
      "Epoch 488/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.0560\n",
      "Epoch 489/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0562\n",
      "Epoch 490/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0575 - val_loss: 0.0555\n",
      "Epoch 491/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0575 - val_loss: 0.0567\n",
      "Epoch 492/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0582 - val_loss: 0.0556\n",
      "Epoch 493/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0551\n",
      "Epoch 494/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.0553\n",
      "Epoch 495/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0555\n",
      "Epoch 496/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0581 - val_loss: 0.0559\n",
      "Epoch 497/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0553\n",
      "Epoch 498/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0579 - val_loss: 0.0556\n",
      "Epoch 499/500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0576 - val_loss: 0.0556\n",
      "Epoch 500/500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0575 - val_loss: 0.0564\n"
     ]
    }
   ],
   "source": [
    "agent_28.train(X_train,batch_size,500,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = agent_28.feature_extract(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
